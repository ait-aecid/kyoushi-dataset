{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Cyber Range Kyoushi - Dataset \u00b6 IDS dataset processing and labeling package for AECID datasets. Check out the documentation of this repository.","title":"Overview"},{"location":"#cyber-range-kyoushi-dataset","text":"IDS dataset processing and labeling package for AECID datasets. Check out the documentation of this repository.","title":"Cyber Range Kyoushi - Dataset"},{"location":"cli/","text":"CLI Reference \u00b6 This page provides documentation for our command line tools. cr-kyoushi-dataset \u00b6 Run Cyber Range Kyoushi Dataset. Usage: cr-kyoushi-dataset [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --dataset , -d directory The dataset to process ./ --logstash , -l file The logstash binary to use for parsing /usr/share/logstash/bin/logstash --elasticsearch , -e text The connection string for the elasticsearch database http://127.0.0.1:9200 --help boolean Show this message and exit. False cr-kyoushi-dataset label \u00b6 Apply the labeling rules to the dataset RULE_DIRS The directories from which to load the label rules (defaults to /rules). Relative paths start at the dataset dir. Rules are automatically loaded from all *.json, *.yaml, *.yml files in the given rule dirs. Usage: cr-kyoushi-dataset label [OPTIONS] [RULE_DIRS]... Options: Name Type Description Default --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --label-object text The field to store the labels in kyoushi_labels --label / --no-label boolean If the labeling rules should be applied or not True --write / --no-write boolean If the label files should be written or not True --write-skip-files text Optionally a comma separated list of log files to not write labels for.(if this is not set label files will be written for all files with labeled log lines) required --write-exclude-index , -e text Comma separated list of indices to explicitly exclude when writing label files required --help boolean Show this message and exit. False cr-kyoushi-dataset prepare \u00b6 Usage: cr-kyoushi-dataset prepare [OPTIONS] Options: Name Type Description Default --gather-dir , -g directory The logs and facts gather source directory. This directory will be copied to the dataset directory. required --process-dir , -p directory The processing source directory (containing the process pipelines, templates and rules. required --name text The name to use for the dataset (will be prompted if not supplied) required --start datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) The the datasets observation start time (will be prompted if not supplied) required --end datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) The the datasets observation end time (will be prompted if not supplied) required --yes , -y boolean Affirm all confirmation prompts (use for non-interactive mode) False --help boolean Show this message and exit. False cr-kyoushi-dataset process \u00b6 Process the dataset and prepare it for labeling. Usage: cr-kyoushi-dataset process [OPTIONS] Options: Name Type Description Default --config , -c file The processing configuration file (defaults to /processing/process.yaml) ./processing/process.yaml --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --skip-pre boolean Skip the pre processing phase False --skip-parse boolean Skip the parsing phase False --skip-post boolean Skip the post processing phase False --help boolean Show this message and exit. False cr-kyoushi-dataset sample \u00b6 Usage: cr-kyoushi-dataset sample [OPTIONS] [SIZE] Options: Name Type Description Default --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --label-object text The field to store the labels in kyoushi_labels --label , -l text The label to get sample log lines for (if this is not set then unlabeled log lines will be sampled) required --from-timestamp datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) Optional minium timestamp for log rows to consider required --until-timestamp datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) Optional maximum timestamp for log rows to consider required --files , -f text Optionally a comma separated list of files to get sample log lines from (if this is not set all files matching the label option will be drawn from). required --related , -r text Optionally a comma separated list of elasticsearch indices for which to include the log line, that is closest (based on the timestamp) to the selected sample, as meta information. Given indices are prefixed with the dataset name. required --default-label text The label to assign to unlabeled log row (e.g., when --label is not used) normal --index , -i text Comma separated list of indices to consider for sampling required --exclude-index , -e text Comma separated list of indices to explicitly exclude from the sampling required --seed , -s text The random seed to use for the sampling query required --seed-field text The field to use for the elasticsearch random score _seq_no --list boolean Only list the available labels with their log line counts as JSON array False --help boolean Show this message and exit. False cr-kyoushi-dataset version \u00b6 Get the library version. Usage: cr-kyoushi-dataset version [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"CLI Reference"},{"location":"cli/#cli-reference","text":"This page provides documentation for our command line tools.","title":"CLI Reference"},{"location":"cli/#cr-kyoushi-dataset","text":"Run Cyber Range Kyoushi Dataset. Usage: cr-kyoushi-dataset [OPTIONS] COMMAND [ARGS]... Options: Name Type Description Default --dataset , -d directory The dataset to process ./ --logstash , -l file The logstash binary to use for parsing /usr/share/logstash/bin/logstash --elasticsearch , -e text The connection string for the elasticsearch database http://127.0.0.1:9200 --help boolean Show this message and exit. False","title":"cr-kyoushi-dataset"},{"location":"cli/#cr-kyoushi-dataset-label","text":"Apply the labeling rules to the dataset RULE_DIRS The directories from which to load the label rules (defaults to /rules). Relative paths start at the dataset dir. Rules are automatically loaded from all *.json, *.yaml, *.yml files in the given rule dirs. Usage: cr-kyoushi-dataset label [OPTIONS] [RULE_DIRS]... Options: Name Type Description Default --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --label-object text The field to store the labels in kyoushi_labels --label / --no-label boolean If the labeling rules should be applied or not True --write / --no-write boolean If the label files should be written or not True --write-skip-files text Optionally a comma separated list of log files to not write labels for.(if this is not set label files will be written for all files with labeled log lines) required --write-exclude-index , -e text Comma separated list of indices to explicitly exclude when writing label files required --help boolean Show this message and exit. False","title":"label"},{"location":"cli/#cr-kyoushi-dataset-prepare","text":"Usage: cr-kyoushi-dataset prepare [OPTIONS] Options: Name Type Description Default --gather-dir , -g directory The logs and facts gather source directory. This directory will be copied to the dataset directory. required --process-dir , -p directory The processing source directory (containing the process pipelines, templates and rules. required --name text The name to use for the dataset (will be prompted if not supplied) required --start datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) The the datasets observation start time (will be prompted if not supplied) required --end datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) The the datasets observation end time (will be prompted if not supplied) required --yes , -y boolean Affirm all confirmation prompts (use for non-interactive mode) False --help boolean Show this message and exit. False","title":"prepare"},{"location":"cli/#cr-kyoushi-dataset-process","text":"Process the dataset and prepare it for labeling. Usage: cr-kyoushi-dataset process [OPTIONS] Options: Name Type Description Default --config , -c file The processing configuration file (defaults to /processing/process.yaml) ./processing/process.yaml --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --skip-pre boolean Skip the pre processing phase False --skip-parse boolean Skip the parsing phase False --skip-post boolean Skip the post processing phase False --help boolean Show this message and exit. False","title":"process"},{"location":"cli/#cr-kyoushi-dataset-sample","text":"Usage: cr-kyoushi-dataset sample [OPTIONS] [SIZE] Options: Name Type Description Default --dataset-config file The dataset configuration file (defaults to /dataset.yaml) ./dataset.yaml --label-object text The field to store the labels in kyoushi_labels --label , -l text The label to get sample log lines for (if this is not set then unlabeled log lines will be sampled) required --from-timestamp datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) Optional minium timestamp for log rows to consider required --until-timestamp datetime ( %Y-%m-%d | %Y-%m-%dT%H:%M:%S | %Y-%m-%d %H:%M:%S | %Y-%m-%dT%H:%M:%S.%f | %Y-%m-%d %H:%M:%S.%f | %Y-%m-%dT%H:%M:%S%z | %Y-%m-%dT%H:%M:%SZ | %Y-%m-%dT%H:%M:%S.%f%z | %Y-%m-%dT%H:%M:%S.%fZ ) Optional maximum timestamp for log rows to consider required --files , -f text Optionally a comma separated list of files to get sample log lines from (if this is not set all files matching the label option will be drawn from). required --related , -r text Optionally a comma separated list of elasticsearch indices for which to include the log line, that is closest (based on the timestamp) to the selected sample, as meta information. Given indices are prefixed with the dataset name. required --default-label text The label to assign to unlabeled log row (e.g., when --label is not used) normal --index , -i text Comma separated list of indices to consider for sampling required --exclude-index , -e text Comma separated list of indices to explicitly exclude from the sampling required --seed , -s text The random seed to use for the sampling query required --seed-field text The field to use for the elasticsearch random score _seq_no --list boolean Only list the available labels with their log line counts as JSON array False --help boolean Show this message and exit. False","title":"sample"},{"location":"cli/#cr-kyoushi-dataset-version","text":"Get the library version. Usage: cr-kyoushi-dataset version [OPTIONS] Options: Name Type Description Default --help boolean Show this message and exit. False","title":"version"},{"location":"contributing/","text":"Contributing \u00b6 Packaging and Installation \u00b6 This project uses poetry for packaging and dependency management. So before you can start developing you will have to install poetry . Please refer to the poetry documentation on how to install and use poetry . Install for development \u00b6 After you have installed poetry and added your access token you can use it to install a local development clone of the repository using: $ poetry install Tests can be run using poetry or by using the Makefile provided with the project. $ poetry run pytest $ make test # or if you wish to generate a HTML coverage report $ make test-html Code Quality and Formatting \u00b6 This project uses pre-commit and various code linters and formatters to ensure that committed code meets certain quality criteria. Note that these criteria are also checked by CI pipelines. This means that should you push unchecked code the pipelines will fail! So it is highly recommend that you install pre-commit so that such code does not make it into the the git history. To make sure that you don't accidentally commit code that does not follow the coding style, you can install a pre-commit hook that will check that everything is in order: $ poetry run pre-commit install The pre-commit git hook runs automatically when you try to commit your changes. It will automatically check and try to format your code. Should your code not conform to the quality criteria or if some re-formatting was necessary the commit will fail. In such cases verify the formatting changes made by pre-commit and fix any other code quality issues reported by the checks before committing your changes again. You can also manually run the pre-commit hook by issuing the following command. $ poetry run pre-commit run --all-files Alternatively you can also use the Makefile to run just the quality checks, auto formatting or just specific checks. $ make check $ make format Run make list for a full list of available make targets. Build & View Documentation \u00b6 This project uses MkDocs for documentation. If you wish to view or edit the docs you can start a live server by running: $ mkdocs serve The live server will automatically detect changes and refresh the documentation. We also provide convenience make targets for building the documentation and starting the live server: # build the distributable documentation site $ make docs # build a offline viewable documentation site, without web server simply open the index file # (Note that the search function is disabled with this) $ make docs-offline # start the mkdocs live server $ make docs-serve","title":"Contributing"},{"location":"contributing/#contributing","text":"","title":"Contributing"},{"location":"contributing/#packaging-and-installation","text":"This project uses poetry for packaging and dependency management. So before you can start developing you will have to install poetry . Please refer to the poetry documentation on how to install and use poetry .","title":"Packaging and Installation"},{"location":"contributing/#install-for-development","text":"After you have installed poetry and added your access token you can use it to install a local development clone of the repository using: $ poetry install Tests can be run using poetry or by using the Makefile provided with the project. $ poetry run pytest $ make test # or if you wish to generate a HTML coverage report $ make test-html","title":"Install for development"},{"location":"contributing/#code-quality-and-formatting","text":"This project uses pre-commit and various code linters and formatters to ensure that committed code meets certain quality criteria. Note that these criteria are also checked by CI pipelines. This means that should you push unchecked code the pipelines will fail! So it is highly recommend that you install pre-commit so that such code does not make it into the the git history. To make sure that you don't accidentally commit code that does not follow the coding style, you can install a pre-commit hook that will check that everything is in order: $ poetry run pre-commit install The pre-commit git hook runs automatically when you try to commit your changes. It will automatically check and try to format your code. Should your code not conform to the quality criteria or if some re-formatting was necessary the commit will fail. In such cases verify the formatting changes made by pre-commit and fix any other code quality issues reported by the checks before committing your changes again. You can also manually run the pre-commit hook by issuing the following command. $ poetry run pre-commit run --all-files Alternatively you can also use the Makefile to run just the quality checks, auto formatting or just specific checks. $ make check $ make format Run make list for a full list of available make targets.","title":"Code Quality and Formatting"},{"location":"contributing/#build-view-documentation","text":"This project uses MkDocs for documentation. If you wish to view or edit the docs you can start a live server by running: $ mkdocs serve The live server will automatically detect changes and refresh the documentation. We also provide convenience make targets for building the documentation and starting the live server: # build the distributable documentation site $ make docs # build a offline viewable documentation site, without web server simply open the index file # (Note that the search function is disabled with this) $ make docs-offline # start the mkdocs live server $ make docs-serve","title":"Build &amp; View Documentation"},{"location":"license/","text":"GNU GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/> Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. Preamble The GNU General Public License is a free, copyleft license for software and other kinds of works. The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too. When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things. To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others. For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights. Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it. For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions. Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users. Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free. The precise terms and conditions for copying, distribution and modification follow. TERMS AND CONDITIONS 0. Definitions. \"This License\" refers to version 3 of the GNU General Public License. \"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks. \"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations. To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work. A \"covered work\" means either the unmodified Program or a work based on the Program. To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well. To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying. An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion. 1. Source Code. The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work. A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language. The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it. The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work. The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source. The Corresponding Source for a work in source code form is that same work. 2. Basic Permissions. All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law. You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you. Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary. 3. Protecting Users' Legal Rights From Anti-Circumvention Law. No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures. When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures. 4. Conveying Verbatim Copies. You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program. You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee. 5. Conveying Modified Source Versions. You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions: a) The work must carry prominent notices stating that you modified it, and giving a relevant date. b) The work must carry prominent notices stating that it is released under this License and any conditions added under section 7. This requirement modifies the requirement in section 4 to \"keep intact all notices\". c) You must license the entire work, as a whole, under this License to anyone who comes into possession of a copy. This License will therefore apply, along with any applicable section 7 additional terms, to the whole of the work, and all its parts, regardless of how they are packaged. This License gives no permission to license the work in any other way, but it does not invalidate such permission if you have separately received it. d) If the work has interactive user interfaces, each must display Appropriate Legal Notices; however, if the Program has interactive interfaces that do not display Appropriate Legal Notices, your work need not make them do so. A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate. 6. Conveying Non-Source Forms. You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways: a) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by the Corresponding Source fixed on a durable physical medium customarily used for software interchange. b) Convey the object code in, or embodied in, a physical product (including a physical distribution medium), accompanied by a written offer, valid for at least three years and valid for as long as you offer spare parts or customer support for that product model, to give anyone who possesses the object code either (1) a copy of the Corresponding Source for all the software in the product that is covered by this License, on a durable physical medium customarily used for software interchange, for a price no more than your reasonable cost of physically performing this conveying of source, or (2) access to copy the Corresponding Source from a network server at no charge. c) Convey individual copies of the object code with a copy of the written offer to provide the Corresponding Source. This alternative is allowed only occasionally and noncommercially, and only if you received the object code with such an offer, in accord with subsection 6b. d) Convey the object code by offering access from a designated place (gratis or for a charge), and offer equivalent access to the Corresponding Source in the same way through the same place at no further charge. You need not require recipients to copy the Corresponding Source along with the object code. If the place to copy the object code is a network server, the Corresponding Source may be on a different server (operated by you or a third party) that supports equivalent copying facilities, provided you maintain clear directions next to the object code saying where to find the Corresponding Source. Regardless of what server hosts the Corresponding Source, you remain obligated to ensure that it is available for as long as needed to satisfy these requirements. e) Convey the object code using peer-to-peer transmission, provided you inform other peers where the object code and Corresponding Source of the work are being offered to the general public at no charge under subsection 6d. A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work. A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product. \"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made. If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM). The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network. Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying. 7. Additional Terms. \"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions. When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission. Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms: a) Disclaiming warranty or limiting liability differently from the terms of sections 15 and 16 of this License; or b) Requiring preservation of specified reasonable legal notices or author attributions in that material or in the Appropriate Legal Notices displayed by works containing it; or c) Prohibiting misrepresentation of the origin of that material, or requiring that modified versions of such material be marked in reasonable ways as different from the original version; or d) Limiting the use for publicity purposes of names of licensors or authors of the material; or e) Declining to grant rights under trademark law for use of some trade names, trademarks, or service marks; or f) Requiring indemnification of licensors and authors of that material by anyone who conveys the material (or modified versions of it) with contractual assumptions of liability to the recipient, for any liability that these contractual assumptions directly impose on those licensors and authors. All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying. If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms. Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way. 8. Termination. You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11). However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10. 9. Acceptance Not Required for Having Copies. You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so. 10. Automatic Licensing of Downstream Recipients. Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License. An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts. You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it. 11. Patents. A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\". A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License. Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version. In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party. If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid. If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it. A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007. Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law. 12. No Surrender of Others' Freedom. If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program. 13. Use with the GNU Affero General Public License. Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such. 14. Revised Versions of this License. The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation. If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program. Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version. 15. Disclaimer of Warranty. THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION. 16. Limitation of Liability. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES. 17. Interpretation of Sections 15 and 16. If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee. END OF TERMS AND CONDITIONS How to Apply These Terms to Your New Programs If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms. To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found. <one line to give the program's name and a brief idea of what it does.> Copyright (C) <year> <name of author> This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>. Also add information on how to contact you by electronic and paper mail. If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode: <program> Copyright (C) <year> <name of author> This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'. This is free software, and you are welcome to redistribute it under certain conditions; type `show c' for details. The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\". You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see <https://www.gnu.org/licenses/>. The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read <https://www.gnu.org/licenses/why-not-lgpl.html>.","title":"License"},{"location":"processors/","text":"Dataset Processors \u00b6 On this page you can learn about the various dataset processors available with Cyber Range Kyoushi Dataset. Util and Debug \u00b6 Console Print ( print ) \u00b6 Debug processor that simply prints a message. Examples: - name : Print Hello World type : print msg : Hello World msg : str pydantic-field required \u00b6 The message to print File Manipulation \u00b6 Create Directory ( mkdir ) \u00b6 Processor for creating file directories. Examples: - name : Ensure processing config directory exists type : mkdir path : processing/config path : Path pydantic-field required \u00b6 The directory path to create recursive : bool pydantic-field \u00b6 If all missing parent directories should als be created GZip Decompress ( gzip ) \u00b6 Processor for decompressing gzip files. It is possible to either define a glob of gzip files or a path to a single gzip file. If a glob is defined it is resolved relative to the defined path (default= <dataset dir> ). Examples: - name : Decompress all GZIP logs type : gzip path : gather glob : \"*/logs/**/*.gz\" glob : str pydantic-field \u00b6 The file glob expression to use path : Path pydantic-field \u00b6 The base path to search for the gzipped files. File Template ( template ) \u00b6 Processor for rendering template files. In addition to the normal processor context it is also possible to define a template_context . If template_context is defined it will be used for rendering the template otherwise the normal processor context will be used. Examples: - type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\" dest : Path pydantic-field required \u00b6 The destination to save the rendered file to src : Path pydantic-field required \u00b6 The template file to render template_context : ProcessorContext pydantic-field \u00b6 Optional template context if this is not set the processor context is used instead File Trimming ( trim ) \u00b6 Processor for trimming log files to a defined time frame. This processor can be used to remove all log lines outside of defined dataset observation times. Note Currently only support simple time frames with a single start and end time. Examples: - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml # we only want to trim the logs of servers that will be part # of the IDS dataset indices : - attacker_0-* end : datetime pydantic-field \u00b6 The end time to trim the logs to (defaults to dataset end) exclude : str pydantic-field \u00b6 Indices to exclude from triming. This will overwrite/exclude indices from any patterns supplied in indices indices : str pydantic-field \u00b6 The log indices to trim (defaults to <dataset>-* ) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. start : datetime pydantic-field \u00b6 The start time to trim the logs to (defaults to dataset start) PCAP Conversion ( pcap.elasticsearch ) \u00b6 Processor for converting PCAP files to ndjson format. This processor uses tshark to convert PCAP files to a line based JSON format ( ek output). Examples: - name : Convert attacker pcap to elasticsearch json type : pcap.elasticsearch pcap : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.pcap dest : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.json tls_keylog : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/premaster.txt read_filter : \"tcp or udp or icmp\" create_destination_dirs : bool pydantic-field \u00b6 If the processor should create missing destination parent directories dest : Path pydantic-field required \u00b6 The destination file force : bool pydantic-field \u00b6 If the pcap should be created even when the destination file already exists. packet_details : bool pydantic-field \u00b6 If the packet details should be included, when packet_summary=False then details are always included (-V option). packet_summary : bool pydantic-field \u00b6 If the packet summaries should be included (-P option). pcap : FilePath pydantic-field required \u00b6 The pcap file to convert protocol_match_filter : str pydantic-field \u00b6 Display filter for protocols and their fields (-J option).Parent and child nodes are included for all matches lower level protocols must be added explicitly. protocol_match_filter_parent : str pydantic-field \u00b6 Display filter for protocols and their fields. Only partent nodes are included (-j option). read_filter : str pydantic-field \u00b6 The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) remove_filtered : bool pydantic-field \u00b6 Remove filtered fields from the event dicts. remove_index_messages : bool pydantic-field \u00b6 If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. tls_keylog : FilePath pydantic-field \u00b6 TLS keylog file to decrypt TLS on the fly. tshark_bin : FilePath pydantic-field \u00b6 Path to your tshark binary (searches in common paths if not supplied) Elasticsearch \u00b6 Elasticsearch ingest pipeline ( elasticsearch.ingest ) \u00b6 Processor for creating Elasticsearch ingest pipelines. This processor can be used to create Elasticsearch ingest pipelines for parsing log event. The log file parsing can then be configured to use the pipelines for upstream parsing instead of local Logstash parsing. Examples: - name : Add auditd ingest pipeline to elasticsearch type : elasticsearch.ingest ingest_pipeline : processing/logstash/auditd-ingest.yml ingest_pipeline_id : auditd-logs ingest_pipeline : FilePath pydantic-field required \u00b6 The ingest pipeline to add to elasticsearch ingest_pipeline_id : str pydantic-field required \u00b6 The id to use for the ingest pipeline Elasticsearch Index Template ( elasticsearch.template ) \u00b6 Processor for configuring Elasticsearch index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ] composed_of : str pydantic-field \u00b6 Optional list of component templates the index template should be composed of. create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. index_patterns : str pydantic-field \u00b6 The index patterns the template should be applied to. If this is not set then the index template file must contain this information already! indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. priority : int pydantic-field \u00b6 The priority to assign to this index template (higher values take precedent). template : FilePath pydantic-field required \u00b6 The index template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index template Elasticsearch Index Component Template ( elasticsearch.component_template ) \u00b6 Processor for creating Elasticsearch index component templates. This processor can be used to create Elasticsearch index component templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap component template type : elasticsearch.component_template template : processing/logstash/pcap-component-template.json template_name : pcap create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. template : FilePath pydantic-field required \u00b6 The index component template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index component template Elasticsearch Legacy Index Template ( elasticsearch.legacy_template ) \u00b6 Processor for configuring Elasticsearch legacy index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the legacy index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.legacy_template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ] create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. index_patterns : str pydantic-field \u00b6 The index patterns the template should be applied to. If this is not set then the index template file must contain this information already! indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. order : int pydantic-field \u00b6 The order to assign to this index template (higher values take precedent). template : FilePath pydantic-field required \u00b6 The index template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index template Logstash \u00b6 Logstash setup ( logstash.setup ) \u00b6 Logstash parser setup processor. This processor is used to create all the configuration files required for the Logstash parser (e.g., input and filter configs). Unless you provide a static Logstash parsing configuration you must invoke this processor at somepoint during the pre-processing phase. Note The processor only does the basic setup any Logstash parsing filters used for processing specific log events must be prepared separately. Examples: - name : Setup logstash pipeline type : logstash.setup context : var_files : servers : processing/config/servers.yaml servers : \"{{ servers }}\" index_template_template : Path pydantic-field \u00b6 The template to use for the elasticsearch dataset index patterns index template input_config_name : str pydantic-field \u00b6 The name of the log inputs config file. (relative to the pipeline config dir) input_template : Path pydantic-field \u00b6 The template to use for the file input plugin configuration legacy_index_template_template : Path pydantic-field \u00b6 The template to use for the elasticsearch dataset legacy index patterns index template logstash_template : Path pydantic-field \u00b6 The template to use for the logstash configuration output_config_name : str pydantic-field \u00b6 The name of the log outputs config file. (relative to the pipeline config dir) output_template : Path pydantic-field \u00b6 The template to use for the file output plugin configuration piplines_template : Path pydantic-field \u00b6 The template to use for the logstash pipelines configuration pre_process_name : str pydantic-field \u00b6 The file name to use for the pre process filters config. This is prefixed with 0000_ to ensure that the filters are run first. pre_process_template : Path pydantic-field \u00b6 The template to use for the file output plugin configuration servers : Any pydantic-field required \u00b6 Dictionary of servers and their log configurations use_legacy_template : bool pydantic-field \u00b6 If the output config should use the legacy index template or the modern index template Data Flow and Logic \u00b6 ForEach Loop ( foreach ) \u00b6 For each processor This is a special processor container allowing for the dynamic creation of a list of processor based on a list of items. Examples: - name : Render labeling rules type : foreach # processing/templates/rules items : - src : 0_auth.yaml.j2 dest : 0_auth.yaml - src : apache.yaml.j2 dest : apache.yaml - src : audit.yaml.j2 dest : audit.yaml - src : openvpn.yaml.j2 dest : openvpn.yaml processor : type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\" items : Any pydantic-field required \u00b6 List of items to create processors for loop_var : str pydantic-field \u00b6 The variable name to use for current loops item in the processor context processor : Any pydantic-field required \u00b6 The processor template config to create multiple instances of","title":"Dataset Processors"},{"location":"processors/#dataset-processors","text":"On this page you can learn about the various dataset processors available with Cyber Range Kyoushi Dataset.","title":"Dataset Processors"},{"location":"processors/#util-and-debug","text":"","title":"Util and Debug"},{"location":"processors/#console-print-print","text":"Debug processor that simply prints a message. Examples: - name : Print Hello World type : print msg : Hello World","title":"Console Print (print)"},{"location":"processors/#file-manipulation","text":"","title":"File Manipulation"},{"location":"processors/#create-directory-mkdir","text":"Processor for creating file directories. Examples: - name : Ensure processing config directory exists type : mkdir path : processing/config","title":"Create Directory (mkdir)"},{"location":"processors/#gzip-decompress-gzip","text":"Processor for decompressing gzip files. It is possible to either define a glob of gzip files or a path to a single gzip file. If a glob is defined it is resolved relative to the defined path (default= <dataset dir> ). Examples: - name : Decompress all GZIP logs type : gzip path : gather glob : \"*/logs/**/*.gz\"","title":"GZip Decompress (gzip)"},{"location":"processors/#file-template-template","text":"Processor for rendering template files. In addition to the normal processor context it is also possible to define a template_context . If template_context is defined it will be used for rendering the template otherwise the normal processor context will be used. Examples: - type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\"","title":"File Template (template)"},{"location":"processors/#file-trimming-trim","text":"Processor for trimming log files to a defined time frame. This processor can be used to remove all log lines outside of defined dataset observation times. Note Currently only support simple time frames with a single start and end time. Examples: - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml # we only want to trim the logs of servers that will be part # of the IDS dataset indices : - attacker_0-*","title":"File Trimming (trim)"},{"location":"processors/#pcap-conversion-pcapelasticsearch","text":"Processor for converting PCAP files to ndjson format. This processor uses tshark to convert PCAP files to a line based JSON format ( ek output). Examples: - name : Convert attacker pcap to elasticsearch json type : pcap.elasticsearch pcap : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.pcap dest : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.json tls_keylog : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/premaster.txt read_filter : \"tcp or udp or icmp\"","title":"PCAP Conversion (pcap.elasticsearch)"},{"location":"processors/#elasticsearch","text":"","title":"Elasticsearch"},{"location":"processors/#elasticsearch-ingest-pipeline-elasticsearchingest","text":"Processor for creating Elasticsearch ingest pipelines. This processor can be used to create Elasticsearch ingest pipelines for parsing log event. The log file parsing can then be configured to use the pipelines for upstream parsing instead of local Logstash parsing. Examples: - name : Add auditd ingest pipeline to elasticsearch type : elasticsearch.ingest ingest_pipeline : processing/logstash/auditd-ingest.yml ingest_pipeline_id : auditd-logs","title":"Elasticsearch ingest pipeline (elasticsearch.ingest)"},{"location":"processors/#elasticsearch-index-template-elasticsearchtemplate","text":"Processor for configuring Elasticsearch index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ]","title":"Elasticsearch Index Template (elasticsearch.template)"},{"location":"processors/#elasticsearch-index-component-template-elasticsearchcomponent_template","text":"Processor for creating Elasticsearch index component templates. This processor can be used to create Elasticsearch index component templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap component template type : elasticsearch.component_template template : processing/logstash/pcap-component-template.json template_name : pcap","title":"Elasticsearch Index Component Template (elasticsearch.component_template)"},{"location":"processors/#elasticsearch-legacy-index-template-elasticsearchlegacy_template","text":"Processor for configuring Elasticsearch legacy index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the legacy index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.legacy_template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ]","title":"Elasticsearch Legacy Index Template (elasticsearch.legacy_template)"},{"location":"processors/#logstash","text":"","title":"Logstash"},{"location":"processors/#logstash-setup-logstashsetup","text":"Logstash parser setup processor. This processor is used to create all the configuration files required for the Logstash parser (e.g., input and filter configs). Unless you provide a static Logstash parsing configuration you must invoke this processor at somepoint during the pre-processing phase. Note The processor only does the basic setup any Logstash parsing filters used for processing specific log events must be prepared separately. Examples: - name : Setup logstash pipeline type : logstash.setup context : var_files : servers : processing/config/servers.yaml servers : \"{{ servers }}\"","title":"Logstash setup (logstash.setup)"},{"location":"processors/#data-flow-and-logic","text":"","title":"Data Flow and Logic"},{"location":"processors/#foreach-loop-foreach","text":"For each processor This is a special processor container allowing for the dynamic creation of a list of processor based on a list of items. Examples: - name : Render labeling rules type : foreach # processing/templates/rules items : - src : 0_auth.yaml.j2 dest : 0_auth.yaml - src : apache.yaml.j2 dest : apache.yaml - src : audit.yaml.j2 dest : audit.yaml - src : openvpn.yaml.j2 dest : openvpn.yaml processor : type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\"","title":"ForEach Loop (foreach)"},{"location":"rules/","text":"Labeling Rules \u00b6 On this page you can learn about the various labeling rules available with Cyber Range Kyoushi Dataset. DSL Query Rule ( elasticsearch.query ) \u00b6 Applies labels based on a simple Elasticsearch DSL query. Examples: - type : elasticsearch.query id : attacker.foothold.vpn.ip labels : - attacker_vpn - foothold description : >- This rule applies the labels to all openvpn log rows that have the attacker server as source ip and are within the foothold phase. index : - openvpn-vpn filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" query : - match : source.ip : '192.42.0.255' description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config EQL Sequence Rule ( elasticsearch.sequence ) \u00b6 Applies labels to a sequence of log events defined by an EQL query. This labeling rule is defined as an EQL query . Using this syntax it is possible to define a sequence of related events and retrieve them. All events part of retrieved sequences are then labeled. Examples: - type : elasticsearch.sequence id : attacker.webshell.upload.seq labels : [ webshell_upload ] description : >- This rule labels the web shell upload step by matching the 3 step sequence within the foothold phase. index : - apache_access-intranet_server # since we do these requests very fast # we need the line number as tie breaker tiebreaker_field : log.file.line by : source.address max_span : 2m filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" sequences : - '[ apache where event.action == \"access\" and url.original == \"/\" ]' - '[ apache where event.action == \"access\" and url.original == \"/?p=5\" ]' - '[ apache where event.action == \"access\" and http.request.method == \"POST\" and url.original == \"/wp-admin/admin-ajax.php\" ]' batch_size : int pydantic-field \u00b6 The amount of sequences to update with each batch. Cannot be bigger than max_result_window by : Union [ List [ str ], str ] pydantic-field \u00b6 Optional global sequence by fields description : str pydantic-field \u00b6 An optional description for the rule event_category_field : str pydantic-field \u00b6 The field used to categories events filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule max_result_window : int pydantic-field \u00b6 The max result window allowed on the elasticsearch instance max_span : str pydantic-field \u00b6 Optional max time span in which a sequence must occur to be considered a match sequences : str pydantic-field required \u00b6 Event sequences to search. Must contain at least two events. tiebreaker_field : str pydantic-field \u00b6 (Optional, string) Field used to sort hits with the same timestamp in ascending order. timestamp_field : str pydantic-field \u00b6 The field containing the event timestamp type_field : str pydantic-field required \u00b6 The rule type as passed in from the config until : str pydantic-field \u00b6 Optional until event marking the end of valid sequences. The until event will not be labeled. DSL Sub Query Rule ( elasticsearch.sub_query ) \u00b6 Labeling rule that labels the results of multiple sub queries. This labeling rule first executes a base query to retrieve information. It then renders and executes a templated sub query for each row retrieved from the base query. The result rows of these dynamically generated sub queries are then labled. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.sub_query id : attacker.foothold.apache.access_dropped labels : - attacker_http - foothold description : >- This rule tries to match attacker requests that we where unable to match to a labeled response with access log entries. Such cases can happen if the corresponding response gets lost in the network or otherwise is not sent. index : - pcap-attacker_0 # obligatory match all query : - term : destination.ip : \"172.16.0.217\" filter : - term : event.category : http - term : event.action : request # we are looking for requests that have not been marked as attacker http yet # most likely they did not have a matching response due to some network error # or timeout - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] sub_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"172.16.100.151\" filter : - range : \"@timestamp\" : # the access log entry should be after the request, but since the access log # does not have microseconds we drop them here as well gte : \"{{ (HIT['@timestamp'] | as_datetime).replace(microsecond=0) }}\" # the type of error we are looking for should create an access log entry almost immediately # se we keep the time frame short lte : \"{{ ( HIT['@timestamp'] | as_datetime).replace(microsecond=0) + timedelta(seconds=1) }}\" description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. sub_query : QueryBase pydantic-field required \u00b6 The templated sub query to use to apply the labels. Executed for each hit of the parent query. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config DSL Parent Query Rule ( elasticsearch.parent_query ) \u00b6 Applies the labels to all rows of a base query for which a parent query returns results. This labeling rule first executes a base query to retrieve rows we might want to apply labels to. It then renders and executes a templated parent query for each retrieved row. The parent queries are then used to indicate if the initial result row should be labeled or not. By default result rows of the base query are labeled if the corresponding parent query returns at leas one row. It is possible to configure this minimum number e.g., to require at least two results. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.parent_query id : attacker.foothold.apache.error_access labels : - attacker_http - foothold description : >- This rule looks for unlabeled error messages resulting from VPN server traffic within the attack time and tries to match it to an already labeled access log row. index : - apache_error-intranet_server query : match : source.address : \"172.16.100.151\" filter : # use script query to match only entries that # are not already tagged for as attacker http in the foothold phase - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] parent_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"{{ HIT.source.address }}\" # we are looking for parents that are labeled as attacker http - bool : must : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] filter : - range : # parent must be within +-1s of potential child \"@timestamp\" : gte : \"{{ (HIT['@timestamp'] | as_datetime) - timedelta(seconds=1) }}\" lte : \"{{ ( HIT['@timestamp'] | as_datetime) + timedelta(seconds=1) }}\" description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule max_result_window : int pydantic-field \u00b6 The max result window allowed on the elasticsearch instance min_match : int pydantic-field \u00b6 The minimum number of parent matches needed for the main query to be labeled. parent_query : QueryBase pydantic-field required \u00b6 The templated parent query to check if the labels should be applied to a query hit. query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config check_parent ( self , parent_query , min_match , dataset_config , es ) \u00b6 Executes a parent query and returns if there were enough result rows. Parameters: Name Type Description Default parent_query QueryBase The parent query to execute required min_match int The minimum number of result rows required required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required Returns: Type Description bool True if the query returned >= min_match rows and False otherwise.","title":"Labeling Rules"},{"location":"rules/#labeling-rules","text":"On this page you can learn about the various labeling rules available with Cyber Range Kyoushi Dataset.","title":"Labeling Rules"},{"location":"rules/#dsl-query-rule-elasticsearchquery","text":"Applies labels based on a simple Elasticsearch DSL query. Examples: - type : elasticsearch.query id : attacker.foothold.vpn.ip labels : - attacker_vpn - foothold description : >- This rule applies the labels to all openvpn log rows that have the attacker server as source ip and are within the foothold phase. index : - openvpn-vpn filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" query : - match : source.ip : '192.42.0.255'","title":"DSL Query Rule (elasticsearch.query)"},{"location":"rules/#eql-sequence-rule-elasticsearchsequence","text":"Applies labels to a sequence of log events defined by an EQL query. This labeling rule is defined as an EQL query . Using this syntax it is possible to define a sequence of related events and retrieve them. All events part of retrieved sequences are then labeled. Examples: - type : elasticsearch.sequence id : attacker.webshell.upload.seq labels : [ webshell_upload ] description : >- This rule labels the web shell upload step by matching the 3 step sequence within the foothold phase. index : - apache_access-intranet_server # since we do these requests very fast # we need the line number as tie breaker tiebreaker_field : log.file.line by : source.address max_span : 2m filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" sequences : - '[ apache where event.action == \"access\" and url.original == \"/\" ]' - '[ apache where event.action == \"access\" and url.original == \"/?p=5\" ]' - '[ apache where event.action == \"access\" and http.request.method == \"POST\" and url.original == \"/wp-admin/admin-ajax.php\" ]'","title":"EQL Sequence Rule (elasticsearch.sequence)"},{"location":"rules/#dsl-sub-query-rule-elasticsearchsub_query","text":"Labeling rule that labels the results of multiple sub queries. This labeling rule first executes a base query to retrieve information. It then renders and executes a templated sub query for each row retrieved from the base query. The result rows of these dynamically generated sub queries are then labled. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.sub_query id : attacker.foothold.apache.access_dropped labels : - attacker_http - foothold description : >- This rule tries to match attacker requests that we where unable to match to a labeled response with access log entries. Such cases can happen if the corresponding response gets lost in the network or otherwise is not sent. index : - pcap-attacker_0 # obligatory match all query : - term : destination.ip : \"172.16.0.217\" filter : - term : event.category : http - term : event.action : request # we are looking for requests that have not been marked as attacker http yet # most likely they did not have a matching response due to some network error # or timeout - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] sub_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"172.16.100.151\" filter : - range : \"@timestamp\" : # the access log entry should be after the request, but since the access log # does not have microseconds we drop them here as well gte : \"{{ (HIT['@timestamp'] | as_datetime).replace(microsecond=0) }}\" # the type of error we are looking for should create an access log entry almost immediately # se we keep the time frame short lte : \"{{ ( HIT['@timestamp'] | as_datetime).replace(microsecond=0) + timedelta(seconds=1) }}\"","title":"DSL Sub Query Rule (elasticsearch.sub_query)"},{"location":"rules/#dsl-parent-query-rule-elasticsearchparent_query","text":"Applies the labels to all rows of a base query for which a parent query returns results. This labeling rule first executes a base query to retrieve rows we might want to apply labels to. It then renders and executes a templated parent query for each retrieved row. The parent queries are then used to indicate if the initial result row should be labeled or not. By default result rows of the base query are labeled if the corresponding parent query returns at leas one row. It is possible to configure this minimum number e.g., to require at least two results. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.parent_query id : attacker.foothold.apache.error_access labels : - attacker_http - foothold description : >- This rule looks for unlabeled error messages resulting from VPN server traffic within the attack time and tries to match it to an already labeled access log row. index : - apache_error-intranet_server query : match : source.address : \"172.16.100.151\" filter : # use script query to match only entries that # are not already tagged for as attacker http in the foothold phase - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] parent_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"{{ HIT.source.address }}\" # we are looking for parents that are labeled as attacker http - bool : must : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] filter : - range : # parent must be within +-1s of potential child \"@timestamp\" : gte : \"{{ (HIT['@timestamp'] | as_datetime) - timedelta(seconds=1) }}\" lte : \"{{ ( HIT['@timestamp'] | as_datetime) + timedelta(seconds=1) }}\"","title":"DSL Parent Query Rule (elasticsearch.parent_query)"},{"location":"examples/process/","text":"Processing Example \u00b6 The example directory contains an example directory layout and mini configuration for dataset processing using the Cyber Range Kyoushi Dataset CLI tool. While the tool itself does not enforce any specific directory or file layout, other than configuring the processing and parsing pipeline through the process.yaml file, we recommend following the structure presented here as it promotes separating processing into multiple steps making each step smaller and easier to understand. Note The layout presented here is also reflected by many of the default values of configuration options. Layout \u00b6 process.yaml config <name>_queries.yaml logs.yaml logstash conf.d 0000_pre_process.conf * input.conf * output.conf * <log type>.conf data ** log ** jvm.options * log4j2.properties * logstash.yml * pipelines.yml * <name>-index-template.json <name>-ingest.yaml templates rules <rule class>.yaml.j2 attacker attacker.yaml.j2 <phase name>.yaml.j2 groups.json.j2 servers.json.j2 Note * These files and directories are automatically generated by the Logstash setup processor . Note ** By default the logstash/data and logstash/log directories are used as Logstash runtime directories for storing current state, temporary files and generated logs. process.yaml \u00b6 The processing and parsing configuration. config \u00b6 The config directory should be used to store any static configuration files to be used or loaded as part of the dataset processing pipeline. Note that we also highly recommend to make this directory or a sub directory of it the target location for any config file that is dynamically rendered as part of the pipeline. <name>_queries.yaml \u00b6 As part of the post-processing phase it is likely that some of the used configuration file or processor templates will have to read the Elasticsearch database. This can either be done through an EQL or DSL query, both syntax types can result long query definitions. To keep template files and processor configurations brief we recommend putting Elasticsearch queries in separate files and load the query definitions when needed as part of the template or processor context. logs.yaml \u00b6 The logs.yaml file is used to define which of the gathered log files should be parsed and what meta information should be added to them. The file is structured into two main fields servers and groups mirroring the Ansible group and host vars structure. Log file configurations can be either assigned to a host (server) directly or indirectly via the groups it belongs to. See the Logstash Log Config model for details on the configuration format for a single log file. While it is possible to integrate this information as part of the testbeds TIM group and host vars we recommend using the logs.yaml file, because it ensures all Dataset processing configuration can be viewed and accessed in a single location. Host and group vars could only be inspected in the servers fact files (which are very large and hard to read) or after pre-processing once the facts have been curated. logstash \u00b6 The logstash directory is used for both storing the Logstash parser configuration as well as files generated at runtime (e.g., Logstash logs). conf.d \u00b6 The conf.d directory contains all Logstash parsing configuration files that should be loaded i.e., input, output and filter definitions. Note that if you use the Logstash setup processor input and output configuration are automatically generated and you only have to define filter definitions used to parse raw logs into structured data. 0000_pre_process.conf * \u00b6 A special Logstash filter configuration file containing filters used to bootstrap log events for parsing. For example patching line numbers into log events is done by a ruby filter configured in this file. input.conf * \u00b6 Logstash input configuration file containing all log file inputs and their input settings. output.conf * \u00b6 Logstash output configuration containing the elasticsearch output settings and an optional file output used to store parsed versions of the log files (only used for logs configured with save_parsed: true ). <log type>.conf \u00b6 To configure how log data is parsed it is recommended to create a Logstash filter configuration file for each log type. This makes it easier to edit, update and read parsing configuration as each file will be smaller and less complicated than having a single file containing filters for all log data. data ** \u00b6 The Logstash runtime directory used to store state und plugin data such as pointers for currently processed files. log ** \u00b6 The Logstash runtime logs directory. jvm.options * \u00b6 Options file for the Java Virtual Machine (JVM) used to run Logstash. These options can be used to e.g., change the amount of RAM used by Logstash. log4j2.properties * \u00b6 Logstash runtime logger configuration file. logstash.yml * \u00b6 The Logstash main configuration file used to set the data and log runtime directories. pipelines.yml * \u00b6 The Logstash pipeline configuration used to configure the processing mode and options. This is generated by the Logstash setup processor and the config is setup so that Logstash runs in single thread mode as to preserve the log line order. <name>-index-template.json \u00b6 While by default Elasticsearch will try to automatically create correct field mappings (i.e., type definitions) for the data fields created through the parsing process, but this is not very efficient if there are many different fields or fields for which Elasticsearch would produce an incorrect mapping. Also when using Eql Sequence Rules all referenced fields must have a defined field mapping or otherwise the underlying EQL query will result in an error. This can be problematic if one of the reference fields is optional (i.e., might not occur in all dataset instances) no automatic field mapping would be created. Thus it is recommend to pre-define the index field mappings using index templates . Each of the created index template files must be imported using the Index Template processor during the pre-processing phase. <name>-ingest.yaml \u00b6 As an alternative to configuring Logstash filters for parsing log data it is also possible to define Ingest Pipelines on the Elasticsearch server. This way the log data processing is handled by the Elasticsearch server upon receiving the log event. Each of the created ingest pipeline configurations must be imported into the Elasticsearch server during the pre-processing phase using the Ingest Pipeline processor . Logs can then be marked for processing by using the add_field configuration option. add_field : \"[@metadata][pipeline]\" : \"<pipeline id>\" templates \u00b6 The templates directory should be used to store all template files used in the processing pipeline. rules \u00b6 The rules directory should contain all labeling rule templates to be rendered during the post-processing phase. <rule class>.yaml.j2 \u00b6 We recommend to split labeling rule templates in multiple files based on the log class/file they are written for. For example, if you write labeling rules for Apache server logs you might want to put them in a file called apache.yaml.j2 . During the labeling phase rules are applied in lexicographical order so you might want to use numerical prefixes to ensure a certain rule file is applied before another. attacker \u00b6 One of the main inputs for labeling rule templates are the attacker facts and logs, because of this we recommend to keep attacker related config templates in a separate directory. This makes it easier for other people to read your processing-pipeline configuration. attacker.yaml.j2 \u00b6 Further we recommend that you define general attacker information template for rendering basic attacker data (e.g., IP addresses, users, etc.). Should you have more than one attacker you might want to render the file multiple times. <phase name>.yaml.j2 \u00b6 Also it is recommend to prepare a configuration template for each of the define attack phases for rendering phase specific information such as execution timestamps (parsed from the attacker logs) or dynamically chosen execution parameters (e.g. executed CLI commands). groups.json.j2 \u00b6 While the host group information can also be read directly from the fact files, we recommend to curate this information into a simple groups.json file using a template processor. This makes the data easier to use in subsequent processors. servers.json.j2 \u00b6 Similarly we recommend curating the server facts into a file only containing information necessary for dataset processing or documentation. This makes it easier to access the information as well as easier to understand for a human inspecting the resulting dataset.","title":"Dataset Processing"},{"location":"examples/process/#processing-example","text":"The example directory contains an example directory layout and mini configuration for dataset processing using the Cyber Range Kyoushi Dataset CLI tool. While the tool itself does not enforce any specific directory or file layout, other than configuring the processing and parsing pipeline through the process.yaml file, we recommend following the structure presented here as it promotes separating processing into multiple steps making each step smaller and easier to understand. Note The layout presented here is also reflected by many of the default values of configuration options.","title":"Processing Example "},{"location":"examples/process/#layout","text":"process.yaml config <name>_queries.yaml logs.yaml logstash conf.d 0000_pre_process.conf * input.conf * output.conf * <log type>.conf data ** log ** jvm.options * log4j2.properties * logstash.yml * pipelines.yml * <name>-index-template.json <name>-ingest.yaml templates rules <rule class>.yaml.j2 attacker attacker.yaml.j2 <phase name>.yaml.j2 groups.json.j2 servers.json.j2 Note * These files and directories are automatically generated by the Logstash setup processor . Note ** By default the logstash/data and logstash/log directories are used as Logstash runtime directories for storing current state, temporary files and generated logs.","title":"Layout "},{"location":"examples/process/#processyaml","text":"The processing and parsing configuration.","title":"process.yaml"},{"location":"examples/process/#config","text":"The config directory should be used to store any static configuration files to be used or loaded as part of the dataset processing pipeline. Note that we also highly recommend to make this directory or a sub directory of it the target location for any config file that is dynamically rendered as part of the pipeline.","title":"config"},{"location":"examples/process/#name_queriesyaml","text":"As part of the post-processing phase it is likely that some of the used configuration file or processor templates will have to read the Elasticsearch database. This can either be done through an EQL or DSL query, both syntax types can result long query definitions. To keep template files and processor configurations brief we recommend putting Elasticsearch queries in separate files and load the query definitions when needed as part of the template or processor context.","title":"&lt;name&gt;_queries.yaml"},{"location":"examples/process/#logsyaml","text":"The logs.yaml file is used to define which of the gathered log files should be parsed and what meta information should be added to them. The file is structured into two main fields servers and groups mirroring the Ansible group and host vars structure. Log file configurations can be either assigned to a host (server) directly or indirectly via the groups it belongs to. See the Logstash Log Config model for details on the configuration format for a single log file. While it is possible to integrate this information as part of the testbeds TIM group and host vars we recommend using the logs.yaml file, because it ensures all Dataset processing configuration can be viewed and accessed in a single location. Host and group vars could only be inspected in the servers fact files (which are very large and hard to read) or after pre-processing once the facts have been curated.","title":"logs.yaml"},{"location":"examples/process/#logstash","text":"The logstash directory is used for both storing the Logstash parser configuration as well as files generated at runtime (e.g., Logstash logs).","title":"logstash"},{"location":"examples/process/#confd","text":"The conf.d directory contains all Logstash parsing configuration files that should be loaded i.e., input, output and filter definitions. Note that if you use the Logstash setup processor input and output configuration are automatically generated and you only have to define filter definitions used to parse raw logs into structured data.","title":"conf.d"},{"location":"examples/process/#data","text":"The Logstash runtime directory used to store state und plugin data such as pointers for currently processed files.","title":"data**"},{"location":"examples/process/#log","text":"The Logstash runtime logs directory.","title":"log**"},{"location":"examples/process/#jvmoptions","text":"Options file for the Java Virtual Machine (JVM) used to run Logstash. These options can be used to e.g., change the amount of RAM used by Logstash.","title":"jvm.options*"},{"location":"examples/process/#log4j2properties","text":"Logstash runtime logger configuration file.","title":"log4j2.properties*"},{"location":"examples/process/#logstashyml","text":"The Logstash main configuration file used to set the data and log runtime directories.","title":"logstash.yml*"},{"location":"examples/process/#pipelinesyml","text":"The Logstash pipeline configuration used to configure the processing mode and options. This is generated by the Logstash setup processor and the config is setup so that Logstash runs in single thread mode as to preserve the log line order.","title":"pipelines.yml*"},{"location":"examples/process/#name-index-templatejson","text":"While by default Elasticsearch will try to automatically create correct field mappings (i.e., type definitions) for the data fields created through the parsing process, but this is not very efficient if there are many different fields or fields for which Elasticsearch would produce an incorrect mapping. Also when using Eql Sequence Rules all referenced fields must have a defined field mapping or otherwise the underlying EQL query will result in an error. This can be problematic if one of the reference fields is optional (i.e., might not occur in all dataset instances) no automatic field mapping would be created. Thus it is recommend to pre-define the index field mappings using index templates . Each of the created index template files must be imported using the Index Template processor during the pre-processing phase.","title":"&lt;name&gt;-index-template.json"},{"location":"examples/process/#name-ingestyaml","text":"As an alternative to configuring Logstash filters for parsing log data it is also possible to define Ingest Pipelines on the Elasticsearch server. This way the log data processing is handled by the Elasticsearch server upon receiving the log event. Each of the created ingest pipeline configurations must be imported into the Elasticsearch server during the pre-processing phase using the Ingest Pipeline processor . Logs can then be marked for processing by using the add_field configuration option. add_field : \"[@metadata][pipeline]\" : \"<pipeline id>\"","title":"&lt;name&gt;-ingest.yaml"},{"location":"examples/process/#templates","text":"The templates directory should be used to store all template files used in the processing pipeline.","title":"templates"},{"location":"examples/process/#rules","text":"The rules directory should contain all labeling rule templates to be rendered during the post-processing phase.","title":"rules"},{"location":"examples/process/#attacker","text":"One of the main inputs for labeling rule templates are the attacker facts and logs, because of this we recommend to keep attacker related config templates in a separate directory. This makes it easier for other people to read your processing-pipeline configuration.","title":"attacker"},{"location":"examples/process/#groupsjsonj2","text":"While the host group information can also be read directly from the fact files, we recommend to curate this information into a simple groups.json file using a template processor. This makes the data easier to use in subsequent processors.","title":"groups.json.j2"},{"location":"examples/process/#serversjsonj2","text":"Similarly we recommend curating the server facts into a file only containing information necessary for dataset processing or documentation. This makes it easier to access the information as well as easier to understand for a human inspecting the resulting dataset.","title":"servers.json.j2"},{"location":"getting_started/intro/","text":"Getting Started \u00b6 The Cyber Range Kyoushi Dataset tool implements the processing layer for the Kyoushi model-driven IDS dataset generation and labeling framework as described by Frank [Frank21] and shown in Figure 1. Dataset definition and generation are handled by the Model , Testbed and Data Collection layers. Figure 1: Cyber Range Kyoushi System Layers The processing layer takes a raw dataset (logs and facts) and model definition as input for processing to create IDS labels for the log events contained in the dataset. This is done in a 5 step processes implemented by the Cyber Range Kyoushi Dataset tool: Prepare Process Pre-Process Parse Post-Process Label Additionally the tool also implements a CLI command that can be used for sampling a labeled Kyoushi dataset (also see the CLI reference ). References \u00b6 Frank21: Frank, M. Quality improvement of labels for model-driven benchmark data generation for intrusion detection systems. (2021) doi: 0.34726/HSS.2021.82646 .","title":"Intro"},{"location":"getting_started/intro/#getting-started","text":"The Cyber Range Kyoushi Dataset tool implements the processing layer for the Kyoushi model-driven IDS dataset generation and labeling framework as described by Frank [Frank21] and shown in Figure 1. Dataset definition and generation are handled by the Model , Testbed and Data Collection layers. Figure 1: Cyber Range Kyoushi System Layers The processing layer takes a raw dataset (logs and facts) and model definition as input for processing to create IDS labels for the log events contained in the dataset. This is done in a 5 step processes implemented by the Cyber Range Kyoushi Dataset tool: Prepare Process Pre-Process Parse Post-Process Label Additionally the tool also implements a CLI command that can be used for sampling a labeled Kyoushi dataset (also see the CLI reference ).","title":"Getting Started"},{"location":"getting_started/label/","text":"Label Dataset ( cr-kyoushi-dataset label ) \u00b6 The final dataset processing step is the application of labels to log events. The Cyber Range Kyoushi Dataset tool implements this step through the label command. This command reads rendered labeling rules and applies them to the dataset by executing the labeling logic (e.g., Elasticsearch queries) and applying the given labels to the returned log rows. Note for this to be possible all templated labeling rules must be rendered during the post-processing phase of the process command. The Cyber Range Kyoushi Dataset tool implements four labeling rule types that can be used to configure how and which logs are labeled. See Labeling rules for an overview and example rules. Note that should the four provided labeling rule types not be sufficient for labeling a certain dataset, new rules implementing different labeling logic can be added.","title":"Label Dataset"},{"location":"getting_started/label/#label-dataset-cr-kyoushi-dataset-label","text":"The final dataset processing step is the application of labels to log events. The Cyber Range Kyoushi Dataset tool implements this step through the label command. This command reads rendered labeling rules and applies them to the dataset by executing the labeling logic (e.g., Elasticsearch queries) and applying the given labels to the returned log rows. Note for this to be possible all templated labeling rules must be rendered during the post-processing phase of the process command. The Cyber Range Kyoushi Dataset tool implements four labeling rule types that can be used to configure how and which logs are labeled. See Labeling rules for an overview and example rules. Note that should the four provided labeling rule types not be sufficient for labeling a certain dataset, new rules implementing different labeling logic can be added.","title":"Label Dataset (cr-kyoushi-dataset label)"},{"location":"getting_started/prepare/","text":"Prepare Dataset ( cr-kyoushi-dataset prepare ) \u00b6 The prepare command is used to merge the defined TIM dataset processing configuration with the gathered logs and facts of a TSM. Additionally it is also used to configure dataset specific options, such as, capture time frame or the dataset name (the command will prompt for these options, but they can also be set as CLI options). The command will copy both parts into the current work directory using the Cyber Range Kyoushi dataset directory layout. Cyber Range Kyoushi Dataset Layout \u00b6 dataset.yaml : The dataset configuration file containing dataset config options (e.g., name) gather : The gather directory containing collected logs, configs and facts per testbed host. <host name> facts.json : The facts file containing all gathered facts for the host in JSON format. configs : Directory containing all the system and software configuration files gathered from the host. logs : Directory containing all the log file gathered from the host. ... processing : The processing directory containing all the files and configuration for the processing and parsing pipeline. process.yaml : The processing pipeline configuration file. rules : The labeling rules directory containing all Cyber Range Kyoushi labeling rules. Note that this directory should only contain rendered labeling rules. Templated labeling rules should be stored in the processing directory and the processing pipeline should be configured to save rendered versions into the rules directory. labels : The labels directory mirrors the gather directories logs. It is used to store labeling information in NDJSON format and will be populated at the end of the labeling step. <host name> logs ... Example Usage \u00b6 cr-kyoushi-dataset prepare demo","title":"Prepare Dataset"},{"location":"getting_started/prepare/#prepare-dataset-cr-kyoushi-dataset-prepare","text":"The prepare command is used to merge the defined TIM dataset processing configuration with the gathered logs and facts of a TSM. Additionally it is also used to configure dataset specific options, such as, capture time frame or the dataset name (the command will prompt for these options, but they can also be set as CLI options). The command will copy both parts into the current work directory using the Cyber Range Kyoushi dataset directory layout.","title":"Prepare Dataset (cr-kyoushi-dataset prepare)"},{"location":"getting_started/prepare/#cyber-range-kyoushi-dataset-layout","text":"dataset.yaml : The dataset configuration file containing dataset config options (e.g., name) gather : The gather directory containing collected logs, configs and facts per testbed host. <host name> facts.json : The facts file containing all gathered facts for the host in JSON format. configs : Directory containing all the system and software configuration files gathered from the host. logs : Directory containing all the log file gathered from the host. ... processing : The processing directory containing all the files and configuration for the processing and parsing pipeline. process.yaml : The processing pipeline configuration file. rules : The labeling rules directory containing all Cyber Range Kyoushi labeling rules. Note that this directory should only contain rendered labeling rules. Templated labeling rules should be stored in the processing directory and the processing pipeline should be configured to save rendered versions into the rules directory. labels : The labels directory mirrors the gather directories logs. It is used to store labeling information in NDJSON format and will be populated at the end of the labeling step. <host name> logs ...","title":"Cyber Range Kyoushi Dataset Layout"},{"location":"getting_started/prepare/#example-usage","text":"cr-kyoushi-dataset prepare demo","title":"Example Usage"},{"location":"getting_started/process/","text":"Process Dataset ( cr-kyoushi-dataset process ) \u00b6 Before a dataset can be labeled we have to process the raw unstructured information into a format on which we can easily apply reasoning logic (i.e., the labeling rules). The Cyber Range Kyoushi framework implements this process through the Cyber Range Kyoushi Dataset CLI tools process command. The command implements a 3 step processing pipeline Pre-Process Parse Post-Process to first prepare raw datasets processing, then parse the log events using Logstash to store structured data in an Elasticsearch database. Finally use the generated knowledge base to make final adjustments (e.g., trimming the dataset to only relevant time frames) and rendering the labeling rules. While the parse step is handled by the third party software Logstash the pre- and post-processing steps are implemented by the Dataset tool directly. For these so called Processors are used to configure and perform specific processing actions (e.g., decompressing GZip files, rendering templates, etc.). Configuration of processing pipeline (i.e., the processors and the Logstash parser) is done through in the processing/process.yaml . The processing directory also contains all other extra configuration, context, template, etc. files and information necessary to process raw datasets created as results of instantiating a TIM and executing the resulting TSM . Configuration \u00b6 As mentioned above pipeline.yml defines the three step processing pipeline. For this each step has its own configuration key i.e., pre_processors : The list of processors to sequentially execute during the pre-processing step. parser : Is the Cyber Range Kyoushi Dataset logstash parser configuration (i.e., how logstash should be executed). See below for details. post_processors : The list of processors to sequentially execute during the post-processing step. Processors \u00b6 A processor is similar to an Ansible module. That is a processor is represented by a partially Jinja2 templatable YAML configuration and is used to execute specific actions on a system. Meaning a processor is used to define what and how an action should be executed. The pre_processors and post_processors configuration each define a list of `processors that should be executed in the defined order during the pre-processing and post-processing steps respectively. Not all actions necessary for processing a dataset can be defined statically, in some cases it might be necessary to execute specific actions based on the dataset contents. For example, if you want to render a specific file for each command executed by a simulated attacker (information that can only be obtained after the raw dataset has been created). To support this and similar use cases the processor configs can contain Jinja2 template strings making it possible to define partial processing pipeline configurations that are dynamically rendered during pipeline execution. Every processor has at least the following basic configuration fields in addition to any specific fields, i.e., configuration specific to the processor action. type : This is a special field used to indicate the processor that is being configured e.g., setting it to print would result in executing the Print-Processor. name : A textual description of the action that should be achieved by executing the configured processor. This is used to ensure that each processor configuration entry has a minimum documentation as well as for outputting the current state during execution. context : The template render context to use for rendering any Jinja2 template string used as part of the processor definition. It is possible to either define inline variables or load them from YAML or JSON files. variables : Dictionary of context variables to use during processor rendering. variable_files : Dictionary of var files containing context variables to use during processor rendering. Note Context variables and the contents of variable_files are merged into a single context dictionary. Variable files take override inline variables and the priority between variable files is based on their dictionary position i.e., variable files defined first are overridden by those defined later. Hint See the Dataset Processors section for a list of all processors shipped as part of the Cyber Range Kyoushi Dataset tool. Parser Config \u00b6 The parser configuration is used to configure how the Logstash parser is executed. That is you can use it to define some of the command line arguments used to run Logstash. The more complex configuration options, such as, Logstash filter configurations must be done through normal Logstash configuration syntax. By default Logstash will use the <dataset>/processing/logstash directory as its main directory for storing both configuration files and runtime data (e.g., log files), but this behavior can be change by setting the appropriate parser options. See the Logstash Parser Configuration Model for details on the available options. Example Config \u00b6 The below example process.yml shows the configuration for a processing pipeline with 1 pre-processor, 1 post-processor and a parser configured to print debug output. Warning The shown configuration is just a bare minimum configuration example and does not configure any meaning full dataset processing on its own. Both shown processor use Jinja2 and the context field to define partial configurations that are rendered dynamically during pipeline execution. pre_processors : - name : Prepare server facts type : template context : var_files : processing/config/groups.yaml template_context : vars : exclude_groups : [ \"Region.*\" , \"envvars.*\" , \"instance-.*\" , \"meta-.*\" , \"nova\" ] exclude_interfaces : [ \"lo\" , 'tun\\d*' ] servers : \"{{ all }}\" var_files : | { {% for server in all %} \"server_logs\": \"processing/config/logs.yaml\", \"{{ server }}\": \"gather/{{ server }}/facts.json\"{% if not loop.last %},{% endif %} {% endfor %} } src : processing/templates/servers.json.j2 dest : processing/config/servers.yaml parser : settings_dir : processing/logstash conf_dir : processing/logstash/conf.d log_level : debug log_dir : processing/logstash/log completed_log : processing/logstash/log/file-completed.log data_dir : processing/logstash/data parsed_dir : parsed save_parsed : false post_processors : - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml indices : | [ {% for server in groups[\"servers\"] %} \"*-{{ server }}\"{% if not loop.last %},{% endif %} {% endfor %} ] Pre Processing \u00b6 The pre-processing phase can be used to prepare your raw dataset for parsing and data storage with Logstash and Elasticsearch. This could, for example, involve converting a binary data type (e.g., network captures in PCAP format) into a text format that can be processed by Logstash. See the processors reference for an overview of processors. Parsing \u00b6 Below we will give a brief introduction on how to configure the way raw log data is parsed into structured data. As the Cyber Range Kyoushi Dataset tool uses Logstash as its parsing component and Elasticsearch for storage we recommend to consult the Logstash and Elasticsearch documentation for more extensive explanations of available features and configurations. The Cyber Range Kyoushi Dataset tool basically supports two ways of configuring and processing log dissection. Either Logstash filters processed by Logstash or using Elasticsearch ingest pipelines . Logstash Filters \u00b6 Logstash filters can be used to parse log data directly with Logstash. See the Logstash documentation for an overview of available filters. Filters must be configured in Logstash conf files (default conf dir <dataset>/processing/logstash/conf.d/ ) in so called filter blocks. The below example shows a grok filter used to dissect OpenVPN logs. filter { if [ type ] == \"openvpn\" { grok { match => { \"message\" => [ \"%{OPENVPN_BASE} peer info: %{OPENVPN_PEER_INFO}\" , \"%{OPENVPN_BASE} VERIFY EKU %{GREEDYDATA:[openvpn][verify][eku][status]}\" , \"%{OPENVPN_BASE} VERIFY KU %{GREEDYDATA:[openvpn][verify][ku][status]}\" , \"%{OPENVPN_BASE} VERIFY %{DATA:[openvpn][verify][status]}: depth=%{NONNEGINT:[openvpn][verify][depth]:int}, %{GREEDYDATA:[openvpn][peer][cert][info]}\" , \"%{OPENVPN_BASE} (?<message>MULTI: Learn: %{IP:[destination][ip]} -> %{OPENVPN_USER}/%{OPENVPN_CONNECTION})\" , \"%{OPENVPN_BASE} (?<message>MULTI: primary virtual IP for %{OPENVPN_USER}/%{OPENVPN_CONNECTION}: %{IP:[destination][ip]})\" , \"%{OPENVPN_BASE} (?<message>MULTI_sva: pool returned IPv4=%{OPENVPN_POOL_RETURN:[openvpn][pool][return][ipv4]}, IPv6=%{OPENVPN_POOL_RETURN:[openvpn][pool][return][ipv6]})\" , \"%{OPENVPN_BASE} (?<message>MULTI: new connection by client '%{USERNAME:[openvpn][peer][duplicate]}' will cause previous active sessions by this client to be dropped. Remember to use the --duplicate-cn option if you want multiple clients using the same certificate or username to concurrently connect.)\" , \"%{OPENVPN_BASE} %{OPENVPN_PUSH:message}\" , \"%{OPENVPN_BASE} %{OPENVPN_SENT_CONTROL:message}\" , \"%{OPENVPN_BASE} %{OPENVPN_DATA_CHANNEL:message}\" , \"%{OPENVPN_BASE} \\[UNDEF\\] %{GREEDYDATA:message}\" , \"%{OPENVPN_BASE} \\[%{OPENVPN_USER}\\] %{GREEDYDATA:message}\" , \"%{OPENVPN_BASE} %{GREEDYDATA:message}\" ] } pattern_definitions => { \"OPENVPN_PUSH\" => \"(PUSH: %{GREEDYDATA:[openvpn][push][message]})\" \"OPENVPN_SENT_CONTROL\" => \"(SENT CONTROL \\[%{USERNAME:[openvpn][control][user]}\\]: '%{DATA:[openvpn][control][message]}' \\(status=%{INT:[openvpn][control][status]:int}\\))\" \"OPENVPN_DATA_CHANNEL\" => \"(%{NOTSPACE:[openvpn][data][channel]} Data Channel: %{GREEDYDATA:[openvpn][data][message]})\" \"OPENVPN_POOL_RETURN\" => \"(%{IP:[openvpn][pool][returned]}|\\(Not enabled\\))\" \"OPENVPN_TIMESTAMP\" => \"%{YEAR}-%{MONTHNUM2}-%{MONTHDAY} %{TIME}\" \"OPENVPN_USER\" => \"%{USERNAME:[source][user][name]}\" \"OPENVPN_CONNECTION\" => \"(%{IP:[source][ip]}:%{POSINT:[source][port]:int})\" \"OPENVPN_PEER_INFO\" => \"%{GREEDYDATA:[openvpn][peer][info][field]}=%{GREEDYDATA:[openvpn][peer][info][value]}\" \"OPENVPN_BASE\" => \"%{OPENVPN_TIMESTAMP:timestamp}( %{OPENVPN_USER}/)?( \\s ?%{OPENVPN_CONNECTION})?\" } overwrite => [ \"message\" ] } } } Ingest Pipelines \u00b6 In contrast to Logstash filters which are directly processed by Logstash using an ingest pipeline means that log data is parsed upon ingestion into Elasticsearch. Ingest pipelines use so ingest processors for parsing log data. There are many different types of ingest processors available, please consult the Elasticsearch documentation for more details. The below code shows a simple pipeline definition (in YAML notation) using only a single grok processor. description : Pipeline for parsing Linux auditd logs processors : - grok : field : message pattern_definitions : AUDIT_TYPE : \"type=%{NOTSPACE:auditd.log.record_type}\" AUDIT_NODE : \"node=%{IPORHOST:auditd.log.node} \" AUDIT_PREFIX : \"^(?:%{AUDIT_NODE})?%{AUDIT_TYPE} msg=audit\\\\(%{NUMBER:auditd.log.epoch}:%{NUMBER:auditd.log.sequence}\\\\):(%{DATA})?\" AUDIT_KEY_VALUES : \"%{WORD}=%{GREEDYDATA}\" ANY : \".*\" patterns : - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv} old auid=%{NUMBER:auditd.log.old_auid} new auid=%{NUMBER:auditd.log.new_auid} old ses=%{NUMBER:auditd.log.old_ses} new ses=%{NUMBER:auditd.log.new_ses}\" - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv} msg=['\\\"]([^=]*\\\\s)?%{ANY:auditd.log.sub_kv}['\\\"]\" - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv}\" - \"%{AUDIT_PREFIX}\" - \"%{AUDIT_TYPE} %{AUDIT_KEY_VALUES:auditd.log.kv}\" To configure logs to use an ingest pipeline you have to set the log config to add the @metadata.pipeline field (assuming the Logstash Setup processor is used). add_field : \"[@metadata][pipeline]\" : \"auditd-logs\" Note that you also have to use the Ingest Pipeline processor as part of your pre-processing configuration to setup the pipeline before the parsing phase. Index Templates \u00b6 While by default Elasticsearch will try to automatically create correct field mappings (i.e., type definitions) for the data fields created through the parsing process, but this is not very efficient if there are many different fields or fields for which Elasticsearch would produce an incorrect mapping. Also when using Eql Sequence Rules all referenced fields must have a defined field mapping or otherwise the underlying EQL query will result in an error. This can be problematic if one of the reference fields is optional (i.e., might not occur in all dataset instances) no automatic field mapping would be created. Thus it is recommend to pre-define the index field mappings using index templates . Each of the created index template files must be imported using the Index Template processor during the pre-processing phase. Post Processing \u00b6 The post-processing phase occurs after all logs have been parsed and stored in a structured data format in Elasticsearch. Thus processors configured to be executed in the post-processing phase can use log data stored in Elasticsearch as configuration input or as part of Jinja2 template logic. This can be done through the special query objects Search and EQL exposed as part of the Jinja2 template context. These query objects can be used to execute Elasticsearch DSL queries ( Search ) or EQL queries ( EQL ). The code snippet below shows example usage for both query objects. Note that the query body is read from a context variable for brevities sake. {% - set wp_cracked = Search ( index = \"kyoushi-attacker_0\" ) .query ( queries.escalate.wp_cracked ) .source ([ \"@timestamp\" ]) .extra ( size = 1 ) .execute () .hits.hits - %} {% - set vpn_disconnect = EQL ( index = \"kyoushi-attacker_0\" , body = queries.escalate.vpn_disconnect )[ \"hits\" ][ \"sequences\" ][ 0 ][ \"events\" ] - %} Additionally we also expose the following query objects, which can be used to define DSL queries to be executed by Search . See the for more details. Q : Can be used to define arbitrary DSL queries. Q_ALL : Can be used to define a DSL query with multiple search terms using the same operator (e.g., match ) connected by and clauses. Q_MATCH_ALL : Same as Q_ALL , but the operator is always match . Q_TERM_ALL : Same as Q_ALL , but the operator is always term . Hint Also see the Elasticsearch DSL Python API search and queries doc for more details on the query object Search , Q , Q_ALL , Q_MATCH_ALL and Q_TERM_ALL .","title":"Process Dataset"},{"location":"getting_started/process/#process-dataset-cr-kyoushi-dataset-process","text":"Before a dataset can be labeled we have to process the raw unstructured information into a format on which we can easily apply reasoning logic (i.e., the labeling rules). The Cyber Range Kyoushi framework implements this process through the Cyber Range Kyoushi Dataset CLI tools process command. The command implements a 3 step processing pipeline Pre-Process Parse Post-Process to first prepare raw datasets processing, then parse the log events using Logstash to store structured data in an Elasticsearch database. Finally use the generated knowledge base to make final adjustments (e.g., trimming the dataset to only relevant time frames) and rendering the labeling rules. While the parse step is handled by the third party software Logstash the pre- and post-processing steps are implemented by the Dataset tool directly. For these so called Processors are used to configure and perform specific processing actions (e.g., decompressing GZip files, rendering templates, etc.). Configuration of processing pipeline (i.e., the processors and the Logstash parser) is done through in the processing/process.yaml . The processing directory also contains all other extra configuration, context, template, etc. files and information necessary to process raw datasets created as results of instantiating a TIM and executing the resulting TSM .","title":"Process Dataset (cr-kyoushi-dataset process)"},{"location":"getting_started/process/#configuration","text":"As mentioned above pipeline.yml defines the three step processing pipeline. For this each step has its own configuration key i.e., pre_processors : The list of processors to sequentially execute during the pre-processing step. parser : Is the Cyber Range Kyoushi Dataset logstash parser configuration (i.e., how logstash should be executed). See below for details. post_processors : The list of processors to sequentially execute during the post-processing step.","title":"Configuration"},{"location":"getting_started/process/#processors","text":"A processor is similar to an Ansible module. That is a processor is represented by a partially Jinja2 templatable YAML configuration and is used to execute specific actions on a system. Meaning a processor is used to define what and how an action should be executed. The pre_processors and post_processors configuration each define a list of `processors that should be executed in the defined order during the pre-processing and post-processing steps respectively. Not all actions necessary for processing a dataset can be defined statically, in some cases it might be necessary to execute specific actions based on the dataset contents. For example, if you want to render a specific file for each command executed by a simulated attacker (information that can only be obtained after the raw dataset has been created). To support this and similar use cases the processor configs can contain Jinja2 template strings making it possible to define partial processing pipeline configurations that are dynamically rendered during pipeline execution. Every processor has at least the following basic configuration fields in addition to any specific fields, i.e., configuration specific to the processor action. type : This is a special field used to indicate the processor that is being configured e.g., setting it to print would result in executing the Print-Processor. name : A textual description of the action that should be achieved by executing the configured processor. This is used to ensure that each processor configuration entry has a minimum documentation as well as for outputting the current state during execution. context : The template render context to use for rendering any Jinja2 template string used as part of the processor definition. It is possible to either define inline variables or load them from YAML or JSON files. variables : Dictionary of context variables to use during processor rendering. variable_files : Dictionary of var files containing context variables to use during processor rendering. Note Context variables and the contents of variable_files are merged into a single context dictionary. Variable files take override inline variables and the priority between variable files is based on their dictionary position i.e., variable files defined first are overridden by those defined later. Hint See the Dataset Processors section for a list of all processors shipped as part of the Cyber Range Kyoushi Dataset tool.","title":"Processors"},{"location":"getting_started/process/#parser-config","text":"The parser configuration is used to configure how the Logstash parser is executed. That is you can use it to define some of the command line arguments used to run Logstash. The more complex configuration options, such as, Logstash filter configurations must be done through normal Logstash configuration syntax. By default Logstash will use the <dataset>/processing/logstash directory as its main directory for storing both configuration files and runtime data (e.g., log files), but this behavior can be change by setting the appropriate parser options. See the Logstash Parser Configuration Model for details on the available options.","title":"Parser Config"},{"location":"getting_started/process/#example-config","text":"The below example process.yml shows the configuration for a processing pipeline with 1 pre-processor, 1 post-processor and a parser configured to print debug output. Warning The shown configuration is just a bare minimum configuration example and does not configure any meaning full dataset processing on its own. Both shown processor use Jinja2 and the context field to define partial configurations that are rendered dynamically during pipeline execution. pre_processors : - name : Prepare server facts type : template context : var_files : processing/config/groups.yaml template_context : vars : exclude_groups : [ \"Region.*\" , \"envvars.*\" , \"instance-.*\" , \"meta-.*\" , \"nova\" ] exclude_interfaces : [ \"lo\" , 'tun\\d*' ] servers : \"{{ all }}\" var_files : | { {% for server in all %} \"server_logs\": \"processing/config/logs.yaml\", \"{{ server }}\": \"gather/{{ server }}/facts.json\"{% if not loop.last %},{% endif %} {% endfor %} } src : processing/templates/servers.json.j2 dest : processing/config/servers.yaml parser : settings_dir : processing/logstash conf_dir : processing/logstash/conf.d log_level : debug log_dir : processing/logstash/log completed_log : processing/logstash/log/file-completed.log data_dir : processing/logstash/data parsed_dir : parsed save_parsed : false post_processors : - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml indices : | [ {% for server in groups[\"servers\"] %} \"*-{{ server }}\"{% if not loop.last %},{% endif %} {% endfor %} ]","title":"Example Config"},{"location":"getting_started/process/#pre-processing","text":"The pre-processing phase can be used to prepare your raw dataset for parsing and data storage with Logstash and Elasticsearch. This could, for example, involve converting a binary data type (e.g., network captures in PCAP format) into a text format that can be processed by Logstash. See the processors reference for an overview of processors.","title":"Pre Processing"},{"location":"getting_started/process/#parsing","text":"Below we will give a brief introduction on how to configure the way raw log data is parsed into structured data. As the Cyber Range Kyoushi Dataset tool uses Logstash as its parsing component and Elasticsearch for storage we recommend to consult the Logstash and Elasticsearch documentation for more extensive explanations of available features and configurations. The Cyber Range Kyoushi Dataset tool basically supports two ways of configuring and processing log dissection. Either Logstash filters processed by Logstash or using Elasticsearch ingest pipelines .","title":"Parsing"},{"location":"getting_started/process/#logstash-filters","text":"Logstash filters can be used to parse log data directly with Logstash. See the Logstash documentation for an overview of available filters. Filters must be configured in Logstash conf files (default conf dir <dataset>/processing/logstash/conf.d/ ) in so called filter blocks. The below example shows a grok filter used to dissect OpenVPN logs. filter { if [ type ] == \"openvpn\" { grok { match => { \"message\" => [ \"%{OPENVPN_BASE} peer info: %{OPENVPN_PEER_INFO}\" , \"%{OPENVPN_BASE} VERIFY EKU %{GREEDYDATA:[openvpn][verify][eku][status]}\" , \"%{OPENVPN_BASE} VERIFY KU %{GREEDYDATA:[openvpn][verify][ku][status]}\" , \"%{OPENVPN_BASE} VERIFY %{DATA:[openvpn][verify][status]}: depth=%{NONNEGINT:[openvpn][verify][depth]:int}, %{GREEDYDATA:[openvpn][peer][cert][info]}\" , \"%{OPENVPN_BASE} (?<message>MULTI: Learn: %{IP:[destination][ip]} -> %{OPENVPN_USER}/%{OPENVPN_CONNECTION})\" , \"%{OPENVPN_BASE} (?<message>MULTI: primary virtual IP for %{OPENVPN_USER}/%{OPENVPN_CONNECTION}: %{IP:[destination][ip]})\" , \"%{OPENVPN_BASE} (?<message>MULTI_sva: pool returned IPv4=%{OPENVPN_POOL_RETURN:[openvpn][pool][return][ipv4]}, IPv6=%{OPENVPN_POOL_RETURN:[openvpn][pool][return][ipv6]})\" , \"%{OPENVPN_BASE} (?<message>MULTI: new connection by client '%{USERNAME:[openvpn][peer][duplicate]}' will cause previous active sessions by this client to be dropped. Remember to use the --duplicate-cn option if you want multiple clients using the same certificate or username to concurrently connect.)\" , \"%{OPENVPN_BASE} %{OPENVPN_PUSH:message}\" , \"%{OPENVPN_BASE} %{OPENVPN_SENT_CONTROL:message}\" , \"%{OPENVPN_BASE} %{OPENVPN_DATA_CHANNEL:message}\" , \"%{OPENVPN_BASE} \\[UNDEF\\] %{GREEDYDATA:message}\" , \"%{OPENVPN_BASE} \\[%{OPENVPN_USER}\\] %{GREEDYDATA:message}\" , \"%{OPENVPN_BASE} %{GREEDYDATA:message}\" ] } pattern_definitions => { \"OPENVPN_PUSH\" => \"(PUSH: %{GREEDYDATA:[openvpn][push][message]})\" \"OPENVPN_SENT_CONTROL\" => \"(SENT CONTROL \\[%{USERNAME:[openvpn][control][user]}\\]: '%{DATA:[openvpn][control][message]}' \\(status=%{INT:[openvpn][control][status]:int}\\))\" \"OPENVPN_DATA_CHANNEL\" => \"(%{NOTSPACE:[openvpn][data][channel]} Data Channel: %{GREEDYDATA:[openvpn][data][message]})\" \"OPENVPN_POOL_RETURN\" => \"(%{IP:[openvpn][pool][returned]}|\\(Not enabled\\))\" \"OPENVPN_TIMESTAMP\" => \"%{YEAR}-%{MONTHNUM2}-%{MONTHDAY} %{TIME}\" \"OPENVPN_USER\" => \"%{USERNAME:[source][user][name]}\" \"OPENVPN_CONNECTION\" => \"(%{IP:[source][ip]}:%{POSINT:[source][port]:int})\" \"OPENVPN_PEER_INFO\" => \"%{GREEDYDATA:[openvpn][peer][info][field]}=%{GREEDYDATA:[openvpn][peer][info][value]}\" \"OPENVPN_BASE\" => \"%{OPENVPN_TIMESTAMP:timestamp}( %{OPENVPN_USER}/)?( \\s ?%{OPENVPN_CONNECTION})?\" } overwrite => [ \"message\" ] } } }","title":"Logstash Filters"},{"location":"getting_started/process/#ingest-pipelines","text":"In contrast to Logstash filters which are directly processed by Logstash using an ingest pipeline means that log data is parsed upon ingestion into Elasticsearch. Ingest pipelines use so ingest processors for parsing log data. There are many different types of ingest processors available, please consult the Elasticsearch documentation for more details. The below code shows a simple pipeline definition (in YAML notation) using only a single grok processor. description : Pipeline for parsing Linux auditd logs processors : - grok : field : message pattern_definitions : AUDIT_TYPE : \"type=%{NOTSPACE:auditd.log.record_type}\" AUDIT_NODE : \"node=%{IPORHOST:auditd.log.node} \" AUDIT_PREFIX : \"^(?:%{AUDIT_NODE})?%{AUDIT_TYPE} msg=audit\\\\(%{NUMBER:auditd.log.epoch}:%{NUMBER:auditd.log.sequence}\\\\):(%{DATA})?\" AUDIT_KEY_VALUES : \"%{WORD}=%{GREEDYDATA}\" ANY : \".*\" patterns : - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv} old auid=%{NUMBER:auditd.log.old_auid} new auid=%{NUMBER:auditd.log.new_auid} old ses=%{NUMBER:auditd.log.old_ses} new ses=%{NUMBER:auditd.log.new_ses}\" - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv} msg=['\\\"]([^=]*\\\\s)?%{ANY:auditd.log.sub_kv}['\\\"]\" - \"%{AUDIT_PREFIX} %{AUDIT_KEY_VALUES:auditd.log.kv}\" - \"%{AUDIT_PREFIX}\" - \"%{AUDIT_TYPE} %{AUDIT_KEY_VALUES:auditd.log.kv}\" To configure logs to use an ingest pipeline you have to set the log config to add the @metadata.pipeline field (assuming the Logstash Setup processor is used). add_field : \"[@metadata][pipeline]\" : \"auditd-logs\" Note that you also have to use the Ingest Pipeline processor as part of your pre-processing configuration to setup the pipeline before the parsing phase.","title":"Ingest Pipelines"},{"location":"getting_started/process/#index-templates","text":"While by default Elasticsearch will try to automatically create correct field mappings (i.e., type definitions) for the data fields created through the parsing process, but this is not very efficient if there are many different fields or fields for which Elasticsearch would produce an incorrect mapping. Also when using Eql Sequence Rules all referenced fields must have a defined field mapping or otherwise the underlying EQL query will result in an error. This can be problematic if one of the reference fields is optional (i.e., might not occur in all dataset instances) no automatic field mapping would be created. Thus it is recommend to pre-define the index field mappings using index templates . Each of the created index template files must be imported using the Index Template processor during the pre-processing phase.","title":"Index Templates"},{"location":"getting_started/process/#post-processing","text":"The post-processing phase occurs after all logs have been parsed and stored in a structured data format in Elasticsearch. Thus processors configured to be executed in the post-processing phase can use log data stored in Elasticsearch as configuration input or as part of Jinja2 template logic. This can be done through the special query objects Search and EQL exposed as part of the Jinja2 template context. These query objects can be used to execute Elasticsearch DSL queries ( Search ) or EQL queries ( EQL ). The code snippet below shows example usage for both query objects. Note that the query body is read from a context variable for brevities sake. {% - set wp_cracked = Search ( index = \"kyoushi-attacker_0\" ) .query ( queries.escalate.wp_cracked ) .source ([ \"@timestamp\" ]) .extra ( size = 1 ) .execute () .hits.hits - %} {% - set vpn_disconnect = EQL ( index = \"kyoushi-attacker_0\" , body = queries.escalate.vpn_disconnect )[ \"hits\" ][ \"sequences\" ][ 0 ][ \"events\" ] - %} Additionally we also expose the following query objects, which can be used to define DSL queries to be executed by Search . See the for more details. Q : Can be used to define arbitrary DSL queries. Q_ALL : Can be used to define a DSL query with multiple search terms using the same operator (e.g., match ) connected by and clauses. Q_MATCH_ALL : Same as Q_ALL , but the operator is always match . Q_TERM_ALL : Same as Q_ALL , but the operator is always term . Hint Also see the Elasticsearch DSL Python API search and queries doc for more details on the query object Search , Q , Q_ALL , Q_MATCH_ALL and Q_TERM_ALL .","title":"Post Processing"},{"location":"getting_started/sample/","text":"Sample Labels ( cr-kyoushi-dataset sample ) \u00b6 After a dataset has been both processed and labeled it is often necessary to verify it. As a dataset can contain millions of log lines manual verification of all labeled and unlabeled logs is most of the time impossible. Thus we recommend to verify random samples. For this, the Cyber Range Kyoushi Dataset tool provides the sample command (see the CLI Doc ). The command uses Elasticsearch queries with random function scores to sample processed datasets. The sample command can also be configured to retrieve additional log lines related to the samples, i.e., log lines from log sources other than the sample that occur roughly at the same time. This can be very useful when verifying logs as it allows one to easily cross check log events across multiple domains without having to manually search the related log rows. Hint The sample command can also be useful for when debugging during labeling rule development, since it can be used to quickly check the logs for which a certain label has been applied.","title":"Sample Labels"},{"location":"getting_started/sample/#sample-labels-cr-kyoushi-dataset-sample","text":"After a dataset has been both processed and labeled it is often necessary to verify it. As a dataset can contain millions of log lines manual verification of all labeled and unlabeled logs is most of the time impossible. Thus we recommend to verify random samples. For this, the Cyber Range Kyoushi Dataset tool provides the sample command (see the CLI Doc ). The command uses Elasticsearch queries with random function scores to sample processed datasets. The sample command can also be configured to retrieve additional log lines related to the samples, i.e., log lines from log sources other than the sample that occur roughly at the same time. This can be very useful when verifying logs as it allows one to easily cross check log events across multiple domains without having to manually search the related log rows. Hint The sample command can also be useful for when debugging during labeling rule development, since it can be used to quickly check the logs for which a certain label has been applied.","title":"Sample Labels (cr-kyoushi-dataset sample)"},{"location":"reference/config/","text":"Config module \u00b6 This module contains all configuration model definitions. DatasetConfig pydantic-model \u00b6 Configuration model for the dataset defintion. This model controls the attributes of the dataset (e.g., name) currently being processed. These configuration values are set during the dataset preparation phase. Examples: name : example-dataset start : 2021-10-10T12:00 end : 2021-10-12T12:00 end : datetime pydantic-field required \u00b6 The end time of the observation period. name : str pydantic-field required \u00b6 The name of the dataset. This is for example used as part of the elasticsearch index. start : datetime pydantic-field required \u00b6 The start time of the observation period. LogstashLogConfig pydantic-model \u00b6 Configuration model for to be parsed log files. This model is used to create a Logstash input configuration for raw dataset log files. Examples: - type : kyoushi codec : json path : sm.log* save_parse : false exclude : - * .gz - * .zip file_sort_direction : desc file_chunk_size : 320000 delimiter : tags : - statemachine - kyoushi add_field : '[@metadata][kyoushi][sm]' : user add_field : Any pydantic-field \u00b6 A dict of fields to add to each log event. codec : Union [ str , Dict [ str , Dict [ str , Any ]]] pydantic-field \u00b6 The file codec to use for reading. delimiter : str pydantic-field \u00b6 The newline delimiter (does not work for compressed files). exclude : Union [ str , List [ str ]] pydantic-field \u00b6 Glob/s to exclude from reading. file_chunk_size : int pydantic-field \u00b6 The size of the chunks to read from the file (in bytes). Default is 32kb set this to a higher value if your log file contains very long lines. file_sort_direction : typing_extensions . Literal [ 'asc' , 'desc' ] pydantic-field \u00b6 The sort direction for multiple files. path : Union [ str , List [ str ]] pydantic-field required \u00b6 The log file path/s to read. save_parsed : bool pydantic-field \u00b6 If this log should be saved to the disk after parsing. (Overrides parser.save_parsed) tags : str pydantic-field \u00b6 The tags to assign to each log event for this log source. type : str pydantic-field required \u00b6 The type to tag the log input with. LogstashParserConfig pydantic-model \u00b6 Configuration model defining the logstash parser settings. This is used to configure how logstash is used as dataset parser (e.g., log level) Examples: settings_dir : processing/logstash conf_dir : processing/logstash/conf.d log_level : debug log_dir : processing/logstash/log completed_log : processing/logstash/log/file-completed.log data_dir : processing/logstash/data parsed_dir : parsed save_parsed : false completed_log : Path pydantic-field \u00b6 The logstash file input completed log (defaults to <log_dir>/file-completed.log conf_dir : Path pydantic-field \u00b6 The path to the logstash pipeline config (defaults to <settings_dir>/conf.d ) data_dir : Path pydantic-field \u00b6 The directory logstash should use for persistent data (e.g., sincedb). log_dir : Path pydantic-field \u00b6 The directory logstash should use for logging log_level : str pydantic-field \u00b6 The log level to pass to the logstash cli parsed_dir : Path pydantic-field \u00b6 The directory to save the parsed log files in, when save_parsed=true for any log. (defaults to <dataset>/parsed ) save_parsed : bool pydantic-field \u00b6 If the log files should be saved to the disk after parsing. Is overridden by log.save_parsed. settings_dir : Path pydantic-field \u00b6 The logstash settings directory containing the logstash.yml (use for path.settings ). default_completed_log ( val , * , values , ** kwargs ) classmethod \u00b6 Validator for setting default completed_log Parameters: Name Type Description Default val Optional[pathlib.Path] The completed_log config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The completed_log path. Source code in dataset/config.py @validator ( \"completed_log\" , pre = True , always = True ) def default_completed_log ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default completed_log Args: val: The completed_log config value. values: The model attribute dict. Returns: Path: The completed_log path. \"\"\" return val or values [ \"log_dir\" ] . joinpath ( \"file-completed.log\" ) default_conf_dir ( val , * , values , ** kwargs ) classmethod \u00b6 Validator for setting default conf_dir Parameters: Name Type Description Default val Optional[pathlib.Path] The conf_dir config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The conf_dir path. Source code in dataset/config.py @validator ( \"conf_dir\" , pre = True , always = True ) def default_conf_dir ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default conf_dir Args: val: The conf_dir config value. values: The model attribute dict. Returns: Path: The conf_dir path. \"\"\" return val or values [ \"settings_dir\" ] . joinpath ( \"conf.d\" ) default_parsed_dir ( val , * , values , ** kwargs ) classmethod \u00b6 Validator for setting default parsed_dir Parameters: Name Type Description Default val Optional[pathlib.Path] The parsed_dir config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The parsed_dir path. Source code in dataset/config.py @validator ( \"parsed_dir\" , pre = True , always = True ) def default_parsed_dir ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default parsed_dir Args: val: The parsed_dir config value. values: The model attribute dict. Returns: Path: The parsed_dir path. \"\"\" return val or Path ( \"parsed\" ) ProcessingConfig pydantic-model \u00b6 Configuration model for the processing pipeline. The pipline configuration is split into the three steps - pre-processing ( pre_processors ): List of Cyber Range Kyoushi processors executed before parsing the dataset. - parsing ( parser ): Logstash parser configuration. - post-processing ( post_processors ): List of Cyber Range Kyoushi processors executed after the dataset has been parsed. parser : LogstashParserConfig pydantic-field \u00b6 The logstash parser configuration. post_processors : Dict [ str , Any ] pydantic-field \u00b6 The processors to apply to the dataset after parsing and publishing the log data to elasticsearch. pre_processors : Dict [ str , Any ] pydantic-field \u00b6 The processors to apply to the dataset before parsing and publishing the log data to elasticsearch. check_processor_required_fields ( val ) classmethod \u00b6 Validator for ensuring that processors have name and type fields. Parameters: Name Type Description Default val Dict[str, Any] Processor configuration dict required Returns: Type Description Dict[str, Any] Validated processor configuration dict Source code in dataset/config.py @validator ( \"pre_processors\" , \"post_processors\" , each_item = True ) def check_processor_required_fields ( cls , val : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validator for ensuring that processors have `name` and `type` fields. Args: val: Processor configuration dict Returns: Validated processor configuration dict \"\"\" assert \"name\" in val , \"A processor must have a name\" assert ( \"type\" in val ), f \"A processor must have a type, but { val [ 'name' ] } has none\" return val","title":"Config"},{"location":"reference/config/#config-module","text":"This module contains all configuration model definitions.","title":"Config module"},{"location":"reference/config/#cr_kyoushi.dataset.config.DatasetConfig","text":"Configuration model for the dataset defintion. This model controls the attributes of the dataset (e.g., name) currently being processed. These configuration values are set during the dataset preparation phase. Examples: name : example-dataset start : 2021-10-10T12:00 end : 2021-10-12T12:00","title":"DatasetConfig"},{"location":"reference/config/#cr_kyoushi.dataset.config.DatasetConfig.end","text":"The end time of the observation period.","title":"end"},{"location":"reference/config/#cr_kyoushi.dataset.config.DatasetConfig.name","text":"The name of the dataset. This is for example used as part of the elasticsearch index.","title":"name"},{"location":"reference/config/#cr_kyoushi.dataset.config.DatasetConfig.start","text":"The start time of the observation period.","title":"start"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig","text":"Configuration model for to be parsed log files. This model is used to create a Logstash input configuration for raw dataset log files. Examples: - type : kyoushi codec : json path : sm.log* save_parse : false exclude : - * .gz - * .zip file_sort_direction : desc file_chunk_size : 320000 delimiter : tags : - statemachine - kyoushi add_field : '[@metadata][kyoushi][sm]' : user","title":"LogstashLogConfig"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.add_field","text":"A dict of fields to add to each log event.","title":"add_field"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.codec","text":"The file codec to use for reading.","title":"codec"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.delimiter","text":"The newline delimiter (does not work for compressed files).","title":"delimiter"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.exclude","text":"Glob/s to exclude from reading.","title":"exclude"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.file_chunk_size","text":"The size of the chunks to read from the file (in bytes). Default is 32kb set this to a higher value if your log file contains very long lines.","title":"file_chunk_size"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.file_sort_direction","text":"The sort direction for multiple files.","title":"file_sort_direction"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.path","text":"The log file path/s to read.","title":"path"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.save_parsed","text":"If this log should be saved to the disk after parsing. (Overrides parser.save_parsed)","title":"save_parsed"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.tags","text":"The tags to assign to each log event for this log source.","title":"tags"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashLogConfig.type","text":"The type to tag the log input with.","title":"type"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig","text":"Configuration model defining the logstash parser settings. This is used to configure how logstash is used as dataset parser (e.g., log level) Examples: settings_dir : processing/logstash conf_dir : processing/logstash/conf.d log_level : debug log_dir : processing/logstash/log completed_log : processing/logstash/log/file-completed.log data_dir : processing/logstash/data parsed_dir : parsed save_parsed : false","title":"LogstashParserConfig"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.completed_log","text":"The logstash file input completed log (defaults to <log_dir>/file-completed.log","title":"completed_log"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.conf_dir","text":"The path to the logstash pipeline config (defaults to <settings_dir>/conf.d )","title":"conf_dir"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.data_dir","text":"The directory logstash should use for persistent data (e.g., sincedb).","title":"data_dir"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.log_dir","text":"The directory logstash should use for logging","title":"log_dir"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.log_level","text":"The log level to pass to the logstash cli","title":"log_level"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.parsed_dir","text":"The directory to save the parsed log files in, when save_parsed=true for any log. (defaults to <dataset>/parsed )","title":"parsed_dir"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.save_parsed","text":"If the log files should be saved to the disk after parsing. Is overridden by log.save_parsed.","title":"save_parsed"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.settings_dir","text":"The logstash settings directory containing the logstash.yml (use for path.settings ).","title":"settings_dir"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.default_completed_log","text":"Validator for setting default completed_log Parameters: Name Type Description Default val Optional[pathlib.Path] The completed_log config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The completed_log path. Source code in dataset/config.py @validator ( \"completed_log\" , pre = True , always = True ) def default_completed_log ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default completed_log Args: val: The completed_log config value. values: The model attribute dict. Returns: Path: The completed_log path. \"\"\" return val or values [ \"log_dir\" ] . joinpath ( \"file-completed.log\" )","title":"default_completed_log()"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.default_conf_dir","text":"Validator for setting default conf_dir Parameters: Name Type Description Default val Optional[pathlib.Path] The conf_dir config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The conf_dir path. Source code in dataset/config.py @validator ( \"conf_dir\" , pre = True , always = True ) def default_conf_dir ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default conf_dir Args: val: The conf_dir config value. values: The model attribute dict. Returns: Path: The conf_dir path. \"\"\" return val or values [ \"settings_dir\" ] . joinpath ( \"conf.d\" )","title":"default_conf_dir()"},{"location":"reference/config/#cr_kyoushi.dataset.config.LogstashParserConfig.default_parsed_dir","text":"Validator for setting default parsed_dir Parameters: Name Type Description Default val Optional[pathlib.Path] The parsed_dir config value. required values Dict[str, Any] The model attribute dict. required Returns: Type Description Path Path: The parsed_dir path. Source code in dataset/config.py @validator ( \"parsed_dir\" , pre = True , always = True ) def default_parsed_dir ( cls , val : Optional [ Path ], * , values : Dict [ str , Any ], ** kwargs ) -> Path : \"\"\"Validator for setting default parsed_dir Args: val: The parsed_dir config value. values: The model attribute dict. Returns: Path: The parsed_dir path. \"\"\" return val or Path ( \"parsed\" )","title":"default_parsed_dir()"},{"location":"reference/config/#cr_kyoushi.dataset.config.ProcessingConfig","text":"Configuration model for the processing pipeline. The pipline configuration is split into the three steps - pre-processing ( pre_processors ): List of Cyber Range Kyoushi processors executed before parsing the dataset. - parsing ( parser ): Logstash parser configuration. - post-processing ( post_processors ): List of Cyber Range Kyoushi processors executed after the dataset has been parsed.","title":"ProcessingConfig"},{"location":"reference/config/#cr_kyoushi.dataset.config.ProcessingConfig.parser","text":"The logstash parser configuration.","title":"parser"},{"location":"reference/config/#cr_kyoushi.dataset.config.ProcessingConfig.post_processors","text":"The processors to apply to the dataset after parsing and publishing the log data to elasticsearch.","title":"post_processors"},{"location":"reference/config/#cr_kyoushi.dataset.config.ProcessingConfig.pre_processors","text":"The processors to apply to the dataset before parsing and publishing the log data to elasticsearch.","title":"pre_processors"},{"location":"reference/config/#cr_kyoushi.dataset.config.ProcessingConfig.check_processor_required_fields","text":"Validator for ensuring that processors have name and type fields. Parameters: Name Type Description Default val Dict[str, Any] Processor configuration dict required Returns: Type Description Dict[str, Any] Validated processor configuration dict Source code in dataset/config.py @validator ( \"pre_processors\" , \"post_processors\" , each_item = True ) def check_processor_required_fields ( cls , val : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validator for ensuring that processors have `name` and `type` fields. Args: val: Processor configuration dict Returns: Validated processor configuration dict \"\"\" assert \"name\" in val , \"A processor must have a name\" assert ( \"type\" in val ), f \"A processor must have a type, but { val [ 'name' ] } has none\" return val","title":"check_processor_required_fields()"},{"location":"reference/elasticsearch/","text":"Elasticsearch module \u00b6 This module contains Elasticsearch related utility functions get_transport_variables ( es ) \u00b6 Utility function for getting Elasticsearch connection info. This function takes an Elasticsearch client object and extracts the host connection info and returns it in a dict. Parameters: Name Type Description Default es Elasticsearch Elasticsearch client object required Exceptions: Type Description TypeError If the passed client object is not connected to a host Returns: Type Description Dict[str, Any] Host connection information dict containing the following fields: ELASTICSEARCH_HOST : The host IP/FQDN and port ELASTICSEARCH_HOST_PATH : The HTTP path if it is part of the full address ELASTICSEARCH_SSL : Boolean indicating SSL on or off ELASTICSEARCH_USER : The username if HTTP auth is used ELASTICSEARCH_PASSWORD : The password if HTTP auth is used Source code in dataset/elasticsearch.py def get_transport_variables ( es : Elasticsearch ) -> Dict [ str , Any ]: \"\"\"Utility function for getting Elasticsearch connection info. This function takes an Elasticsearch client object and extracts the host connection info and returns it in a dict. Args: es: Elasticsearch client object Raises: TypeError: If the passed client object is not connected to a host Returns: Host connection information dict containing the following fields: - `ELASTICSEARCH_HOST`: The host IP/FQDN and port - `ELASTICSEARCH_HOST_PATH`: The HTTP path if it is part of the full address - `ELASTICSEARCH_SSL`: Boolean indicating SSL on or off - `ELASTICSEARCH_USER`: The username if HTTP auth is used - `ELASTICSEARCH_PASSWORD`: The password if HTTP auth is used \"\"\" if es . transport . hosts is not None and len ( es . transport . hosts ) > 0 : # get the first host host = es . transport . hosts [ 0 ] # these we can always set host_variables = { \"ELASTICSEARCH_HOST\" : f \" { host [ 'host' ] } : { host . get ( 'port' , 80 ) } \" , \"ELASTICSEARCH_SSL\" : host . get ( \"use_ssl\" , False ), } if \"http_auth\" in host : # split RFC http auth into user and password pair ( host_variables [ \"ELASTICSEARCH_USER\" ], host_variables [ \"ELASTICSEARCH_PASSWORD\" ], ) = host [ \"http_auth\" ] . split ( \":\" , 1 ) if \"url_prefix\" in host : host_variables [ \"ELASTICSEARCH_HOST_PATH\" ] = host [ \"url_prefix\" ] return host_variables raise TypeError ( \"Uninitialized elasticsearch client!\" ) scan_composite ( search , name ) \u00b6 Utility function for getting all result buckets of a composite aggregate. Parameters: Name Type Description Default search Search The composite aggregate search object. required name str The name of the aggregate required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of result buckets Source code in dataset/elasticsearch.py def scan_composite ( search : Search , name : str ) -> List [ Bucket ]: \"\"\"Utility function for getting all result buckets of a composite aggregate. Args: search: The composite aggregate search object. name: The name of the aggregate Returns: List of result buckets \"\"\" # ensure that we do not get documents for no reason search = search . extra ( size = 0 ) buckets = [] while True : # need to disable cache or the API will keep returning the same result result = search . execute ( ignore_cache = True ) buckets . extend ( result . aggregations [ name ] . buckets ) if \"after_key\" not in result . aggregations [ name ]: # no after key indicates we got everything return buckets # resume query after the key search . aggs [ name ] . after = result . aggregations [ name ] . after_key search_eql ( es , index , body , check_interval = 0.5 ) \u00b6 Utility function for issueing and receiving the result of a EQL query. Parameters: Name Type Description Default es Elasticsearch The Elasticsearch client object required index Union[Sequence[str], str] The indices to search on required body Dict[str, Any] The EQL query body required check_interval float The time to wait in between query ready checks. 0.5 Returns: Type Description Dict[str, Any] The EQL query result Source code in dataset/elasticsearch.py def search_eql ( es : Elasticsearch , index : Union [ Sequence [ str ], str , None ], body : Dict [ str , Any ], check_interval : float = 0.5 , ) -> Dict [ str , Any ]: \"\"\"Utility function for issueing and receiving the result of a EQL query. Args: es: The Elasticsearch client object index: The indices to search on body: The EQL query body check_interval: The time to wait in between query ready checks. Returns: The EQL query result \"\"\" result = es . eql . search ( index = index , body = body , wait_for_completion_timeout = \"0s\" ) result_id = None while result [ \"is_running\" ]: result_id = result [ \"id\" ] sleep ( check_interval ) result = es . eql . get_status ( id = result_id ) # if result id is not none then we have a async request and have # to retrieve the actual result and delete the async data if result_id is not None : result = es . eql . get ( id = result_id ) # delete the async request once we have its data es . eql . delete ( id = result_id ) return result [ \"hits\" ]","title":"Elasticsearch"},{"location":"reference/elasticsearch/#elasticsearch-module","text":"This module contains Elasticsearch related utility functions","title":"Elasticsearch module"},{"location":"reference/elasticsearch/#cr_kyoushi.dataset.elasticsearch.get_transport_variables","text":"Utility function for getting Elasticsearch connection info. This function takes an Elasticsearch client object and extracts the host connection info and returns it in a dict. Parameters: Name Type Description Default es Elasticsearch Elasticsearch client object required Exceptions: Type Description TypeError If the passed client object is not connected to a host Returns: Type Description Dict[str, Any] Host connection information dict containing the following fields: ELASTICSEARCH_HOST : The host IP/FQDN and port ELASTICSEARCH_HOST_PATH : The HTTP path if it is part of the full address ELASTICSEARCH_SSL : Boolean indicating SSL on or off ELASTICSEARCH_USER : The username if HTTP auth is used ELASTICSEARCH_PASSWORD : The password if HTTP auth is used Source code in dataset/elasticsearch.py def get_transport_variables ( es : Elasticsearch ) -> Dict [ str , Any ]: \"\"\"Utility function for getting Elasticsearch connection info. This function takes an Elasticsearch client object and extracts the host connection info and returns it in a dict. Args: es: Elasticsearch client object Raises: TypeError: If the passed client object is not connected to a host Returns: Host connection information dict containing the following fields: - `ELASTICSEARCH_HOST`: The host IP/FQDN and port - `ELASTICSEARCH_HOST_PATH`: The HTTP path if it is part of the full address - `ELASTICSEARCH_SSL`: Boolean indicating SSL on or off - `ELASTICSEARCH_USER`: The username if HTTP auth is used - `ELASTICSEARCH_PASSWORD`: The password if HTTP auth is used \"\"\" if es . transport . hosts is not None and len ( es . transport . hosts ) > 0 : # get the first host host = es . transport . hosts [ 0 ] # these we can always set host_variables = { \"ELASTICSEARCH_HOST\" : f \" { host [ 'host' ] } : { host . get ( 'port' , 80 ) } \" , \"ELASTICSEARCH_SSL\" : host . get ( \"use_ssl\" , False ), } if \"http_auth\" in host : # split RFC http auth into user and password pair ( host_variables [ \"ELASTICSEARCH_USER\" ], host_variables [ \"ELASTICSEARCH_PASSWORD\" ], ) = host [ \"http_auth\" ] . split ( \":\" , 1 ) if \"url_prefix\" in host : host_variables [ \"ELASTICSEARCH_HOST_PATH\" ] = host [ \"url_prefix\" ] return host_variables raise TypeError ( \"Uninitialized elasticsearch client!\" )","title":"get_transport_variables()"},{"location":"reference/elasticsearch/#cr_kyoushi.dataset.elasticsearch.scan_composite","text":"Utility function for getting all result buckets of a composite aggregate. Parameters: Name Type Description Default search Search The composite aggregate search object. required name str The name of the aggregate required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of result buckets Source code in dataset/elasticsearch.py def scan_composite ( search : Search , name : str ) -> List [ Bucket ]: \"\"\"Utility function for getting all result buckets of a composite aggregate. Args: search: The composite aggregate search object. name: The name of the aggregate Returns: List of result buckets \"\"\" # ensure that we do not get documents for no reason search = search . extra ( size = 0 ) buckets = [] while True : # need to disable cache or the API will keep returning the same result result = search . execute ( ignore_cache = True ) buckets . extend ( result . aggregations [ name ] . buckets ) if \"after_key\" not in result . aggregations [ name ]: # no after key indicates we got everything return buckets # resume query after the key search . aggs [ name ] . after = result . aggregations [ name ] . after_key","title":"scan_composite()"},{"location":"reference/elasticsearch/#cr_kyoushi.dataset.elasticsearch.search_eql","text":"Utility function for issueing and receiving the result of a EQL query. Parameters: Name Type Description Default es Elasticsearch The Elasticsearch client object required index Union[Sequence[str], str] The indices to search on required body Dict[str, Any] The EQL query body required check_interval float The time to wait in between query ready checks. 0.5 Returns: Type Description Dict[str, Any] The EQL query result Source code in dataset/elasticsearch.py def search_eql ( es : Elasticsearch , index : Union [ Sequence [ str ], str , None ], body : Dict [ str , Any ], check_interval : float = 0.5 , ) -> Dict [ str , Any ]: \"\"\"Utility function for issueing and receiving the result of a EQL query. Args: es: The Elasticsearch client object index: The indices to search on body: The EQL query body check_interval: The time to wait in between query ready checks. Returns: The EQL query result \"\"\" result = es . eql . search ( index = index , body = body , wait_for_completion_timeout = \"0s\" ) result_id = None while result [ \"is_running\" ]: result_id = result [ \"id\" ] sleep ( check_interval ) result = es . eql . get_status ( id = result_id ) # if result id is not none then we have a async request and have # to retrieve the actual result and delete the async data if result_id is not None : result = es . eql . get ( id = result_id ) # delete the async request once we have its data es . eql . delete ( id = result_id ) return result [ \"hits\" ]","title":"search_eql()"},{"location":"reference/labels/","text":"Labels module \u00b6 This module contains labeling rule implementations and utility functions used during the labeling process EqlQueryBase pydantic-model \u00b6 Pydantic model representing an EQL query batch_size : int pydantic-field \u00b6 The amount of sequences to update with each batch. Cannot be bigger than max_result_window by : Union [ List [ str ], str ] pydantic-field \u00b6 Optional global sequence by fields event_category_field : str pydantic-field \u00b6 The field used to categories events filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. max_result_window : int pydantic-field \u00b6 The max result window allowed on the elasticsearch instance max_span : str pydantic-field \u00b6 Optional max time span in which a sequence must occur to be considered a match sequences : str pydantic-field required \u00b6 Event sequences to search. Must contain at least two events. tiebreaker_field : str pydantic-field \u00b6 (Optional, string) Field used to sort hits with the same timestamp in ascending order. timestamp_field : str pydantic-field \u00b6 The field containing the event timestamp until : str pydantic-field \u00b6 Optional until event marking the end of valid sequences. The until event will not be labeled. query ( self ) \u00b6 Converts the EQL query object into an EQL query string. Returns: Type Description str The query in EQL syntax Source code in dataset/labels.py def query ( self ) -> str : \"\"\"Converts the EQL query object into an EQL query string. Returns: The query in EQL syntax \"\"\" query = \"sequence\" if self . by is not None : if isinstance ( self . by , Text ): query += f \" by { self . by } \" elif len ( self . by ) > 0 : query += f \" by { ', ' . join ( self . by ) } \" if self . max_span is not None : query += f \" with maxspan= { self . max_span } \" for sequence in self . sequences : query += f \" \\n { sequence } \" if self . until is not None : query += f \" \\n until { self . until } \" return query EqlSequenceRule pydantic-model \u00b6 Applies labels to a sequence of log events defined by an EQL query. This labeling rule is defined as an EQL query . Using this syntax it is possible to define a sequence of related events and retrieve them. All events part of retrieved sequences are then labeled. Examples: - type : elasticsearch.sequence id : attacker.webshell.upload.seq labels : [ webshell_upload ] description : >- This rule labels the web shell upload step by matching the 3 step sequence within the foothold phase. index : - apache_access-intranet_server # since we do these requests very fast # we need the line number as tie breaker tiebreaker_field : log.file.line by : source.address max_span : 2m filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" sequences : - '[ apache where event.action == \"access\" and url.original == \"/\" ]' - '[ apache where event.action == \"access\" and url.original == \"/?p=5\" ]' - '[ apache where event.action == \"access\" and http.request.method == \"POST\" and url.original == \"/wp-admin/admin-ajax.php\" ]' batch_size : int pydantic-field \u00b6 The amount of sequences to update with each batch. Cannot be bigger than max_result_window by : Union [ List [ str ], str ] pydantic-field \u00b6 Optional global sequence by fields description : str pydantic-field \u00b6 An optional description for the rule event_category_field : str pydantic-field \u00b6 The field used to categories events filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule max_result_window : int pydantic-field \u00b6 The max result window allowed on the elasticsearch instance max_span : str pydantic-field \u00b6 Optional max time span in which a sequence must occur to be considered a match sequences : str pydantic-field required \u00b6 Event sequences to search. Must contain at least two events. tiebreaker_field : str pydantic-field \u00b6 (Optional, string) Field used to sort hits with the same timestamp in ascending order. timestamp_field : str pydantic-field \u00b6 The field containing the event timestamp type_field : str pydantic-field required \u00b6 The rule type as passed in from the config until : str pydantic-field \u00b6 Optional until event marking the end of valid sequences. The until event will not be labeled. apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the labels to all events part of retrieved sequences. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all events part of retrieved sequences. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) body = self . _make_body ( label_object ) updated = 0 # as of elk 7.12 of there is no way to ensure we get all even through the EQL api # (there is no scan or search after like for the DSL API) # so manually search in batches for sequences with events that are not labeled yet # we stop only when we do not get any results anymore i.e., all events have been labeled # this is obviously not the most efficient approach but its the best we can do for now while True : hits = search_eql ( es , index , body ) if hits [ \"total\" ][ \"value\" ] > 0 : index_ids : Dict [ str , List [ str ]] = {} # we have to sort the events by indices # because ids are only guranteed to be uniq per index for sequence in hits [ \"sequences\" ]: for event in sequence [ \"events\" ]: index_ids . setdefault ( event [ \"_index\" ], []) . append ( event [ \"_id\" ]) # add labels to each event per index for _index , ids in index_ids . items (): # split the update requests into chunks of at most max result window update_chunks = [ ids [ i : i + self . max_result_window ] for i in range ( 0 , len ( ids ), self . max_result_window ) ] for chunk in update_chunks : update = UpdateByQuery ( using = es , index = _index ) . query ( \"ids\" , values = chunk ) # apply labels to events updated += apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) else : # end loop once we do not find new events anymore break return updated query ( self ) inherited \u00b6 Converts the EQL query object into an EQL query string. Returns: Type Description str The query in EQL syntax Source code in dataset/labels.py def query ( self ) -> str : \"\"\"Converts the EQL query object into an EQL query string. Returns: The query in EQL syntax \"\"\" query = \"sequence\" if self . by is not None : if isinstance ( self . by , Text ): query += f \" by { self . by } \" elif len ( self . by ) > 0 : query += f \" by { ', ' . join ( self . by ) } \" if self . max_span is not None : query += f \" with maxspan= { self . max_span } \" for sequence in self . sequences : query += f \" \\n { sequence } \" if self . until is not None : query += f \" \\n until { self . until } \" return query update_params ( self ) inherited \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } LabelException \u00b6 Generic exception for errors during labeling phase. Labeler \u00b6 Cyber Range Kyoushi labeler This class is used to configure and execute the labeling process. __init__ ( self , rule_types = {}, update_script_id = 'kyoushi_label_update' , label_object = 'kyoushi_labels' ) special \u00b6 Parameters: Name Type Description Default rule_types Dict[str, Any] Dictionary containing all available labeling rule types. {} update_script_id str The Elasticsearch ID for the update script. 'kyoushi_label_update' label_object str The field to store labels in. 'kyoushi_labels' Source code in dataset/labels.py def __init__ ( self , rule_types : Dict [ str , Any ] = {}, update_script_id : str = \"kyoushi_label_update\" , label_object : str = \"kyoushi_labels\" , ): \"\"\" Args: rule_types : Dictionary containing all available labeling rule types. update_script_id: The Elasticsearch ID for the update script. label_object: The field to store labels in. \"\"\" self . rule_types : Dict [ str , Any ] = rule_types # default rule types self . rule_types . update ( { NoopRule . type_ : NoopRule , UpdateByQueryRule . type_ : UpdateByQueryRule , UpdateSubQueryRule . type_ : UpdateSubQueryRule , UpdateParentQueryRule . type_ : UpdateParentQueryRule , EqlSequenceRule . type_ : EqlSequenceRule , } ) self . update_script_id : str = update_script_id self . label_object : str = label_object add_label_object_mapping ( self , es , dataset_name , rules ) \u00b6 Utility function for adding the field definitions for the label field. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object. required dataset_name str The name of the dataset. required rules List[cr_kyoushi.dataset.labels.Rule] List of all to be applied rules. required Source code in dataset/labels.py def add_label_object_mapping ( self , es : Elasticsearch , dataset_name : str , rules : List [ Rule ], ): \"\"\"Utility function for adding the field definitions for the label field. Args: es: The elasticsearch client object. dataset_name: The name of the dataset. rules: List of all to be applied rules. \"\"\" root = Mapping () flat = {} list_ = {} for rule in rules : flat [ rule . id_ ] = Keyword () list_ [ rule . id_ ] = Keyword ( multi = True ) properties = { \"flat\" : { \"properties\" : flat , }, \"list\" : { \"properties\" : list_ , }, \"rules\" : { \"type\" : \"keyword\" }, } root . field ( self . label_object , \"object\" , properties = properties ) es . indices . put_mapping ( index = f \" { dataset_name } -*\" , body = root . to_dict ()) execute ( self , rules , dataset_dir , dataset_config , es ) \u00b6 Parse and apply all labeling rules. Note This function only write labels to the database. The resulting labels can be written to the file system using the write(...) function of the labeler. Parameters: Name Type Description Default rules List[Dict[str, Any]] The unparsed list of labeling rules. required dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required Exceptions: Type Description ValidationError If a labeling rule or configuration is invalid LabelException If an error occurs while applying a labeling rule Source code in dataset/labels.py def execute ( self , rules : List [ Dict [ str , Any ]], dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , ): \"\"\"Parse and apply all labeling rules. !!! Note This function only write labels to the database. The resulting labels can be written to the file system using the `write(...)` function of the labeler. Args: rules: The unparsed list of labeling rules. dataset_dir: The dataset path dataset_config: The dataset configuration es: The elasticsearch client object Raises: ValidationError: If a labeling rule or configuration is invalid LabelException: If an error occurs while applying a labeling rule \"\"\" # validate the general rule list RuleList . parse_obj ( rules ) # convert and validate rule types rule_objects : List [ Rule ] = [] errors : List [ ErrorWrapper ] = [] for r in rules : try : rule_objects . append ( self . rule_types [ r [ \"type\" ]] . parse_obj ( r )) except ValidationError as e : errors . append ( ErrorWrapper ( e , r [ \"id\" ])) if len ( errors ) > 0 : raise ValidationError ( errors , RuleList ) # create mappings for rule label fields # we need to do this since EQL queries cannot check existence of non mapped fields self . add_label_object_mapping ( es , dataset_config . name , rule_objects ) # ensure update script exists create_kyoushi_scripts ( es , dataset_config . name , self . label_object ) # start labeling process for rule in rule_objects : try : print ( f \"Applying rule { rule . id_ } ...\" ) updated = rule . apply ( dataset_dir , dataset_config , es , f \" { dataset_config . name } _ { self . update_script_id } \" , self . label_object , ) print ( f \"Rule { rule . id_ } applied labels: { rule . labels } to { updated } lines.\" ) except ElasticsearchRequestError as e : raise LabelException ( f \"Error executing rule ' { rule . id_ } '\" , e ) write ( self , dataset_dir , dataset_config , es , index , skip_files ) \u00b6 Write labeles stored in the database to the file system. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required index List[str] The indices to consider required skip_files List[str] The files to ignore required Source code in dataset/labels.py def write ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , index : List [ str ], skip_files : List [ str ], ): \"\"\"Write labeles stored in the database to the file system. Args: dataset_dir: The dataset path dataset_config: The dataset configuration es: The elasticsearch client object index: The indices to consider skip_files: The files to ignore \"\"\" files = self . _get_label_files ( dataset_config , es , index , skip_files ) for current_file in files : # disable request cache to ensure we always get latest info search_labeled = Search ( using = es , index = f \" { dataset_config . name } -*\" ) . params ( request_cache = False , preserve_order = True ) search_labeled = search_labeled . filter ( \"exists\" , field = f \" { self . label_object } .rules\" ) . filter ( \"term\" , log__file__path = current_file ) search_labeled = search_labeled . sort ({ \"log.file.line\" : \"asc\" }) base_path = dataset_dir . joinpath ( LAYOUT . GATHER . value ) label_path = dataset_dir . joinpath ( LAYOUT . LABELS . value ) label_file_path = label_path . joinpath ( Path ( current_file ) . relative_to ( base_path ) ) print ( f \"Start writing { current_file } \" ) self . _write_file ( search_labeled , label_file_path ) NoopRule pydantic-model \u00b6 A noop rule that does absolutely nothing. description : str pydantic-field \u00b6 An optional description for the rule id_ : str pydantic-field required \u00b6 The unique rule id labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule type_field : str pydantic-field required \u00b6 The rule type as passed in from the config apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the NoopRule i.e., does nothing. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int Always returns 0 since nothing happens. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the NoopRule i.e., does nothing. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: Always returns 0 since nothing happens. \"\"\" return 0 update_params ( self ) inherited \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } QueryBase pydantic-model \u00b6 Base model for DSL query based labeling rules exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. validate_exclude ( value , values ) classmethod \u00b6 Validator to check the DSL query exclude syntax Exclude is simply a negative filter clause i.e., not (DSL QUERY). Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL exclude required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the exclude not be valid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated exclude Source code in dataset/labels.py @validator ( \"exclude\" ) def validate_exclude ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator to check the DSL query exclude syntax Exclude is simply a negative filter clause i.e., not (DSL QUERY). Args: value: The DSL exclude values: The model dictionary Raises: ValidationError: Should the exclude not be valid Returns: The validated exclude \"\"\" # temporary update by query object used to validate the input excludes _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , exclude in enumerate ( value ): try : _temp = _temp . exclude ( exclude ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value validate_filter ( value , values ) classmethod \u00b6 Validator to check the DSL query filter syntax Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL filter required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the filter not be valid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated filter Source code in dataset/labels.py @validator ( \"filter_\" ) def validate_filter ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator to check the DSL query filter syntax Args: value: The DSL filter values: The model dictionary Raises: ValidationError: Should the filter not be valid Returns: The validated filter \"\"\" # temporary update by query object used to validate the input filters _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , filter_ in enumerate ( value ): try : _temp = _temp . filter ( filter_ ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value validate_queries ( value , values ) classmethod \u00b6 Validator checking DSL syntax Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL query required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the given query be invalid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated query Source code in dataset/labels.py @validator ( \"query\" ) def validate_queries ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator checking DSL syntax Args: value: The DSL query values: The model dictionary Raises: ValidationError: Should the given query be invalid Returns: The validated query \"\"\" # temporary update by query object used to validate the input queries _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , query in enumerate ( value ): try : _temp = _temp . query ( query ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value Rule \u00b6 Interface definition for labeling rules. apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the configured rule and assigns the labels to all matching rows. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the rule was applied to Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the configured rule and assigns the labels to all matching rows. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the rule was applied to \"\"\" ... update_params ( self ) \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" ... RuleBase pydantic-model \u00b6 Pydantic base model for labeling rules. This class can be extended to define custom labeling rules. description : str pydantic-field \u00b6 An optional description for the rule id_ : str pydantic-field required \u00b6 The unique rule id labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule type_field : str pydantic-field required \u00b6 The rule type as passed in from the config apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the configured rule and assigns the labels to all matching rows. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the rule was applied to Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the configured rule and assigns the labels to all matching rows. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the rule was applied to \"\"\" raise NotImplementedError () update_params ( self ) \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } validate_label_no_semicolon ( val ) classmethod \u00b6 Validator ensuring label names do not contain ; . Parameters: Name Type Description Default val str A single label required Returns: Type Description str The validated label Source code in dataset/labels.py @validator ( \"labels\" , each_item = True ) def validate_label_no_semicolon ( cls , val : str ) -> str : \"\"\"Validator ensuring label names do not contain `;`. Args: val: A single label Returns: The validated label \"\"\" assert \";\" not in val , f \"Labels must not contain semicolons, but got ' { val } '\" return val RuleList pydantic-model \u00b6 Model definition of a list of labeling rules. check_rule_ids_uniq ( values ) classmethod \u00b6 Validator for ensuring that rule IDs are unique. Parameters: Name Type Description Default values Dict[str, List[cr_kyoushi.dataset.labels.RuleBase]] The model dictionary required Returns: Type Description Dict[str, List[cr_kyoushi.dataset.labels.RuleBase]] The validated model dictionary Source code in dataset/labels.py @root_validator def check_rule_ids_uniq ( cls , values : Dict [ str , List [ RuleBase ]] ) -> Dict [ str , List [ RuleBase ]]: \"\"\"Validator for ensuring that rule IDs are unique. Args: values: The model dictionary Returns: The validated model dictionary \"\"\" duplicates = set () temp = [] if \"__root__\" in values : for r in values [ \"__root__\" ]: if r . id_ in temp : duplicates . add ( r . id_ ) else : temp . append ( r . id_ ) assert ( len ( duplicates ) == 0 ), f \"Rule IDs must be uniq, but got duplicates: { duplicates } \" return values UpdateByQueryRule pydantic-model \u00b6 Applies labels based on a simple Elasticsearch DSL query. Examples: - type : elasticsearch.query id : attacker.foothold.vpn.ip labels : - attacker_vpn - foothold description : >- This rule applies the labels to all openvpn log rows that have the attacker server as source ip and are within the foothold phase. index : - openvpn-vpn filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" query : - match : source.ip : '192.42.0.255' description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the labels to all rows matching the DSL query. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all rows matching the DSL query. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) update = UpdateByQuery ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set update = update . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : update = update . query ( _query ) for _filter in self . filter_ : update = update . filter ( _filter ) for _exclude in self . exclude : update = update . exclude ( _exclude ) result = apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) return result update_params ( self ) inherited \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } UpdateParentQueryRule pydantic-model \u00b6 Applies the labels to all rows of a base query for which a parent query returns results. This labeling rule first executes a base query to retrieve rows we might want to apply labels to. It then renders and executes a templated parent query for each retrieved row. The parent queries are then used to indicate if the initial result row should be labeled or not. By default result rows of the base query are labeled if the corresponding parent query returns at leas one row. It is possible to configure this minimum number e.g., to require at least two results. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.parent_query id : attacker.foothold.apache.error_access labels : - attacker_http - foothold description : >- This rule looks for unlabeled error messages resulting from VPN server traffic within the attack time and tries to match it to an already labeled access log row. index : - apache_error-intranet_server query : match : source.address : \"172.16.100.151\" filter : # use script query to match only entries that # are not already tagged for as attacker http in the foothold phase - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] parent_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"{{ HIT.source.address }}\" # we are looking for parents that are labeled as attacker http - bool : must : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] filter : - range : # parent must be within +-1s of potential child \"@timestamp\" : gte : \"{{ (HIT['@timestamp'] | as_datetime) - timedelta(seconds=1) }}\" lte : \"{{ ( HIT['@timestamp'] | as_datetime) + timedelta(seconds=1) }}\" description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule max_result_window : int pydantic-field \u00b6 The max result window allowed on the elasticsearch instance min_match : int pydantic-field \u00b6 The minimum number of parent matches needed for the main query to be labeled. parent_query : QueryBase pydantic-field required \u00b6 The templated parent query to check if the labels should be applied to a query hit. query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the labels to result from the base query for which a parent was found. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to result from the base query for which a parent was found. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set search = search . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : search = search . query ( _query ) for _filter in self . filter_ : search = search . filter ( _filter ) for _exclude in self . exclude : search = search . exclude ( _exclude ) result = 0 update_map : Dict [ str , List [ str ]] = {} _i = 0 for hit in search . scan (): _i += 1 # make deep copy of parent query so we can template it parent_query = self . parent_query . copy ( deep = True ) # render the subquery parent_query = render_query_base ( hit , parent_query ) if self . check_parent ( parent_query , self . min_match , dataset_config , es ): update_map . setdefault ( hit . meta . index , []) . append ( hit . meta . id ) # add labels to each event per index for _index , ids in update_map . items (): # split the update requests into chunks of at most max result window update_chunks = [ ids [ i : i + self . max_result_window ] for i in range ( 0 , len ( ids ), self . max_result_window ) ] for chunk in update_chunks : update = UpdateByQuery ( using = es , index = _index ) . query ( \"ids\" , values = chunk ) # apply labels to events result += apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) return result check_parent ( self , parent_query , min_match , dataset_config , es ) \u00b6 Executes a parent query and returns if there were enough result rows. Parameters: Name Type Description Default parent_query QueryBase The parent query to execute required min_match int The minimum number of result rows required required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required Returns: Type Description bool True if the query returned >= min_match rows and False otherwise. Source code in dataset/labels.py def check_parent ( self , parent_query : QueryBase , min_match : int , dataset_config : DatasetConfig , es : Elasticsearch , ) -> bool : \"\"\"Executes a parent query and returns if there were enough result rows. Args: parent_query: The parent query to execute min_match: The minimum number of result rows required dataset_config: The dataset configuration es: The elasticsearch client object Returns: `True` if the query returned >= min_match rows and `False` otherwise. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , parent_query . indices_prefix_dataset , parent_query . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( parent_query . query , List ): parent_query . query = [ parent_query . query ] if not isinstance ( parent_query . filter_ , List ): parent_query . filter_ = ( [ parent_query . filter_ ] if parent_query . filter_ is not None else [] ) if not isinstance ( parent_query . exclude , List ): parent_query . exclude = ( [ parent_query . exclude ] if parent_query . exclude is not None else [] ) for _query in parent_query . query : search = search . query ( _query ) for _filter in parent_query . filter_ : search = search . filter ( _filter ) for _exclude in parent_query . exclude : search = search . exclude ( _exclude ) return search . execute () . hits . total . value >= min_match update_params ( self ) inherited \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } UpdateSubQueryRule pydantic-model \u00b6 Labeling rule that labels the results of multiple sub queries. This labeling rule first executes a base query to retrieve information. It then renders and executes a templated sub query for each row retrieved from the base query. The result rows of these dynamically generated sub queries are then labled. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.sub_query id : attacker.foothold.apache.access_dropped labels : - attacker_http - foothold description : >- This rule tries to match attacker requests that we where unable to match to a labeled response with access log entries. Such cases can happen if the corresponding response gets lost in the network or otherwise is not sent. index : - pcap-attacker_0 # obligatory match all query : - term : destination.ip : \"172.16.0.217\" filter : - term : event.category : http - term : event.action : request # we are looking for requests that have not been marked as attacker http yet # most likely they did not have a matching response due to some network error # or timeout - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] sub_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"172.16.100.151\" filter : - range : \"@timestamp\" : # the access log entry should be after the request, but since the access log # does not have microseconds we drop them here as well gte : \"{{ (HIT['@timestamp'] | as_datetime).replace(microsecond=0) }}\" # the type of error we are looking for should create an access log entry almost immediately # se we keep the time frame short lte : \"{{ ( HIT['@timestamp'] | as_datetime).replace(microsecond=0) + timedelta(seconds=1) }}\" description : str pydantic-field \u00b6 An optional description for the rule exclude : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 Similar to filters, but used to exclude results filter_ : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field \u00b6 The filter/s to limit queried to documents to only those that match the filters id_ : str pydantic-field required \u00b6 The unique rule id index : Union [ List [ str ], str ] pydantic-field \u00b6 The indices to query (by default prefixed with the dataset name) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. labels : str pydantic-field required \u00b6 The list of labels to apply to log lines matching this rule query : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]] pydantic-field required \u00b6 The query/s to use for identifying log lines to apply the tags to. sub_query : QueryBase pydantic-field required \u00b6 The templated sub query to use to apply the labels. Executed for each hit of the parent query. type_field : str pydantic-field required \u00b6 The rule type as passed in from the config apply ( self , dataset_dir , dataset_config , es , update_script_id , label_object ) \u00b6 Applies the labels to all rows matching the created sub queries. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all rows matching the created sub queries. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set search = search . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : search = search . query ( _query ) for _filter in self . filter_ : search = search . filter ( _filter ) for _exclude in self . exclude : search = search . exclude ( _exclude ) result = 0 for hit in search . params ( scroll = \"30m\" ) . scan (): # make deep copy of sub query so we can template it sub_query = self . sub_query . copy ( deep = True ) # render the subquery sub_query = render_query_base ( hit , sub_query ) sub_rule = UpdateByQueryRule ( type = \"elasticsearch.query\" , id = self . id_ , labels = self . labels , description = self . description , index = sub_query . index , query = sub_query . query , filter = sub_query . filter_ , exclude = sub_query . exclude , indices_prefix_dataset = sub_query . indices_prefix_dataset , ) result += sub_rule . apply ( dataset_dir , dataset_config , es , update_script_id , label_object ) return result update_params ( self ) inherited \u00b6 Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , } apply_labels_by_query ( es , query , script_params , update_script_id , index = '_all' , check_interval = 0.5 ) \u00b6 Utility function for applying labels based on a DSL query. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required query Dict[str, Any] The DSL query dict required script_params Dict[str, Any] The parameters for the label update script. required update_script_id str The label update script to use required index Union[List[str], str] The indices to query '_all' check_interval float The amount in time between the status checks. 0.5 Returns: Type Description int The number of updated rows. Source code in dataset/labels.py def apply_labels_by_query ( es : Elasticsearch , query : Dict [ str , Any ], script_params : Dict [ str , Any ], update_script_id : str , index : Union [ List [ str ], str ] = \"_all\" , check_interval : float = 0.5 , ) -> int : \"\"\"Utility function for applying labels based on a DSL query. Args: es: The elasticsearch client object query: The DSL query dict script_params: The parameters for the label update script. update_script_id: The label update script to use index: The indices to query check_interval: The amount in time between the status checks. Returns: The number of updated rows. \"\"\" update = UpdateByQuery ( using = es , index = index ) update = update . update_from_dict ( query ) return apply_labels_by_update_dsl ( update , script_params , update_script_id , check_interval ) apply_labels_by_update_dsl ( update , script_params , update_script_id , check_interval = 0.5 ) \u00b6 Utility function for applying labels based on a DSL query object. The function takes a update by query object and uses the Cyber Range Kyoushi label update script to apply the given labels to all matching rows. This function is blocking as it waits for the process to complete. Parameters: Name Type Description Default update UpdateByQuery The update by query object containing the DSL query. required script_params Dict[str, Any] The parameters for the label update script. required update_script_id str The label update script to use required check_interval float The amount in time between the status checks. 0.5 Returns: Type Description int The number of updated rows. Source code in dataset/labels.py def apply_labels_by_update_dsl ( update : UpdateByQuery , script_params : Dict [ str , Any ], update_script_id : str , check_interval : float = 0.5 , ) -> int : \"\"\"Utility function for applying labels based on a DSL query object. The function takes a update by query object and uses the Cyber Range Kyoushi label update script to apply the given labels to all matching rows. This function is blocking as it waits for the process to complete. Args: update: The update by query object containing the DSL query. script_params: The parameters for the label update script. update_script_id: The label update script to use check_interval: The amount in time between the status checks. Returns: The number of updated rows. \"\"\" # refresh=True is important so that consecutive rules # have a consitant state es : Elasticsearch = get_connection ( update . _using ) # add update script update = update . script ( id = update_script_id , params = script_params ) # run update task task = update . params ( refresh = True , wait_for_completion = False ) . execute () . task task_info = es . tasks . get ( task_id = task ) while not task_info [ \"completed\" ]: sleep ( check_interval ) task_info = es . tasks . get ( task_id = task ) with warnings . catch_warnings (): # ToDo: Elasticsearch (7.12) does not provide any API to delete update by query tasks # The only option is to delete the document directly this will be deprecated # and as such gives warnings. For now we ignore these and wait for elasticsearch # to provide an API for it. warnings . simplefilter ( \"ignore\" ) es . delete ( index = \".tasks\" , doc_type = \"task\" , id = task ) return task_info [ \"response\" ][ \"updated\" ] create_kyoushi_scripts ( es , dataset_name , label_object = 'kyoushi_labels' ) \u00b6 Utility function for creating labeling update, filter and field scripts. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required dataset_name str The name of the dataset to create the scripts for. This is used as prefix for script names to ensure different versions of these scripts can exist for different datasets in the same Elasticsearch instance. required label_object str The name of field to store labeling information in. 'kyoushi_labels' Source code in dataset/labels.py def create_kyoushi_scripts ( es : Elasticsearch , dataset_name : str , label_object : str = \"kyoushi_labels\" , ): \"\"\"Utility function for creating labeling update, filter and field scripts. Args: es: The elasticsearch client object dataset_name: The name of the dataset to create the scripts for. This is used as prefix for script names to ensure different versions of these scripts can exist for different datasets in the same Elasticsearch instance. label_object: The name of field to store labeling information in. \"\"\" update_script = { \"script\" : { \"description\" : \"Kyoushi Dataset - Update by Query label script\" , \"lang\" : \"painless\" , \"source\" : UPDATE_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_update\" , body = update_script , context = \"update\" ) filter_script = { \"script\" : { \"lang\" : \"painless\" , \"source\" : LABEL_FILTER_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_filter\" , body = filter_script , context = \"filter\" , ) labels_field = { \"script\" : { \"lang\" : \"painless\" , \"source\" : LABELS_FIELD_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_field\" , body = labels_field , context = \"field\" , ) get_label_counts ( es , index = None , label_object = 'kyoushi_labels' ) \u00b6 Utility function for getting number of rows per label. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index Union[List[str], str] The indices to get the information for None label_object str The field containing the label information 'kyoushi_labels' Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of result buckets Source code in dataset/labels.py def get_label_counts ( es : Elasticsearch , index : Union [ List [ str ], str , None ] = None , label_object : str = \"kyoushi_labels\" , ) -> List [ Bucket ]: \"\"\"Utility function for getting number of rows per label. Args: es: The elasticsearch client object index: The indices to get the information for label_object: The field containing the label information Returns: List of result buckets \"\"\" # disable request cache to ensure we always get latest info search_labels = Search ( using = es , index = index ) . params ( request_cache = False ) runtime_mappings = { \"labels\" : { \"type\" : \"keyword\" , \"script\" : LABELS_AGGREGATES_FIELD_SCRIPT , } } search_labels = search_labels . extra ( runtime_mappings = runtime_mappings ) # setup aggregations search_labels . aggs . bucket ( \"labels\" , \"composite\" , sources = [{ \"label\" : { \"terms\" : { \"field\" : \"labels\" }}}], ) search_labels . aggs [ \"labels\" ] . bucket ( \"file\" , \"terms\" , field = \"log.file.path\" ) return scan_composite ( search_labels , \"labels\" ) render_query_base ( hit , query ) \u00b6 Utility function for rendering sub or parent query. Parameters: Name Type Description Default hit Hit The Elasticsearch query result row to render for. required query QueryBase The templated query definition. required Returns: Type Description QueryBase The rendered DSL query Source code in dataset/labels.py def render_query_base ( hit : Hit , query : QueryBase ) -> QueryBase : \"\"\"Utility function for rendering sub or parent query. Args: hit: The Elasticsearch query result row to render for. query: The templated query definition. Returns: The rendered DSL query \"\"\" variables = { \"HIT\" : hit } # render the index var if isinstance ( query . index , str ): query . index = render_template ( query . index , variables ) elif isinstance ( query . index , Sequence ): query . index = [ render_template ( i , variables ) for i in query . index ] # ensure we have lists if not isinstance ( query . query , List ): query . query = [ query . query ] if not isinstance ( query . filter_ , List ): query . filter_ = [ query . filter_ ] if query . filter_ is not None else [] if not isinstance ( query . exclude , List ): query . exclude = [ query . exclude ] if query . exclude is not None else [] query . query = [ render_template_recursive ( _query , variables ) for _query in query . query ] query . filter_ = [ render_template_recursive ( _filter , variables ) for _filter in query . filter_ ] query . exclude = [ render_template_recursive ( _exclude , variables ) for _exclude in query . exclude ] return query","title":"Labels"},{"location":"reference/labels/#labels-module","text":"This module contains labeling rule implementations and utility functions used during the labeling process","title":"Labels module"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase","text":"Pydantic model representing an EQL query","title":"EqlQueryBase"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.batch_size","text":"The amount of sequences to update with each batch. Cannot be bigger than max_result_window","title":"batch_size"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.by","text":"Optional global sequence by fields","title":"by"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.event_category_field","text":"The field used to categories events","title":"event_category_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.max_result_window","text":"The max result window allowed on the elasticsearch instance","title":"max_result_window"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.max_span","text":"Optional max time span in which a sequence must occur to be considered a match","title":"max_span"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.sequences","text":"Event sequences to search. Must contain at least two events.","title":"sequences"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.tiebreaker_field","text":"(Optional, string) Field used to sort hits with the same timestamp in ascending order.","title":"tiebreaker_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.timestamp_field","text":"The field containing the event timestamp","title":"timestamp_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.until","text":"Optional until event marking the end of valid sequences. The until event will not be labeled.","title":"until"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlQueryBase.query","text":"Converts the EQL query object into an EQL query string. Returns: Type Description str The query in EQL syntax Source code in dataset/labels.py def query ( self ) -> str : \"\"\"Converts the EQL query object into an EQL query string. Returns: The query in EQL syntax \"\"\" query = \"sequence\" if self . by is not None : if isinstance ( self . by , Text ): query += f \" by { self . by } \" elif len ( self . by ) > 0 : query += f \" by { ', ' . join ( self . by ) } \" if self . max_span is not None : query += f \" with maxspan= { self . max_span } \" for sequence in self . sequences : query += f \" \\n { sequence } \" if self . until is not None : query += f \" \\n until { self . until } \" return query","title":"query()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule","text":"Applies labels to a sequence of log events defined by an EQL query. This labeling rule is defined as an EQL query . Using this syntax it is possible to define a sequence of related events and retrieve them. All events part of retrieved sequences are then labeled. Examples: - type : elasticsearch.sequence id : attacker.webshell.upload.seq labels : [ webshell_upload ] description : >- This rule labels the web shell upload step by matching the 3 step sequence within the foothold phase. index : - apache_access-intranet_server # since we do these requests very fast # we need the line number as tie breaker tiebreaker_field : log.file.line by : source.address max_span : 2m filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" sequences : - '[ apache where event.action == \"access\" and url.original == \"/\" ]' - '[ apache where event.action == \"access\" and url.original == \"/?p=5\" ]' - '[ apache where event.action == \"access\" and http.request.method == \"POST\" and url.original == \"/wp-admin/admin-ajax.php\" ]'","title":"EqlSequenceRule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.batch_size","text":"The amount of sequences to update with each batch. Cannot be bigger than max_result_window","title":"batch_size"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.by","text":"Optional global sequence by fields","title":"by"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.event_category_field","text":"The field used to categories events","title":"event_category_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.max_result_window","text":"The max result window allowed on the elasticsearch instance","title":"max_result_window"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.max_span","text":"Optional max time span in which a sequence must occur to be considered a match","title":"max_span"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.sequences","text":"Event sequences to search. Must contain at least two events.","title":"sequences"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.tiebreaker_field","text":"(Optional, string) Field used to sort hits with the same timestamp in ascending order.","title":"tiebreaker_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.timestamp_field","text":"The field containing the event timestamp","title":"timestamp_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.until","text":"Optional until event marking the end of valid sequences. The until event will not be labeled.","title":"until"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.apply","text":"Applies the labels to all events part of retrieved sequences. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all events part of retrieved sequences. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) body = self . _make_body ( label_object ) updated = 0 # as of elk 7.12 of there is no way to ensure we get all even through the EQL api # (there is no scan or search after like for the DSL API) # so manually search in batches for sequences with events that are not labeled yet # we stop only when we do not get any results anymore i.e., all events have been labeled # this is obviously not the most efficient approach but its the best we can do for now while True : hits = search_eql ( es , index , body ) if hits [ \"total\" ][ \"value\" ] > 0 : index_ids : Dict [ str , List [ str ]] = {} # we have to sort the events by indices # because ids are only guranteed to be uniq per index for sequence in hits [ \"sequences\" ]: for event in sequence [ \"events\" ]: index_ids . setdefault ( event [ \"_index\" ], []) . append ( event [ \"_id\" ]) # add labels to each event per index for _index , ids in index_ids . items (): # split the update requests into chunks of at most max result window update_chunks = [ ids [ i : i + self . max_result_window ] for i in range ( 0 , len ( ids ), self . max_result_window ) ] for chunk in update_chunks : update = UpdateByQuery ( using = es , index = _index ) . query ( \"ids\" , values = chunk ) # apply labels to events updated += apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) else : # end loop once we do not find new events anymore break return updated","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.query","text":"Converts the EQL query object into an EQL query string. Returns: Type Description str The query in EQL syntax Source code in dataset/labels.py def query ( self ) -> str : \"\"\"Converts the EQL query object into an EQL query string. Returns: The query in EQL syntax \"\"\" query = \"sequence\" if self . by is not None : if isinstance ( self . by , Text ): query += f \" by { self . by } \" elif len ( self . by ) > 0 : query += f \" by { ', ' . join ( self . by ) } \" if self . max_span is not None : query += f \" with maxspan= { self . max_span } \" for sequence in self . sequences : query += f \" \\n { sequence } \" if self . until is not None : query += f \" \\n until { self . until } \" return query","title":"query()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.EqlSequenceRule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.LabelException","text":"Generic exception for errors during labeling phase.","title":"LabelException"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Labeler","text":"Cyber Range Kyoushi labeler This class is used to configure and execute the labeling process.","title":"Labeler"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Labeler.__init__","text":"Parameters: Name Type Description Default rule_types Dict[str, Any] Dictionary containing all available labeling rule types. {} update_script_id str The Elasticsearch ID for the update script. 'kyoushi_label_update' label_object str The field to store labels in. 'kyoushi_labels' Source code in dataset/labels.py def __init__ ( self , rule_types : Dict [ str , Any ] = {}, update_script_id : str = \"kyoushi_label_update\" , label_object : str = \"kyoushi_labels\" , ): \"\"\" Args: rule_types : Dictionary containing all available labeling rule types. update_script_id: The Elasticsearch ID for the update script. label_object: The field to store labels in. \"\"\" self . rule_types : Dict [ str , Any ] = rule_types # default rule types self . rule_types . update ( { NoopRule . type_ : NoopRule , UpdateByQueryRule . type_ : UpdateByQueryRule , UpdateSubQueryRule . type_ : UpdateSubQueryRule , UpdateParentQueryRule . type_ : UpdateParentQueryRule , EqlSequenceRule . type_ : EqlSequenceRule , } ) self . update_script_id : str = update_script_id self . label_object : str = label_object","title":"__init__()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Labeler.add_label_object_mapping","text":"Utility function for adding the field definitions for the label field. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object. required dataset_name str The name of the dataset. required rules List[cr_kyoushi.dataset.labels.Rule] List of all to be applied rules. required Source code in dataset/labels.py def add_label_object_mapping ( self , es : Elasticsearch , dataset_name : str , rules : List [ Rule ], ): \"\"\"Utility function for adding the field definitions for the label field. Args: es: The elasticsearch client object. dataset_name: The name of the dataset. rules: List of all to be applied rules. \"\"\" root = Mapping () flat = {} list_ = {} for rule in rules : flat [ rule . id_ ] = Keyword () list_ [ rule . id_ ] = Keyword ( multi = True ) properties = { \"flat\" : { \"properties\" : flat , }, \"list\" : { \"properties\" : list_ , }, \"rules\" : { \"type\" : \"keyword\" }, } root . field ( self . label_object , \"object\" , properties = properties ) es . indices . put_mapping ( index = f \" { dataset_name } -*\" , body = root . to_dict ())","title":"add_label_object_mapping()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Labeler.execute","text":"Parse and apply all labeling rules. Note This function only write labels to the database. The resulting labels can be written to the file system using the write(...) function of the labeler. Parameters: Name Type Description Default rules List[Dict[str, Any]] The unparsed list of labeling rules. required dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required Exceptions: Type Description ValidationError If a labeling rule or configuration is invalid LabelException If an error occurs while applying a labeling rule Source code in dataset/labels.py def execute ( self , rules : List [ Dict [ str , Any ]], dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , ): \"\"\"Parse and apply all labeling rules. !!! Note This function only write labels to the database. The resulting labels can be written to the file system using the `write(...)` function of the labeler. Args: rules: The unparsed list of labeling rules. dataset_dir: The dataset path dataset_config: The dataset configuration es: The elasticsearch client object Raises: ValidationError: If a labeling rule or configuration is invalid LabelException: If an error occurs while applying a labeling rule \"\"\" # validate the general rule list RuleList . parse_obj ( rules ) # convert and validate rule types rule_objects : List [ Rule ] = [] errors : List [ ErrorWrapper ] = [] for r in rules : try : rule_objects . append ( self . rule_types [ r [ \"type\" ]] . parse_obj ( r )) except ValidationError as e : errors . append ( ErrorWrapper ( e , r [ \"id\" ])) if len ( errors ) > 0 : raise ValidationError ( errors , RuleList ) # create mappings for rule label fields # we need to do this since EQL queries cannot check existence of non mapped fields self . add_label_object_mapping ( es , dataset_config . name , rule_objects ) # ensure update script exists create_kyoushi_scripts ( es , dataset_config . name , self . label_object ) # start labeling process for rule in rule_objects : try : print ( f \"Applying rule { rule . id_ } ...\" ) updated = rule . apply ( dataset_dir , dataset_config , es , f \" { dataset_config . name } _ { self . update_script_id } \" , self . label_object , ) print ( f \"Rule { rule . id_ } applied labels: { rule . labels } to { updated } lines.\" ) except ElasticsearchRequestError as e : raise LabelException ( f \"Error executing rule ' { rule . id_ } '\" , e )","title":"execute()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Labeler.write","text":"Write labeles stored in the database to the file system. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required index List[str] The indices to consider required skip_files List[str] The files to ignore required Source code in dataset/labels.py def write ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , index : List [ str ], skip_files : List [ str ], ): \"\"\"Write labeles stored in the database to the file system. Args: dataset_dir: The dataset path dataset_config: The dataset configuration es: The elasticsearch client object index: The indices to consider skip_files: The files to ignore \"\"\" files = self . _get_label_files ( dataset_config , es , index , skip_files ) for current_file in files : # disable request cache to ensure we always get latest info search_labeled = Search ( using = es , index = f \" { dataset_config . name } -*\" ) . params ( request_cache = False , preserve_order = True ) search_labeled = search_labeled . filter ( \"exists\" , field = f \" { self . label_object } .rules\" ) . filter ( \"term\" , log__file__path = current_file ) search_labeled = search_labeled . sort ({ \"log.file.line\" : \"asc\" }) base_path = dataset_dir . joinpath ( LAYOUT . GATHER . value ) label_path = dataset_dir . joinpath ( LAYOUT . LABELS . value ) label_file_path = label_path . joinpath ( Path ( current_file ) . relative_to ( base_path ) ) print ( f \"Start writing { current_file } \" ) self . _write_file ( search_labeled , label_file_path )","title":"write()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule","text":"A noop rule that does absolutely nothing.","title":"NoopRule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.apply","text":"Applies the NoopRule i.e., does nothing. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int Always returns 0 since nothing happens. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the NoopRule i.e., does nothing. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: Always returns 0 since nothing happens. \"\"\" return 0","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.NoopRule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase","text":"Base model for DSL query based labeling rules","title":"QueryBase"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.exclude","text":"Similar to filters, but used to exclude results","title":"exclude"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.query","text":"The query/s to use for identifying log lines to apply the tags to.","title":"query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.validate_exclude","text":"Validator to check the DSL query exclude syntax Exclude is simply a negative filter clause i.e., not (DSL QUERY). Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL exclude required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the exclude not be valid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated exclude Source code in dataset/labels.py @validator ( \"exclude\" ) def validate_exclude ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator to check the DSL query exclude syntax Exclude is simply a negative filter clause i.e., not (DSL QUERY). Args: value: The DSL exclude values: The model dictionary Raises: ValidationError: Should the exclude not be valid Returns: The validated exclude \"\"\" # temporary update by query object used to validate the input excludes _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , exclude in enumerate ( value ): try : _temp = _temp . exclude ( exclude ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value","title":"validate_exclude()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.validate_filter","text":"Validator to check the DSL query filter syntax Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL filter required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the filter not be valid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated filter Source code in dataset/labels.py @validator ( \"filter_\" ) def validate_filter ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator to check the DSL query filter syntax Args: value: The DSL filter values: The model dictionary Raises: ValidationError: Should the filter not be valid Returns: The validated filter \"\"\" # temporary update by query object used to validate the input filters _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , filter_ in enumerate ( value ): try : _temp = _temp . filter ( filter_ ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value","title":"validate_filter()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.QueryBase.validate_queries","text":"Validator checking DSL syntax Parameters: Name Type Description Default value Union[List[Dict[str, Any]], Dict[str, Any]] The DSL query required values Dict[str, Any] The model dictionary required Exceptions: Type Description ValidationError Should the given query be invalid Returns: Type Description Union[List[Dict[str, Any]], Dict[str, Any]] The validated query Source code in dataset/labels.py @validator ( \"query\" ) def validate_queries ( cls , value : Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]], values : Dict [ str , Any ] ) -> Union [ List [ Dict [ str , Any ]], Dict [ str , Any ]]: \"\"\"Validator checking DSL syntax Args: value: The DSL query values: The model dictionary Raises: ValidationError: Should the given query be invalid Returns: The validated query \"\"\" # temporary update by query object used to validate the input queries _temp = UpdateByQuery () errors = [] if not isinstance ( value , List ): value = [ value ] for i , query in enumerate ( value ): try : _temp = _temp . query ( query ) except ( TypeError , UnknownDslObject , ValueError ) as e : errors . append ( ErrorWrapper ( e , ( i ,))) if len ( errors ) > 0 : raise ValidationError ( errors , UpdateByQueryRule ) return value","title":"validate_queries()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Rule","text":"Interface definition for labeling rules.","title":"Rule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Rule.apply","text":"Applies the configured rule and assigns the labels to all matching rows. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the rule was applied to Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the configured rule and assigns the labels to all matching rows. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the rule was applied to \"\"\" ...","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.Rule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" ...","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase","text":"Pydantic base model for labeling rules. This class can be extended to define custom labeling rules.","title":"RuleBase"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.apply","text":"Applies the configured rule and assigns the labels to all matching rows. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the rule was applied to Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the configured rule and assigns the labels to all matching rows. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the rule was applied to \"\"\" raise NotImplementedError ()","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleBase.validate_label_no_semicolon","text":"Validator ensuring label names do not contain ; . Parameters: Name Type Description Default val str A single label required Returns: Type Description str The validated label Source code in dataset/labels.py @validator ( \"labels\" , each_item = True ) def validate_label_no_semicolon ( cls , val : str ) -> str : \"\"\"Validator ensuring label names do not contain `;`. Args: val: A single label Returns: The validated label \"\"\" assert \";\" not in val , f \"Labels must not contain semicolons, but got ' { val } '\" return val","title":"validate_label_no_semicolon()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleList","text":"Model definition of a list of labeling rules.","title":"RuleList"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.RuleList.check_rule_ids_uniq","text":"Validator for ensuring that rule IDs are unique. Parameters: Name Type Description Default values Dict[str, List[cr_kyoushi.dataset.labels.RuleBase]] The model dictionary required Returns: Type Description Dict[str, List[cr_kyoushi.dataset.labels.RuleBase]] The validated model dictionary Source code in dataset/labels.py @root_validator def check_rule_ids_uniq ( cls , values : Dict [ str , List [ RuleBase ]] ) -> Dict [ str , List [ RuleBase ]]: \"\"\"Validator for ensuring that rule IDs are unique. Args: values: The model dictionary Returns: The validated model dictionary \"\"\" duplicates = set () temp = [] if \"__root__\" in values : for r in values [ \"__root__\" ]: if r . id_ in temp : duplicates . add ( r . id_ ) else : temp . append ( r . id_ ) assert ( len ( duplicates ) == 0 ), f \"Rule IDs must be uniq, but got duplicates: { duplicates } \" return values","title":"check_rule_ids_uniq()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule","text":"Applies labels based on a simple Elasticsearch DSL query. Examples: - type : elasticsearch.query id : attacker.foothold.vpn.ip labels : - attacker_vpn - foothold description : >- This rule applies the labels to all openvpn log rows that have the attacker server as source ip and are within the foothold phase. index : - openvpn-vpn filter : range : \"@timestamp\" : # foothold phase start gte : \"2021-03-23 20:31:00+00:00\" # foothold phase stop lte : \"2021-03-23 21:13:52+00:00\" query : - match : source.ip : '192.42.0.255'","title":"UpdateByQueryRule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.exclude","text":"Similar to filters, but used to exclude results","title":"exclude"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.query","text":"The query/s to use for identifying log lines to apply the tags to.","title":"query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.apply","text":"Applies the labels to all rows matching the DSL query. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all rows matching the DSL query. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) update = UpdateByQuery ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set update = update . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : update = update . query ( _query ) for _filter in self . filter_ : update = update . filter ( _filter ) for _exclude in self . exclude : update = update . exclude ( _exclude ) result = apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) return result","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateByQueryRule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule","text":"Applies the labels to all rows of a base query for which a parent query returns results. This labeling rule first executes a base query to retrieve rows we might want to apply labels to. It then renders and executes a templated parent query for each retrieved row. The parent queries are then used to indicate if the initial result row should be labeled or not. By default result rows of the base query are labeled if the corresponding parent query returns at leas one row. It is possible to configure this minimum number e.g., to require at least two results. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.parent_query id : attacker.foothold.apache.error_access labels : - attacker_http - foothold description : >- This rule looks for unlabeled error messages resulting from VPN server traffic within the attack time and tries to match it to an already labeled access log row. index : - apache_error-intranet_server query : match : source.address : \"172.16.100.151\" filter : # use script query to match only entries that # are not already tagged for as attacker http in the foothold phase - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] parent_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"{{ HIT.source.address }}\" # we are looking for parents that are labeled as attacker http - bool : must : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] filter : - range : # parent must be within +-1s of potential child \"@timestamp\" : gte : \"{{ (HIT['@timestamp'] | as_datetime) - timedelta(seconds=1) }}\" lte : \"{{ ( HIT['@timestamp'] | as_datetime) + timedelta(seconds=1) }}\"","title":"UpdateParentQueryRule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.exclude","text":"Similar to filters, but used to exclude results","title":"exclude"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.max_result_window","text":"The max result window allowed on the elasticsearch instance","title":"max_result_window"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.min_match","text":"The minimum number of parent matches needed for the main query to be labeled.","title":"min_match"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.parent_query","text":"The templated parent query to check if the labels should be applied to a query hit.","title":"parent_query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.query","text":"The query/s to use for identifying log lines to apply the tags to.","title":"query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.apply","text":"Applies the labels to result from the base query for which a parent was found. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to result from the base query for which a parent was found. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set search = search . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : search = search . query ( _query ) for _filter in self . filter_ : search = search . filter ( _filter ) for _exclude in self . exclude : search = search . exclude ( _exclude ) result = 0 update_map : Dict [ str , List [ str ]] = {} _i = 0 for hit in search . scan (): _i += 1 # make deep copy of parent query so we can template it parent_query = self . parent_query . copy ( deep = True ) # render the subquery parent_query = render_query_base ( hit , parent_query ) if self . check_parent ( parent_query , self . min_match , dataset_config , es ): update_map . setdefault ( hit . meta . index , []) . append ( hit . meta . id ) # add labels to each event per index for _index , ids in update_map . items (): # split the update requests into chunks of at most max result window update_chunks = [ ids [ i : i + self . max_result_window ] for i in range ( 0 , len ( ids ), self . max_result_window ) ] for chunk in update_chunks : update = UpdateByQuery ( using = es , index = _index ) . query ( \"ids\" , values = chunk ) # apply labels to events result += apply_labels_by_update_dsl ( update , self . update_params (), update_script_id ) return result","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.check_parent","text":"Executes a parent query and returns if there were enough result rows. Parameters: Name Type Description Default parent_query QueryBase The parent query to execute required min_match int The minimum number of result rows required required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required Returns: Type Description bool True if the query returned >= min_match rows and False otherwise. Source code in dataset/labels.py def check_parent ( self , parent_query : QueryBase , min_match : int , dataset_config : DatasetConfig , es : Elasticsearch , ) -> bool : \"\"\"Executes a parent query and returns if there were enough result rows. Args: parent_query: The parent query to execute min_match: The minimum number of result rows required dataset_config: The dataset configuration es: The elasticsearch client object Returns: `True` if the query returned >= min_match rows and `False` otherwise. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , parent_query . indices_prefix_dataset , parent_query . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( parent_query . query , List ): parent_query . query = [ parent_query . query ] if not isinstance ( parent_query . filter_ , List ): parent_query . filter_ = ( [ parent_query . filter_ ] if parent_query . filter_ is not None else [] ) if not isinstance ( parent_query . exclude , List ): parent_query . exclude = ( [ parent_query . exclude ] if parent_query . exclude is not None else [] ) for _query in parent_query . query : search = search . query ( _query ) for _filter in parent_query . filter_ : search = search . filter ( _filter ) for _exclude in parent_query . exclude : search = search . exclude ( _exclude ) return search . execute () . hits . total . value >= min_match","title":"check_parent()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateParentQueryRule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule","text":"Labeling rule that labels the results of multiple sub queries. This labeling rule first executes a base query to retrieve information. It then renders and executes a templated sub query for each row retrieved from the base query. The result rows of these dynamically generated sub queries are then labled. Note The sub query uses Jinja2 syntax for templating. The information retrieved by the base query can be accessed through the HIT variable. Examples: - type : elasticsearch.sub_query id : attacker.foothold.apache.access_dropped labels : - attacker_http - foothold description : >- This rule tries to match attacker requests that we where unable to match to a labeled response with access log entries. Such cases can happen if the corresponding response gets lost in the network or otherwise is not sent. index : - pcap-attacker_0 # obligatory match all query : - term : destination.ip : \"172.16.0.217\" filter : - term : event.category : http - term : event.action : request # we are looking for requests that have not been marked as attacker http yet # most likely they did not have a matching response due to some network error # or timeout - bool : must_not : - script : script : id : test_dataset_kyoushi_label_filter params : labels : [ attacker_http ] sub_query : index : - apache_access-intranet_server query : - term : url.full : \"{{ HIT.url.full }}\" - term : source.address : \"172.16.100.151\" filter : - range : \"@timestamp\" : # the access log entry should be after the request, but since the access log # does not have microseconds we drop them here as well gte : \"{{ (HIT['@timestamp'] | as_datetime).replace(microsecond=0) }}\" # the type of error we are looking for should create an access log entry almost immediately # se we keep the time frame short lte : \"{{ ( HIT['@timestamp'] | as_datetime).replace(microsecond=0) + timedelta(seconds=1) }}\"","title":"UpdateSubQueryRule"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.description","text":"An optional description for the rule","title":"description"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.exclude","text":"Similar to filters, but used to exclude results","title":"exclude"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.filter_","text":"The filter/s to limit queried to documents to only those that match the filters","title":"filter_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.id_","text":"The unique rule id","title":"id_"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.index","text":"The indices to query (by default prefixed with the dataset name)","title":"index"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.labels","text":"The list of labels to apply to log lines matching this rule","title":"labels"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.query","text":"The query/s to use for identifying log lines to apply the tags to.","title":"query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.sub_query","text":"The templated sub query to use to apply the labels. Executed for each hit of the parent query.","title":"sub_query"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.type_field","text":"The rule type as passed in from the config","title":"type_field"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.apply","text":"Applies the labels to all rows matching the created sub queries. Parameters: Name Type Description Default dataset_dir Path The dataset base directory required dataset_config DatasetConfig The dataset configuration required es Elasticsearch The elasticsearch client object required update_script_id str The label update script ID required label_object str The field used to store label information required Returns: Type Description int The number of rows the labels were applied to. Source code in dataset/labels.py def apply ( self , dataset_dir : Path , dataset_config : DatasetConfig , es : Elasticsearch , update_script_id : str , label_object : str , ) -> int : \"\"\"Applies the labels to all rows matching the created sub queries. Args: dataset_dir: The dataset base directory dataset_config: The dataset configuration es: The elasticsearch client object update_script_id: The label update script ID label_object: The field used to store label information Returns: The number of rows the labels were applied to. \"\"\" index : Optional [ Union [ Sequence [ str ], str ]] = resolve_indices ( dataset_config . name , self . indices_prefix_dataset , self . index ) search = Search ( using = es , index = index ) # ensure we have lists if not isinstance ( self . query , List ): self . query = [ self . query ] if not isinstance ( self . filter_ , List ): self . filter_ = [ self . filter_ ] if self . filter_ is not None else [] if not isinstance ( self . exclude , List ): self . exclude = [ self . exclude ] if self . exclude is not None else [] # exclude already correctly labeled rows from the result set search = search . exclude ( \"term\" , ** { f \" { label_object } .flat. { self . id_ } \" : \";\" . join ( self . labels )}, ) for _query in self . query : search = search . query ( _query ) for _filter in self . filter_ : search = search . filter ( _filter ) for _exclude in self . exclude : search = search . exclude ( _exclude ) result = 0 for hit in search . params ( scroll = \"30m\" ) . scan (): # make deep copy of sub query so we can template it sub_query = self . sub_query . copy ( deep = True ) # render the subquery sub_query = render_query_base ( hit , sub_query ) sub_rule = UpdateByQueryRule ( type = \"elasticsearch.query\" , id = self . id_ , labels = self . labels , description = self . description , index = sub_query . index , query = sub_query . query , filter = sub_query . filter_ , exclude = sub_query . exclude , indices_prefix_dataset = sub_query . indices_prefix_dataset , ) result += sub_rule . apply ( dataset_dir , dataset_config , es , update_script_id , label_object ) return result","title":"apply()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.UpdateSubQueryRule.update_params","text":"Gets the update script parameters required for this rule. Returns: Type Description Dict[str, Any] The update script parameters. Source code in dataset/labels.py def update_params ( self ) -> Dict [ str , Any ]: \"\"\"Gets the update script parameters required for this rule. Returns: The update script parameters. \"\"\" return { \"rule\" : self . id_ , \"labels\" : self . labels , }","title":"update_params()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.apply_labels_by_query","text":"Utility function for applying labels based on a DSL query. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required query Dict[str, Any] The DSL query dict required script_params Dict[str, Any] The parameters for the label update script. required update_script_id str The label update script to use required index Union[List[str], str] The indices to query '_all' check_interval float The amount in time between the status checks. 0.5 Returns: Type Description int The number of updated rows. Source code in dataset/labels.py def apply_labels_by_query ( es : Elasticsearch , query : Dict [ str , Any ], script_params : Dict [ str , Any ], update_script_id : str , index : Union [ List [ str ], str ] = \"_all\" , check_interval : float = 0.5 , ) -> int : \"\"\"Utility function for applying labels based on a DSL query. Args: es: The elasticsearch client object query: The DSL query dict script_params: The parameters for the label update script. update_script_id: The label update script to use index: The indices to query check_interval: The amount in time between the status checks. Returns: The number of updated rows. \"\"\" update = UpdateByQuery ( using = es , index = index ) update = update . update_from_dict ( query ) return apply_labels_by_update_dsl ( update , script_params , update_script_id , check_interval )","title":"apply_labels_by_query()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.apply_labels_by_update_dsl","text":"Utility function for applying labels based on a DSL query object. The function takes a update by query object and uses the Cyber Range Kyoushi label update script to apply the given labels to all matching rows. This function is blocking as it waits for the process to complete. Parameters: Name Type Description Default update UpdateByQuery The update by query object containing the DSL query. required script_params Dict[str, Any] The parameters for the label update script. required update_script_id str The label update script to use required check_interval float The amount in time between the status checks. 0.5 Returns: Type Description int The number of updated rows. Source code in dataset/labels.py def apply_labels_by_update_dsl ( update : UpdateByQuery , script_params : Dict [ str , Any ], update_script_id : str , check_interval : float = 0.5 , ) -> int : \"\"\"Utility function for applying labels based on a DSL query object. The function takes a update by query object and uses the Cyber Range Kyoushi label update script to apply the given labels to all matching rows. This function is blocking as it waits for the process to complete. Args: update: The update by query object containing the DSL query. script_params: The parameters for the label update script. update_script_id: The label update script to use check_interval: The amount in time between the status checks. Returns: The number of updated rows. \"\"\" # refresh=True is important so that consecutive rules # have a consitant state es : Elasticsearch = get_connection ( update . _using ) # add update script update = update . script ( id = update_script_id , params = script_params ) # run update task task = update . params ( refresh = True , wait_for_completion = False ) . execute () . task task_info = es . tasks . get ( task_id = task ) while not task_info [ \"completed\" ]: sleep ( check_interval ) task_info = es . tasks . get ( task_id = task ) with warnings . catch_warnings (): # ToDo: Elasticsearch (7.12) does not provide any API to delete update by query tasks # The only option is to delete the document directly this will be deprecated # and as such gives warnings. For now we ignore these and wait for elasticsearch # to provide an API for it. warnings . simplefilter ( \"ignore\" ) es . delete ( index = \".tasks\" , doc_type = \"task\" , id = task ) return task_info [ \"response\" ][ \"updated\" ]","title":"apply_labels_by_update_dsl()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.create_kyoushi_scripts","text":"Utility function for creating labeling update, filter and field scripts. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required dataset_name str The name of the dataset to create the scripts for. This is used as prefix for script names to ensure different versions of these scripts can exist for different datasets in the same Elasticsearch instance. required label_object str The name of field to store labeling information in. 'kyoushi_labels' Source code in dataset/labels.py def create_kyoushi_scripts ( es : Elasticsearch , dataset_name : str , label_object : str = \"kyoushi_labels\" , ): \"\"\"Utility function for creating labeling update, filter and field scripts. Args: es: The elasticsearch client object dataset_name: The name of the dataset to create the scripts for. This is used as prefix for script names to ensure different versions of these scripts can exist for different datasets in the same Elasticsearch instance. label_object: The name of field to store labeling information in. \"\"\" update_script = { \"script\" : { \"description\" : \"Kyoushi Dataset - Update by Query label script\" , \"lang\" : \"painless\" , \"source\" : UPDATE_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_update\" , body = update_script , context = \"update\" ) filter_script = { \"script\" : { \"lang\" : \"painless\" , \"source\" : LABEL_FILTER_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_filter\" , body = filter_script , context = \"filter\" , ) labels_field = { \"script\" : { \"lang\" : \"painless\" , \"source\" : LABELS_FIELD_SCRIPT . replace ( \"{{ label_object }}\" , label_object ), } } es . put_script ( id = f \" { dataset_name } _kyoushi_label_field\" , body = labels_field , context = \"field\" , )","title":"create_kyoushi_scripts()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.get_label_counts","text":"Utility function for getting number of rows per label. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index Union[List[str], str] The indices to get the information for None label_object str The field containing the label information 'kyoushi_labels' Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of result buckets Source code in dataset/labels.py def get_label_counts ( es : Elasticsearch , index : Union [ List [ str ], str , None ] = None , label_object : str = \"kyoushi_labels\" , ) -> List [ Bucket ]: \"\"\"Utility function for getting number of rows per label. Args: es: The elasticsearch client object index: The indices to get the information for label_object: The field containing the label information Returns: List of result buckets \"\"\" # disable request cache to ensure we always get latest info search_labels = Search ( using = es , index = index ) . params ( request_cache = False ) runtime_mappings = { \"labels\" : { \"type\" : \"keyword\" , \"script\" : LABELS_AGGREGATES_FIELD_SCRIPT , } } search_labels = search_labels . extra ( runtime_mappings = runtime_mappings ) # setup aggregations search_labels . aggs . bucket ( \"labels\" , \"composite\" , sources = [{ \"label\" : { \"terms\" : { \"field\" : \"labels\" }}}], ) search_labels . aggs [ \"labels\" ] . bucket ( \"file\" , \"terms\" , field = \"log.file.path\" ) return scan_composite ( search_labels , \"labels\" )","title":"get_label_counts()"},{"location":"reference/labels/#cr_kyoushi.dataset.labels.render_query_base","text":"Utility function for rendering sub or parent query. Parameters: Name Type Description Default hit Hit The Elasticsearch query result row to render for. required query QueryBase The templated query definition. required Returns: Type Description QueryBase The rendered DSL query Source code in dataset/labels.py def render_query_base ( hit : Hit , query : QueryBase ) -> QueryBase : \"\"\"Utility function for rendering sub or parent query. Args: hit: The Elasticsearch query result row to render for. query: The templated query definition. Returns: The rendered DSL query \"\"\" variables = { \"HIT\" : hit } # render the index var if isinstance ( query . index , str ): query . index = render_template ( query . index , variables ) elif isinstance ( query . index , Sequence ): query . index = [ render_template ( i , variables ) for i in query . index ] # ensure we have lists if not isinstance ( query . query , List ): query . query = [ query . query ] if not isinstance ( query . filter_ , List ): query . filter_ = [ query . filter_ ] if query . filter_ is not None else [] if not isinstance ( query . exclude , List ): query . exclude = [ query . exclude ] if query . exclude is not None else [] query . query = [ render_template_recursive ( _query , variables ) for _query in query . query ] query . filter_ = [ render_template_recursive ( _filter , variables ) for _filter in query . filter_ ] query . exclude = [ render_template_recursive ( _exclude , variables ) for _exclude in query . exclude ] return query","title":"render_query_base()"},{"location":"reference/parser/","text":"Parser module \u00b6 This module defines a python interface for using Logstash as a dataset parser. LogstashParser \u00b6 Utility class for controling Logstash __init__ ( self , dataset_config , parser_config , logstash ) special \u00b6 Parameters: Name Type Description Default dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The logstash configuration (e.g., CLI options etc.) required logstash Path The path to the logstash executable required Source code in dataset/parser.py def __init__ ( self , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , logstash : Path , ): \"\"\" Args: dataset_config: The dataset configuration parser_config: The logstash configuration (e.g., CLI options etc.) logstash: The path to the logstash executable \"\"\" self . dataset_config : DatasetConfig = dataset_config self . parser_config : LogstashParserConfig = parser_config self . logstash = logstash self . _child : Optional [ subprocess . Popen ] = None self . _sigint_handler : Union [ Callable [[ signal . Signals , FrameType ], None ], int , signal . Handlers , None ] = None parse ( self ) \u00b6 Execute the parsing process by running logstash as sub process. Source code in dataset/parser.py def parse ( self ): \"\"\"Execute the parsing process by running logstash as sub process.\"\"\" args = [ str ( self . logstash . absolute ()), \"--path.settings\" , str ( self . parser_config . settings_dir . absolute ()), ] if self . parser_config . log_level is not None : args . extend ([ \"--log.level\" , self . parser_config . log_level ]) self . proc = subprocess . Popen ( args ) self . proc . wait ()","title":"Parser"},{"location":"reference/parser/#parser-module","text":"This module defines a python interface for using Logstash as a dataset parser.","title":"Parser module"},{"location":"reference/parser/#cr_kyoushi.dataset.parser.LogstashParser","text":"Utility class for controling Logstash","title":"LogstashParser"},{"location":"reference/parser/#cr_kyoushi.dataset.parser.LogstashParser.__init__","text":"Parameters: Name Type Description Default dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The logstash configuration (e.g., CLI options etc.) required logstash Path The path to the logstash executable required Source code in dataset/parser.py def __init__ ( self , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , logstash : Path , ): \"\"\" Args: dataset_config: The dataset configuration parser_config: The logstash configuration (e.g., CLI options etc.) logstash: The path to the logstash executable \"\"\" self . dataset_config : DatasetConfig = dataset_config self . parser_config : LogstashParserConfig = parser_config self . logstash = logstash self . _child : Optional [ subprocess . Popen ] = None self . _sigint_handler : Union [ Callable [[ signal . Signals , FrameType ], None ], int , signal . Handlers , None ] = None","title":"__init__()"},{"location":"reference/parser/#cr_kyoushi.dataset.parser.LogstashParser.parse","text":"Execute the parsing process by running logstash as sub process. Source code in dataset/parser.py def parse ( self ): \"\"\"Execute the parsing process by running logstash as sub process.\"\"\" args = [ str ( self . logstash . absolute ()), \"--path.settings\" , str ( self . parser_config . settings_dir . absolute ()), ] if self . parser_config . log_level is not None : args . extend ([ \"--log.level\" , self . parser_config . log_level ]) self . proc = subprocess . Popen ( args ) self . proc . wait ()","title":"parse()"},{"location":"reference/pcap/","text":"Pcap module \u00b6 This module defines utility functions for working and converting PCAP files. convert_pcap_to_ecs ( pcap , dest , tls_keylog = None , tshark_bin = None , remove_index_messages = False , remove_filtered = False , packet_summary = False , packet_details = True , read_filter = None , protocol_match_filter = None , protocol_match_filter_parent = None ) \u00b6 Converts the given pcap file into elasticsearch compatbile format. Calling convert_pcap_to_ecs(pcap, dest) is equivalent to tshark -r pcap -T ek > dest Note See https://www.wireshark.org/docs/man-pages/tshark.html#j-protocol-match-filter for details on the match filters and other options. Parameters: Name Type Description Default pcap Path The pcap file to convert required dest Path The destination file required tls_keylog Optional[pathlib.Path] TLS keylog file to decrypt TLS on the fly. None tshark_bin Optional[pathlib.Path] Path to your tshark binary (searches in common paths if not supplied) None remove_index_messages bool If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. False remove_filtered bool Remove filtered fields from the event dicts. False packet_summary bool If the packet summaries should be included (-P option). False packet_details bool If the packet details should be included, when packet_summary=False then details are always included (-V option). True read_filter Optional[str] The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) None protocol_match_filter Optional[str] Display filter for protocols and their fields (-J option). Parent and child nodes are included for all matches lower level protocols must be added explicitly. None protocol_match_filter_parent Optional[str] Display filter for protocols and their fields. Only partent nodes are included (-j option). None Source code in dataset/pcap.py def convert_pcap_to_ecs ( pcap : Path , dest : Path , tls_keylog : Optional [ Path ] = None , tshark_bin : Optional [ Path ] = None , remove_index_messages : bool = False , remove_filtered : bool = False , packet_summary : bool = False , packet_details : bool = True , read_filter : Optional [ str ] = None , protocol_match_filter : Optional [ str ] = None , protocol_match_filter_parent : Optional [ str ] = None , ): \"\"\"Converts the given pcap file into elasticsearch compatbile format. Calling `convert_pcap_to_ecs(pcap, dest)` is equivalent to ``` tshark -r pcap -T ek > dest ``` !!! Note See https://www.wireshark.org/docs/man-pages/tshark.html#j-protocol-match-filter for details on the match filters and other options. Args: pcap: The pcap file to convert dest: The destination file tls_keylog: TLS keylog file to decrypt TLS on the fly. tshark_bin: Path to your tshark binary (searches in common paths if not supplied) remove_index_messages: If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. remove_filtered: Remove filtered fields from the event dicts. packet_summary: If the packet summaries should be included (-P option). packet_details: If the packet details should be included, when packet_summary=False then details are always included (-V option). read_filter: The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) protocol_match_filter: Display filter for protocols and their fields (-J option). Parent and child nodes are included for all matches lower level protocols must be added explicitly. protocol_match_filter_parent: Display filter for protocols and their fields. Only partent nodes are included (-j option). \"\"\" # set path to tshark bin use argument or search in common paths tshark_path = ( tshark_bin . absolute () if tshark_bin is not None else get_process_path () ) tshark_version = get_tshark_version ( tshark_path ) args = [ tshark_path , \"-r\" , str ( pcap . absolute ()), \"-T\" , \"ek\" ] # configure tls keylog file for decryption if tls_keylog is not None : keylog_pref = ( \"tls.keylog_file\" # all ssl prefs were renamed to tls with wireshark 3.0 if tshark_version >= LooseVersion ( \"3.0\" ) else \"ssl.keylog_file\" ) args . extend ([ \"-o\" , f \" { keylog_pref } : { tls_keylog . absolute () } \" ]) if packet_summary : args . append ( \"-P\" ) if packet_details : args . append ( \"-V\" ) if read_filter is not None : args . extend ([ \"-Y\" , read_filter ]) if protocol_match_filter is not None : args . extend ([ \"-J\" , protocol_match_filter ]) if protocol_match_filter_parent is not None : args . extend ([ \"-j\" , protocol_match_filter_parent ]) proc = subprocess . Popen ( args , stdout = subprocess . PIPE ) # regex used to skip all index lines from the bulk format index_regex = re . compile ( r '{\"index\":{\"_index\":\".*\",\"_type\":\".*\"}}' ) with open ( dest , \"w\" ) as dest_file : assert proc . stdout is not None , \"TShark process stdout should be available\" for line in io . TextIOWrapper ( proc . stdout , encoding = \"utf-8\" , errors = \"replace\" ): # when remove index is true discard all index lines if not remove_index_messages or not index_regex . match ( line ): if remove_filtered : line = pcap_ecs_remove_filtered ( line ) dest_file . write ( line ) # ensure tshark process has finished proc . wait () pcap_ecs_remove_filtered ( line ) \u00b6 Removes any useless filtered keys from a ek JSON line. Depending on the used display and read filters the tshark conversion process adds filtered keys to fields that have are beeing filter through read or display filters. These markers do not add any value and even break the PCAP field mapping. As such we check for them and remove any we find. Parameters: Name Type Description Default line str The ek JSON line required Returns: Type Description str The modified JSON line Source code in dataset/pcap.py def pcap_ecs_remove_filtered ( line : str ) -> str : \"\"\"Removes any useless filtered keys from a `ek` JSON line. Depending on the used display and read filters the tshark conversion process adds `filtered` keys to fields that have are beeing filter through read or display filters. These markers do not add any value and even break the PCAP field mapping. As such we check for them and remove any we find. Args: line: The `ek` JSON line Returns: The modified JSON line \"\"\" data = ujson . loads ( line ) # we need to re-add the line break that gets lost due to json load return ujson . dumps ( __pcap_ecs_remove_filtered ( data , object ())) + \" \\n \"","title":"PCAP Utilities"},{"location":"reference/pcap/#pcap-module","text":"This module defines utility functions for working and converting PCAP files.","title":"Pcap module"},{"location":"reference/pcap/#cr_kyoushi.dataset.pcap.convert_pcap_to_ecs","text":"Converts the given pcap file into elasticsearch compatbile format. Calling convert_pcap_to_ecs(pcap, dest) is equivalent to tshark -r pcap -T ek > dest Note See https://www.wireshark.org/docs/man-pages/tshark.html#j-protocol-match-filter for details on the match filters and other options. Parameters: Name Type Description Default pcap Path The pcap file to convert required dest Path The destination file required tls_keylog Optional[pathlib.Path] TLS keylog file to decrypt TLS on the fly. None tshark_bin Optional[pathlib.Path] Path to your tshark binary (searches in common paths if not supplied) None remove_index_messages bool If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. False remove_filtered bool Remove filtered fields from the event dicts. False packet_summary bool If the packet summaries should be included (-P option). False packet_details bool If the packet details should be included, when packet_summary=False then details are always included (-V option). True read_filter Optional[str] The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) None protocol_match_filter Optional[str] Display filter for protocols and their fields (-J option). Parent and child nodes are included for all matches lower level protocols must be added explicitly. None protocol_match_filter_parent Optional[str] Display filter for protocols and their fields. Only partent nodes are included (-j option). None Source code in dataset/pcap.py def convert_pcap_to_ecs ( pcap : Path , dest : Path , tls_keylog : Optional [ Path ] = None , tshark_bin : Optional [ Path ] = None , remove_index_messages : bool = False , remove_filtered : bool = False , packet_summary : bool = False , packet_details : bool = True , read_filter : Optional [ str ] = None , protocol_match_filter : Optional [ str ] = None , protocol_match_filter_parent : Optional [ str ] = None , ): \"\"\"Converts the given pcap file into elasticsearch compatbile format. Calling `convert_pcap_to_ecs(pcap, dest)` is equivalent to ``` tshark -r pcap -T ek > dest ``` !!! Note See https://www.wireshark.org/docs/man-pages/tshark.html#j-protocol-match-filter for details on the match filters and other options. Args: pcap: The pcap file to convert dest: The destination file tls_keylog: TLS keylog file to decrypt TLS on the fly. tshark_bin: Path to your tshark binary (searches in common paths if not supplied) remove_index_messages: If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. remove_filtered: Remove filtered fields from the event dicts. packet_summary: If the packet summaries should be included (-P option). packet_details: If the packet details should be included, when packet_summary=False then details are always included (-V option). read_filter: The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) protocol_match_filter: Display filter for protocols and their fields (-J option). Parent and child nodes are included for all matches lower level protocols must be added explicitly. protocol_match_filter_parent: Display filter for protocols and their fields. Only partent nodes are included (-j option). \"\"\" # set path to tshark bin use argument or search in common paths tshark_path = ( tshark_bin . absolute () if tshark_bin is not None else get_process_path () ) tshark_version = get_tshark_version ( tshark_path ) args = [ tshark_path , \"-r\" , str ( pcap . absolute ()), \"-T\" , \"ek\" ] # configure tls keylog file for decryption if tls_keylog is not None : keylog_pref = ( \"tls.keylog_file\" # all ssl prefs were renamed to tls with wireshark 3.0 if tshark_version >= LooseVersion ( \"3.0\" ) else \"ssl.keylog_file\" ) args . extend ([ \"-o\" , f \" { keylog_pref } : { tls_keylog . absolute () } \" ]) if packet_summary : args . append ( \"-P\" ) if packet_details : args . append ( \"-V\" ) if read_filter is not None : args . extend ([ \"-Y\" , read_filter ]) if protocol_match_filter is not None : args . extend ([ \"-J\" , protocol_match_filter ]) if protocol_match_filter_parent is not None : args . extend ([ \"-j\" , protocol_match_filter_parent ]) proc = subprocess . Popen ( args , stdout = subprocess . PIPE ) # regex used to skip all index lines from the bulk format index_regex = re . compile ( r '{\"index\":{\"_index\":\".*\",\"_type\":\".*\"}}' ) with open ( dest , \"w\" ) as dest_file : assert proc . stdout is not None , \"TShark process stdout should be available\" for line in io . TextIOWrapper ( proc . stdout , encoding = \"utf-8\" , errors = \"replace\" ): # when remove index is true discard all index lines if not remove_index_messages or not index_regex . match ( line ): if remove_filtered : line = pcap_ecs_remove_filtered ( line ) dest_file . write ( line ) # ensure tshark process has finished proc . wait ()","title":"convert_pcap_to_ecs()"},{"location":"reference/pcap/#cr_kyoushi.dataset.pcap.pcap_ecs_remove_filtered","text":"Removes any useless filtered keys from a ek JSON line. Depending on the used display and read filters the tshark conversion process adds filtered keys to fields that have are beeing filter through read or display filters. These markers do not add any value and even break the PCAP field mapping. As such we check for them and remove any we find. Parameters: Name Type Description Default line str The ek JSON line required Returns: Type Description str The modified JSON line Source code in dataset/pcap.py def pcap_ecs_remove_filtered ( line : str ) -> str : \"\"\"Removes any useless filtered keys from a `ek` JSON line. Depending on the used display and read filters the tshark conversion process adds `filtered` keys to fields that have are beeing filter through read or display filters. These markers do not add any value and even break the PCAP field mapping. As such we check for them and remove any we find. Args: line: The `ek` JSON line Returns: The modified JSON line \"\"\" data = ujson . loads ( line ) # we need to re-add the line break that gets lost due to json load return ujson . dumps ( __pcap_ecs_remove_filtered ( data , object ())) + \" \\n \"","title":"pcap_ecs_remove_filtered()"},{"location":"reference/processors/","text":"Processors module \u00b6 This module contains the base definitions of dataset pipeline processors and also the core processors shipped with the Cyber Range Kyoushi Dataset package. ProcessorList \u00b6 Type alias for a list of processors ComponentTemplateCreateProcessor pydantic-model \u00b6 Processor for creating Elasticsearch index component templates. This processor can be used to create Elasticsearch index component templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap component template type : elasticsearch.component_template template : processing/logstash/pcap-component-template.json template_name : pcap context : ProcessorContext pydantic-field \u00b6 The variable context for the processor create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. name : str pydantic-field required \u00b6 The processors name template : FilePath pydantic-field required \u00b6 The index component template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index component template type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and configure Elasticsearch index component template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index component template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) ies = ClusterClient ( es ) ies . put_component_template ( name = self . template_name , body = template_data , create = self . create_only , ) CreateDirectoryProcessor pydantic-model \u00b6 Processor for creating file directories. Examples: - name : Ensure processing config directory exists type : mkdir path : processing/config context : ProcessorContext pydantic-field \u00b6 The variable context for the processor name : str pydantic-field required \u00b6 The processors name path : Path pydantic-field required \u00b6 The directory path to create recursive : bool pydantic-field \u00b6 If all missing parent directories should als be created type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and create the directory. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and create the directory. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" self . path . mkdir ( parents = self . recursive , exist_ok = True ) ForEachProcessor pydantic-model \u00b6 For each processor This is a special processor container allowing for the dynamic creation of a list of processor based on a list of items. Examples: - name : Render labeling rules type : foreach # processing/templates/rules items : - src : 0_auth.yaml.j2 dest : 0_auth.yaml - src : apache.yaml.j2 dest : apache.yaml - src : audit.yaml.j2 dest : audit.yaml - src : openvpn.yaml.j2 dest : openvpn.yaml processor : type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\" context : ProcessorContext pydantic-field \u00b6 The variable context for the processor items : Any pydantic-field required \u00b6 List of items to create processors for loop_var : str pydantic-field \u00b6 The variable name to use for current loops item in the processor context name : str pydantic-field required \u00b6 The processors name processor : Any pydantic-field required \u00b6 The processor template config to create multiple instances of type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" raise NotImplementedError ( \"Incomplete processor implementation!\" ) processors ( self ) \u00b6 Create a list of processors for each item. Returns: Type Description List[Dict[str, Any]] List of processors based on the given items and processor template. Source code in dataset/processors.py def processors ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Create a list of processors for each item. Returns: List of processors based on the given items and processor template. \"\"\" processors = [] for item in self . items : processor = copy . deepcopy ( self . processor ) # set the loop var if \"context\" in processor : context = processor [ \"context\" ] else : context = self . context . dict () processor [ \"context\" ] = context variables = context . setdefault ( \"vars\" , {}) variables [ self . loop_var ] = item processors . append ( processor ) return processors GzipProcessor pydantic-model \u00b6 Processor for decompressing gzip files. It is possible to either define a glob of gzip files or a path to a single gzip file. If a glob is defined it is resolved relative to the defined path (default= <dataset dir> ). Examples: - name : Decompress all GZIP logs type : gzip path : gather glob : \"*/logs/**/*.gz\" context : ProcessorContext pydantic-field \u00b6 The variable context for the processor glob : str pydantic-field \u00b6 The file glob expression to use name : str pydantic-field required \u00b6 The processors name path : Path pydantic-field \u00b6 The base path to search for the gzipped files. type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and decompress the files. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and decompress the files. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" files : Iterable if self . glob is None : files = [ self . path ] else : files = self . path . glob ( self . glob ) for gzip_file in files : with gzip . open ( gzip_file , \"rb\" ) as f_in : # with suffix replaces .gz ending with open ( gzip_file . with_suffix ( \"\" ), \"wb\" ) as f_out : shutil . copyfileobj ( f_in , f_out ) # delete the gzip file gzip_file . unlink () IngestCreateProcessor pydantic-model \u00b6 Processor for creating Elasticsearch ingest pipelines. This processor can be used to create Elasticsearch ingest pipelines for parsing log event. The log file parsing can then be configured to use the pipelines for upstream parsing instead of local Logstash parsing. Examples: - name : Add auditd ingest pipeline to elasticsearch type : elasticsearch.ingest ingest_pipeline : processing/logstash/auditd-ingest.yml ingest_pipeline_id : auditd-logs context : ProcessorContext pydantic-field \u00b6 The variable context for the processor ingest_pipeline : FilePath pydantic-field required \u00b6 The ingest pipeline to add to elasticsearch ingest_pipeline_id : str pydantic-field required \u00b6 The id to use for the ingest pipeline name : str pydantic-field required \u00b6 The processors name type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and configure Elasticsearch ingest pipeline. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch ingest pipeline. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" pipeline_data = load_file ( self . ingest_pipeline ) ies = IngestClient ( es ) ies . put_pipeline ( id = self . ingest_pipeline_id , body = pipeline_data ) LegacyTemplateCreateProcessor pydantic-model \u00b6 Processor for configuring Elasticsearch legacy index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the legacy index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.legacy_template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ] context : ProcessorContext pydantic-field \u00b6 The variable context for the processor create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. index_patterns : str pydantic-field \u00b6 The index patterns the template should be applied to. If this is not set then the index template file must contain this information already! indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. name : str pydantic-field required \u00b6 The processors name order : int pydantic-field \u00b6 The order to assign to this index template (higher values take precedent). template : FilePath pydantic-field required \u00b6 The index template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index template type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and configure Elasticsearch index template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) # configure the index patterns if self . index_patterns is not None : template_data [ \"index_patterns\" ] = ( # if prefix is on add the prefix to all patterns [ f \" { dataset_config . name } - { p } \" for p in self . index_patterns ] if self . indices_prefix_dataset # else add list as is else self . index_patterns ) ies = IndicesClient ( es ) ies . put_template ( name = self . template_name , body = template_data , create = self . create_only , order = self . order , ) LogstashSetupProcessor pydantic-model \u00b6 Logstash parser setup processor. This processor is used to create all the configuration files required for the Logstash parser (e.g., input and filter configs). Unless you provide a static Logstash parsing configuration you must invoke this processor at somepoint during the pre-processing phase. Note The processor only does the basic setup any Logstash parsing filters used for processing specific log events must be prepared separately. Examples: - name : Setup logstash pipeline type : logstash.setup context : var_files : servers : processing/config/servers.yaml servers : \"{{ servers }}\" context : ProcessorContext pydantic-field \u00b6 The variable context for the processor index_template_template : Path pydantic-field \u00b6 The template to use for the elasticsearch dataset index patterns index template input_config_name : str pydantic-field \u00b6 The name of the log inputs config file. (relative to the pipeline config dir) input_template : Path pydantic-field \u00b6 The template to use for the file input plugin configuration legacy_index_template_template : Path pydantic-field \u00b6 The template to use for the elasticsearch dataset legacy index patterns index template logstash_template : Path pydantic-field \u00b6 The template to use for the logstash configuration name : str pydantic-field required \u00b6 The processors name output_config_name : str pydantic-field \u00b6 The name of the log outputs config file. (relative to the pipeline config dir) output_template : Path pydantic-field \u00b6 The template to use for the file output plugin configuration piplines_template : Path pydantic-field \u00b6 The template to use for the logstash pipelines configuration pre_process_name : str pydantic-field \u00b6 The file name to use for the pre process filters config. This is prefixed with 0000_ to ensure that the filters are run first. pre_process_template : Path pydantic-field \u00b6 The template to use for the file output plugin configuration servers : Any pydantic-field required \u00b6 Dictionary of servers and their log configurations type_field : str pydantic-field required \u00b6 The processor type as passed in from the config use_legacy_template : bool pydantic-field \u00b6 If the output config should use the legacy index template or the modern index template default_server_timezone ( v ) classmethod \u00b6 Validate that each log config entry has a timezone or set default. Parameters: Name Type Description Default v Dict[str, Any] A single log configuration element required Returns: Type Description Dict[str, Any] The validated and parsed log configuration element Source code in dataset/processors.py @validator ( \"servers\" , each_item = True ) def default_server_timezone ( cls , v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validate that each log config entry has a timezone or set default. Args: v: A single log configuration element Returns: The validated and parsed log configuration element \"\"\" if \"timezone\" not in v : v [ \"timezone\" ] = \"UTC\" return v execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and configure Logstash Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Logstash Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" variables = self . context . load () variables . update ( { \"DATASET_DIR\" : dataset_dir , \"DATASET\" : dataset_config , \"PARSER\" : parser_config , \"servers\" : self . servers , \"USE_LEGACY_TEMPLATE\" : self . use_legacy_template , } ) # add elasticsearch connection variables variables . update ( get_transport_variables ( es )) # create all logstash directories create_dirs ( [ parser_config . settings_dir , parser_config . conf_dir , parser_config . data_dir , parser_config . log_dir , ] ) # copy jvm and log4j config to settings dir if they don't exist copy_package_file ( \"cr_kyoushi.dataset.files\" , \"jvm.options\" , parser_config . settings_dir . joinpath ( \"jvm.options\" ), ) copy_package_file ( \"cr_kyoushi.dataset.files\" , \"log4j2.properties\" , parser_config . settings_dir . joinpath ( \"log4j2.properties\" ), ) # write logstash configuration write_template ( self . logstash_template , parser_config . settings_dir . joinpath ( \"logstash.yml\" ), variables , es , ) # write pipelines configuration write_template ( self . piplines_template , parser_config . settings_dir . joinpath ( \"pipelines.yml\" ), variables , es , ) # write index template write_template ( self . index_template_template , parser_config . settings_dir . joinpath ( f \" { dataset_config . name } -index-template.json\" ), variables , es , ) # write legacy index template write_template ( self . legacy_index_template_template , parser_config . settings_dir . joinpath ( f \" { dataset_config . name } -legacy-index-template.json\" ), variables , es , ) # write input configuration write_template ( self . input_template , parser_config . conf_dir . joinpath ( self . input_config_name ), variables , es , ) # write output configuration write_template ( self . output_template , parser_config . conf_dir . joinpath ( self . output_config_name ), variables , es , ) # write pre process configuration write_template ( self . pre_process_template , parser_config . conf_dir . joinpath ( self . pre_process_name ), variables , es , ) validate_servers ( v ) classmethod \u00b6 Validate the server logs configuration. Parameters: Name Type Description Default v Dict[str, Any] A single log configuration element required Returns: Type Description Dict[str, Any] The validated and parsed log configuration element Source code in dataset/processors.py @validator ( \"servers\" , each_item = True ) def validate_servers ( cls , v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validate the server logs configuration. Args: v: A single log configuration element Returns: The validated and parsed log configuration element \"\"\" assert \"logs\" in v , \"Each server must have a logs configuration\" v [ \"logs\" ] = parse_obj_as ( List [ LogstashLogConfig ], v [ \"logs\" ]) return v PcapElasticsearchProcessor pydantic-model \u00b6 Processor for converting PCAP files to ndjson format. This processor uses tshark to convert PCAP files to a line based JSON format ( ek output). Examples: - name : Convert attacker pcap to elasticsearch json type : pcap.elasticsearch pcap : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.pcap dest : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.json tls_keylog : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/premaster.txt read_filter : \"tcp or udp or icmp\" context : ProcessorContext pydantic-field \u00b6 The variable context for the processor create_destination_dirs : bool pydantic-field \u00b6 If the processor should create missing destination parent directories dest : Path pydantic-field required \u00b6 The destination file force : bool pydantic-field \u00b6 If the pcap should be created even when the destination file already exists. name : str pydantic-field required \u00b6 The processors name packet_details : bool pydantic-field \u00b6 If the packet details should be included, when packet_summary=False then details are always included (-V option). packet_summary : bool pydantic-field \u00b6 If the packet summaries should be included (-P option). pcap : FilePath pydantic-field required \u00b6 The pcap file to convert protocol_match_filter : str pydantic-field \u00b6 Display filter for protocols and their fields (-J option).Parent and child nodes are included for all matches lower level protocols must be added explicitly. protocol_match_filter_parent : str pydantic-field \u00b6 Display filter for protocols and their fields. Only partent nodes are included (-j option). read_filter : str pydantic-field \u00b6 The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option) remove_filtered : bool pydantic-field \u00b6 Remove filtered fields from the event dicts. remove_index_messages : bool pydantic-field \u00b6 If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API. tls_keylog : FilePath pydantic-field \u00b6 TLS keylog file to decrypt TLS on the fly. tshark_bin : FilePath pydantic-field \u00b6 Path to your tshark binary (searches in common paths if not supplied) type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and convert the pcap file. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and convert the pcap file. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . create_destination_dirs : # create destination parent directory if it does not exist self . dest . parent . mkdir ( parents = True , exist_ok = True ) if self . force or not self . dest . exists (): # convert the file convert_pcap_to_ecs ( self . pcap , self . dest , self . tls_keylog , self . tshark_bin , self . remove_index_messages , self . remove_filtered , self . packet_summary , self . packet_details , self . read_filter , self . protocol_match_filter , self . protocol_match_filter_parent , ) PrintProcessor pydantic-model \u00b6 Debug processor that simply prints a message. Examples: - name : Print Hello World type : print msg : Hello World context : ProcessorContext pydantic-field \u00b6 The variable context for the processor msg : str pydantic-field required \u00b6 The message to print name : str pydantic-field required \u00b6 The processors name type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Print the msg Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Print the `msg` Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" print ( self . msg ) Processor \u00b6 Cyber Range Kyoushi Dataset processor interface For the Cyber Range Kyoushi Dataset tool processors are used during the pre and post processing phase. A processor class exposes configuration variables and is executed to achieve a certain task during the processing phases. They work similar to Ansible modules and it is possible to use Jinja2 templates and context variable to define partial configuration. Examples: - name : Render foo template type : template context : vars : foo : bar var_files : gather/server/foo-bar.yaml src : processing/templates/foo.yaml.j2 dest : processing/bar.yaml.j2 execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" render ( context , data ) classmethod \u00b6 Renders all template strings of a processor. Parameters: Name Type Description Default context ProcessorContext The processors context variables required data Dict[str, Any] The raw templated processor configuration. required Returns: Type Description Dict[str, Any] The rendered processor configuration. Source code in dataset/processors.py @classmethod def render ( cls , context : ProcessorContext , data : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Renders all template strings of a processor. Args: context: The processors context variables data: The raw templated processor configuration. Returns: The rendered processor configuration. \"\"\" ... ProcessorBase pydantic-model \u00b6 Pydantic base model of Cyber Range Kyoushi Dataset processor. Use this base model to implement processors it ensures that rendering and data loading is done correctly. context : ProcessorContext pydantic-field \u00b6 The variable context for the processor name : str pydantic-field required \u00b6 The processors name type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" raise NotImplementedError ( \"Incomplete processor implementation!\" ) render ( context , data , es ) classmethod \u00b6 Renders all template strings of a processor. Parameters: Name Type Description Default context ProcessorContext The processors context variables required data Dict[str, Any] The raw templated processor configuration. required Returns: Type Description Dict[str, Any] The rendered processor configuration. Source code in dataset/processors.py @classmethod def render ( cls , context : ProcessorContext , data : Dict [ str , Any ], es : Elasticsearch , ) -> Dict [ str , Any ]: \"\"\"Renders all template strings of a processor. Args: context: The processors context variables data: The raw templated processor configuration. Returns: The rendered processor configuration. \"\"\" # handle main dict data_rendered = {} for key , val in data . items (): # do not render excluded fields if key not in cls . context_render_exclude : data_rendered [ key ] = cls . _render ( context , val , es ) else : data_rendered [ key ] = val return data_rendered ProcessorContainer \u00b6 Cyber Range Kyoushi Dataset processor container interface This interface definition defines the API used for classes implementing processor container types. execute ( self , dataset_dir , dataset_config , parser_config , es ) inherited \u00b6 Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" processors ( self ) \u00b6 Returns a list of processors contained in this container. Returns: Type Description List[Dict[str, Any]] List of processors Source code in dataset/processors.py def processors ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Returns a list of processors contained in this container. Returns: List of processors \"\"\" ... ProcessorContext pydantic-model \u00b6 Processor context model This model is used to configure the variable context used for rendering a processor. It is possible to either define variables directly ( variables ) or load them from variable files ( variable_files ). variable_files : Union [ pathlib . Path , Dict [ str , pathlib . Path ]] pydantic-field \u00b6 Config files to load into the render context variables : Any pydantic-field \u00b6 Context variables to use during rendering load ( self ) \u00b6 Load the configured context variables into a combined dict. The loaded context variables are cached so that variable files are only read on the first call of load() . Returns: Type Description Dict[str, Any] A single dict containing all context variables. Source code in dataset/processors.py def load ( self ) -> Dict [ str , Any ]: \"\"\"Load the configured context variables into a combined dict. The loaded context variables are cached so that variable files are only read on the first call of `load()`. Returns: A single dict containing all context variables. \"\"\" if self . _loaded_variables is None : self . _loaded_variables = load_variables ( self . variable_files ) self . _loaded_variables . update ( self . variables ) return self . _loaded_variables ProcessorPipeline \u00b6 The Cyber Range Kyoushi Dataset processing pipeline implementation. This class is used to configure, parse and execute the pre and post processing steps. __init__ ( self , processor_map = None ) special \u00b6 Parameters: Name Type Description Default processor_map Optional[Dict[str, Any]] Dict of processors to execute None Source code in dataset/processors.py def __init__ ( self , processor_map : Optional [ Dict [ str , Any ]] = None ): \"\"\" Args: processor_map: Dict of processors to execute \"\"\" if processor_map is None : processor_map = {} self . processor_map : Dict [ str , Any ] = processor_map self . processor_map . update ( { PrintProcessor . type_ : PrintProcessor , TemplateProcessor . type_ : TemplateProcessor , ForEachProcessor . type_ : ForEachProcessor , CreateDirectoryProcessor . type_ : CreateDirectoryProcessor , GzipProcessor . type_ : GzipProcessor , LogstashSetupProcessor . type_ : LogstashSetupProcessor , PcapElasticsearchProcessor . type_ : PcapElasticsearchProcessor , ComponentTemplateCreateProcessor . type_ : ComponentTemplateCreateProcessor , TemplateCreateProcessor . type_ : TemplateCreateProcessor , LegacyTemplateCreateProcessor . type_ : LegacyTemplateCreateProcessor , IngestCreateProcessor . type_ : IngestCreateProcessor , TrimProcessor . type_ : TrimProcessor , } ) execute ( self , data , dataset_dir , dataset_config , parser_config , es ) \u00b6 Executes the processor pipeline by running all the configured processors. Parameters: Name Type Description Default data List[Dict[str, Any]] The raw processor information required dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , data : List [ Dict [ str , Any ]], dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor pipeline by running all the configured processors. Args: data: The raw processor information dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" # pre-validate the processor list # check if all processors have a name and type parse_obj_as ( ProcessorList , data ) for p in data : # get the processor context and class context = p . setdefault ( \"context\" , {}) processor_class = self . processor_map [ p [ \"type\" ]] # render the processor template and parse it p_rendered = processor_class . render ( context = ProcessorContext . parse_obj ( context ), data = p , es = es , ) processor = processor_class . parse_obj ( p_rendered ) if isinstance ( processor , ProcessorContainer ): print ( f \"Expanding processor container - { processor . name } ...\" ) self . execute ( processor . processors (), dataset_dir , dataset_config , parser_config , es , ) else : print ( f \"Executing - { processor . name } ...\" ) processor . execute ( dataset_dir , dataset_config , parser_config , es ) TemplateCreateProcessor pydantic-model \u00b6 Processor for configuring Elasticsearch index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ] composed_of : str pydantic-field \u00b6 Optional list of component templates the index template should be composed of. context : ProcessorContext pydantic-field \u00b6 The variable context for the processor create_only : bool pydantic-field \u00b6 If true then an existing template with the given name will not be replaced. index_patterns : str pydantic-field \u00b6 The index patterns the template should be applied to. If this is not set then the index template file must contain this information already! indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. name : str pydantic-field required \u00b6 The processors name priority : int pydantic-field \u00b6 The priority to assign to this index template (higher values take precedent). template : FilePath pydantic-field required \u00b6 The index template to add to elasticsearch template_name : str pydantic-field required \u00b6 The name to use for the index template type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and configure Elasticsearch index template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) # configure the index patterns if self . index_patterns is not None : template_data [ \"index_patterns\" ] = ( # if prefix is on add the prefix to all patterns [ f \" { dataset_config . name } - { p } \" for p in self . index_patterns ] if self . indices_prefix_dataset # else add list as is else self . index_patterns ) # set template priority to given value template_data [ \"priority\" ] = self . priority if self . composed_of is not None : template_data [ \"composed_of\" ] = self . composed_of ies = IndicesClient ( es ) ies . put_index_template ( name = self . template_name , body = template_data , create = self . create_only , ) TemplateProcessor pydantic-model \u00b6 Processor for rendering template files. In addition to the normal processor context it is also possible to define a template_context . If template_context is defined it will be used for rendering the template otherwise the normal processor context will be used. Examples: - type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\" context : ProcessorContext pydantic-field \u00b6 The variable context for the processor dest : Path pydantic-field required \u00b6 The destination to save the rendered file to name : str pydantic-field required \u00b6 The processors name src : Path pydantic-field required \u00b6 The template file to render template_context : ProcessorContext pydantic-field \u00b6 Optional template context if this is not set the processor context is used instead type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Load and render the template file. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Load and render the template file. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . template_context is not None : variables = self . template_context . load () else : variables = self . context . load () variables [ \"DATASET_DIR\" ] = dataset_dir variables [ \"DATASET\" ] = dataset_config variables [ \"PARSER\" ] = parser_config write_template ( self . src , self . dest . absolute (), variables , es , dataset_config ) TrimProcessor pydantic-model \u00b6 Processor for trimming log files to a defined time frame. This processor can be used to remove all log lines outside of defined dataset observation times. Note Currently only support simple time frames with a single start and end time. Examples: - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml # we only want to trim the logs of servers that will be part # of the IDS dataset indices : - attacker_0-* context : ProcessorContext pydantic-field \u00b6 The variable context for the processor end : datetime pydantic-field \u00b6 The end time to trim the logs to (defaults to dataset end) exclude : str pydantic-field \u00b6 Indices to exclude from triming. This will overwrite/exclude indices from any patterns supplied in indices indices : str pydantic-field \u00b6 The log indices to trim (defaults to <dataset>-* ) indices_prefix_dataset : bool pydantic-field \u00b6 If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix. name : str pydantic-field required \u00b6 The processors name start : datetime pydantic-field \u00b6 The start time to trim the logs to (defaults to dataset start) type_field : str pydantic-field required \u00b6 The processor type as passed in from the config execute ( self , dataset_dir , dataset_config , parser_config , es ) \u00b6 Execute the processor and trim the log files. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and trim the log files. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . indices is None : # if not explicitly set use dataset root indices pattern indices = [ f \" { dataset_config . name } -*\" ] else : indices = ( # if given and prefix flag is True add dataset name prefix [ f \" { dataset_config . name } - { ind } \" for ind in self . indices ] if self . indices_prefix_dataset else # otherwise use as is indices ) exclude = ( # add negative match indicator '-' and dataset name prefix [ f \"- { dataset_config . name } - { exc } \" for exc in self . exclude ] if self . indices_prefix_dataset # add negative match indicator only else [ f \"- { exc } \" for exc in self . exclude ] ) start = self . start or dataset_config . start end = self . end or dataset_config . end # exclude must be after indices for negative patterns to work as expected index = indices + exclude # get documents before trim docs_before = { bucket . key . path : bucket . doc_count for bucket in self . get_line_stats ( es , index ) } remove = Search ( using = es , index = index ) # setup trim range filter # start >= @timestamp < end valid_range = Range ( ** { \"@timestamp\" : { \"gte\" : start , \"lt\" : end }}) # remove all elements outside of range i.e., `not` valid_range remove = remove . filter ( ~ valid_range ) # refresh=\"true\" is important to ensure consecutive queries # use up to date information print ( remove . params ( refresh = \"true\" , request_cache = False ) . delete () . to_dict ()) # lines after trim lines_after = self . get_line_stats ( es , index ) # trim each file for bucket in lines_after : first_line = int ( bucket . min_line . value ) last_line : Optional [ int ] = int ( bucket . max_line . value ) if docs_before [ bucket . key . path ] - ( first_line - 1 ) == last_line : # if our last line is already correct we set it to None and skip truncate # since truncating requires us to read up to the truncate point last_line = None trim_file ( bucket . key . path , first_line , last_line ) # delete entry for this file so we later can detect if a file must be deleted completely del docs_before [ bucket . key . path ] # any file that still has a docs before entry # does not have any logs within the trim range and thus should be deleted for path in docs_before . keys (): print ( f \"Removing { path } as it does not have any log lines within the observation time.\" ) # delete the file Path ( path ) . unlink () # update entries in elastic search update_lines = UpdateByQuery ( using = es , index = index ) # adjust map for shifting line numbers in the db to start at our new min line adjust_map = { bucket . key . path : int ( bucket . min_line . value - 1 ) for bucket in lines_after # only include paths that need actual changing if int ( bucket . min_line . value - 1 ) > 0 } # we only have entries to update if the adjust map is non empty if len ( adjust_map ) > 0 : # pre filter our update query to only include file paths we # want to update update_lines = update_lines . filter ( \"terms\" , log__file__path = list ( adjust_map . keys ()), ) # ToDO might be better as async query due to threat of timeouts # (i.e., update_lines.to_dict() and then use low level async API) update_lines . script ( lang = \"painless\" , # subtract matching the entries log file path source = \"ctx._source.log.file.line -= params[ctx._source.log.file.path]\" , params = adjust_map , ) . execute () get_doc_stats ( self , es , index ) \u00b6 Get a list of unique log file paths. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index List[str] The indices to get the data for required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of log file paths. Source code in dataset/processors.py def get_doc_stats ( self , es : Elasticsearch , index : List [ str ]) -> List [ Bucket ]: \"\"\"Get a list of unique log file paths. Args: es: The elasticsearch client object index: The indices to get the data for Returns: List of log file paths. \"\"\" # disable request cache to ensure we always get latest info search_lines = Search ( using = es , index = index ) . params ( request_cache = False ) # setup aggregations search_lines . aggs . bucket ( \"files\" , \"composite\" , sources = [{ \"path\" : { \"terms\" : { \"field\" : \"log.file.path\" }}}], ) # use custom scan function to ensure we get all the buckets return scan_composite ( search_lines , \"files\" ) get_line_stats ( self , es , index ) \u00b6 Retrieve minimum and maximum line numbers for the log files. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index List[str] The indices to get the stats for required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of min and max line numbers per file Source code in dataset/processors.py def get_line_stats ( self , es : Elasticsearch , index : List [ str ]) -> List [ Bucket ]: \"\"\"Retrieve minimum and maximum line numbers for the log files. Args: es: The elasticsearch client object index: The indices to get the stats for Returns: List of min and max line numbers per file \"\"\" # disable request cache to ensure we always get latest info search_lines = Search ( using = es , index = index ) . params ( request_cache = False ) # setup aggregations search_lines . aggs . bucket ( \"files\" , \"composite\" , sources = [{ \"path\" : { \"terms\" : { \"field\" : \"log.file.path\" }}}], ) search_lines . aggs [ \"files\" ] . metric ( \"min_line\" , \"min\" , field = \"log.file.line\" ) search_lines . aggs [ \"files\" ] . metric ( \"max_line\" , \"max\" , field = \"log.file.line\" ) # use custom scan function to ensure we get all the buckets return scan_composite ( search_lines , \"files\" )","title":"Processors"},{"location":"reference/processors/#processors-module","text":"This module contains the base definitions of dataset pipeline processors and also the core processors shipped with the Cyber Range Kyoushi Dataset package.","title":"Processors module"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorList","text":"Type alias for a list of processors","title":"ProcessorList"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor","text":"Processor for creating Elasticsearch index component templates. This processor can be used to create Elasticsearch index component templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap component template type : elasticsearch.component_template template : processing/logstash/pcap-component-template.json template_name : pcap","title":"ComponentTemplateCreateProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.create_only","text":"If true then an existing template with the given name will not be replaced.","title":"create_only"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.template","text":"The index component template to add to elasticsearch","title":"template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.template_name","text":"The name to use for the index component template","title":"template_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ComponentTemplateCreateProcessor.execute","text":"Execute the processor and configure Elasticsearch index component template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index component template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) ies = ClusterClient ( es ) ies . put_component_template ( name = self . template_name , body = template_data , create = self . create_only , )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor","text":"Processor for creating file directories. Examples: - name : Ensure processing config directory exists type : mkdir path : processing/config","title":"CreateDirectoryProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.path","text":"The directory path to create","title":"path"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.recursive","text":"If all missing parent directories should als be created","title":"recursive"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.CreateDirectoryProcessor.execute","text":"Execute the processor and create the directory. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and create the directory. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" self . path . mkdir ( parents = self . recursive , exist_ok = True )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor","text":"For each processor This is a special processor container allowing for the dynamic creation of a list of processor based on a list of items. Examples: - name : Render labeling rules type : foreach # processing/templates/rules items : - src : 0_auth.yaml.j2 dest : 0_auth.yaml - src : apache.yaml.j2 dest : apache.yaml - src : audit.yaml.j2 dest : audit.yaml - src : openvpn.yaml.j2 dest : openvpn.yaml processor : type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\"","title":"ForEachProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.items","text":"List of items to create processors for","title":"items"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.loop_var","text":"The variable name to use for current loops item in the processor context","title":"loop_var"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.processor","text":"The processor template config to create multiple instances of","title":"processor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.execute","text":"Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" raise NotImplementedError ( \"Incomplete processor implementation!\" )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ForEachProcessor.processors","text":"Create a list of processors for each item. Returns: Type Description List[Dict[str, Any]] List of processors based on the given items and processor template. Source code in dataset/processors.py def processors ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Create a list of processors for each item. Returns: List of processors based on the given items and processor template. \"\"\" processors = [] for item in self . items : processor = copy . deepcopy ( self . processor ) # set the loop var if \"context\" in processor : context = processor [ \"context\" ] else : context = self . context . dict () processor [ \"context\" ] = context variables = context . setdefault ( \"vars\" , {}) variables [ self . loop_var ] = item processors . append ( processor ) return processors","title":"processors()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor","text":"Processor for decompressing gzip files. It is possible to either define a glob of gzip files or a path to a single gzip file. If a glob is defined it is resolved relative to the defined path (default= <dataset dir> ). Examples: - name : Decompress all GZIP logs type : gzip path : gather glob : \"*/logs/**/*.gz\"","title":"GzipProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.glob","text":"The file glob expression to use","title":"glob"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.path","text":"The base path to search for the gzipped files.","title":"path"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.GzipProcessor.execute","text":"Execute the processor and decompress the files. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and decompress the files. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" files : Iterable if self . glob is None : files = [ self . path ] else : files = self . path . glob ( self . glob ) for gzip_file in files : with gzip . open ( gzip_file , \"rb\" ) as f_in : # with suffix replaces .gz ending with open ( gzip_file . with_suffix ( \"\" ), \"wb\" ) as f_out : shutil . copyfileobj ( f_in , f_out ) # delete the gzip file gzip_file . unlink ()","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor","text":"Processor for creating Elasticsearch ingest pipelines. This processor can be used to create Elasticsearch ingest pipelines for parsing log event. The log file parsing can then be configured to use the pipelines for upstream parsing instead of local Logstash parsing. Examples: - name : Add auditd ingest pipeline to elasticsearch type : elasticsearch.ingest ingest_pipeline : processing/logstash/auditd-ingest.yml ingest_pipeline_id : auditd-logs","title":"IngestCreateProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.ingest_pipeline","text":"The ingest pipeline to add to elasticsearch","title":"ingest_pipeline"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.ingest_pipeline_id","text":"The id to use for the ingest pipeline","title":"ingest_pipeline_id"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.IngestCreateProcessor.execute","text":"Execute the processor and configure Elasticsearch ingest pipeline. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch ingest pipeline. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" pipeline_data = load_file ( self . ingest_pipeline ) ies = IngestClient ( es ) ies . put_pipeline ( id = self . ingest_pipeline_id , body = pipeline_data )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor","text":"Processor for configuring Elasticsearch legacy index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the legacy index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.legacy_template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ]","title":"LegacyTemplateCreateProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.create_only","text":"If true then an existing template with the given name will not be replaced.","title":"create_only"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.index_patterns","text":"The index patterns the template should be applied to. If this is not set then the index template file must contain this information already!","title":"index_patterns"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.order","text":"The order to assign to this index template (higher values take precedent).","title":"order"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.template","text":"The index template to add to elasticsearch","title":"template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.template_name","text":"The name to use for the index template","title":"template_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LegacyTemplateCreateProcessor.execute","text":"Execute the processor and configure Elasticsearch index template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) # configure the index patterns if self . index_patterns is not None : template_data [ \"index_patterns\" ] = ( # if prefix is on add the prefix to all patterns [ f \" { dataset_config . name } - { p } \" for p in self . index_patterns ] if self . indices_prefix_dataset # else add list as is else self . index_patterns ) ies = IndicesClient ( es ) ies . put_template ( name = self . template_name , body = template_data , create = self . create_only , order = self . order , )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor","text":"Logstash parser setup processor. This processor is used to create all the configuration files required for the Logstash parser (e.g., input and filter configs). Unless you provide a static Logstash parsing configuration you must invoke this processor at somepoint during the pre-processing phase. Note The processor only does the basic setup any Logstash parsing filters used for processing specific log events must be prepared separately. Examples: - name : Setup logstash pipeline type : logstash.setup context : var_files : servers : processing/config/servers.yaml servers : \"{{ servers }}\"","title":"LogstashSetupProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.index_template_template","text":"The template to use for the elasticsearch dataset index patterns index template","title":"index_template_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.input_config_name","text":"The name of the log inputs config file. (relative to the pipeline config dir)","title":"input_config_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.input_template","text":"The template to use for the file input plugin configuration","title":"input_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.legacy_index_template_template","text":"The template to use for the elasticsearch dataset legacy index patterns index template","title":"legacy_index_template_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.logstash_template","text":"The template to use for the logstash configuration","title":"logstash_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.output_config_name","text":"The name of the log outputs config file. (relative to the pipeline config dir)","title":"output_config_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.output_template","text":"The template to use for the file output plugin configuration","title":"output_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.piplines_template","text":"The template to use for the logstash pipelines configuration","title":"piplines_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.pre_process_name","text":"The file name to use for the pre process filters config. This is prefixed with 0000_ to ensure that the filters are run first.","title":"pre_process_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.pre_process_template","text":"The template to use for the file output plugin configuration","title":"pre_process_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.servers","text":"Dictionary of servers and their log configurations","title":"servers"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.use_legacy_template","text":"If the output config should use the legacy index template or the modern index template","title":"use_legacy_template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.default_server_timezone","text":"Validate that each log config entry has a timezone or set default. Parameters: Name Type Description Default v Dict[str, Any] A single log configuration element required Returns: Type Description Dict[str, Any] The validated and parsed log configuration element Source code in dataset/processors.py @validator ( \"servers\" , each_item = True ) def default_server_timezone ( cls , v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validate that each log config entry has a timezone or set default. Args: v: A single log configuration element Returns: The validated and parsed log configuration element \"\"\" if \"timezone\" not in v : v [ \"timezone\" ] = \"UTC\" return v","title":"default_server_timezone()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.execute","text":"Execute the processor and configure Logstash Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Logstash Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" variables = self . context . load () variables . update ( { \"DATASET_DIR\" : dataset_dir , \"DATASET\" : dataset_config , \"PARSER\" : parser_config , \"servers\" : self . servers , \"USE_LEGACY_TEMPLATE\" : self . use_legacy_template , } ) # add elasticsearch connection variables variables . update ( get_transport_variables ( es )) # create all logstash directories create_dirs ( [ parser_config . settings_dir , parser_config . conf_dir , parser_config . data_dir , parser_config . log_dir , ] ) # copy jvm and log4j config to settings dir if they don't exist copy_package_file ( \"cr_kyoushi.dataset.files\" , \"jvm.options\" , parser_config . settings_dir . joinpath ( \"jvm.options\" ), ) copy_package_file ( \"cr_kyoushi.dataset.files\" , \"log4j2.properties\" , parser_config . settings_dir . joinpath ( \"log4j2.properties\" ), ) # write logstash configuration write_template ( self . logstash_template , parser_config . settings_dir . joinpath ( \"logstash.yml\" ), variables , es , ) # write pipelines configuration write_template ( self . piplines_template , parser_config . settings_dir . joinpath ( \"pipelines.yml\" ), variables , es , ) # write index template write_template ( self . index_template_template , parser_config . settings_dir . joinpath ( f \" { dataset_config . name } -index-template.json\" ), variables , es , ) # write legacy index template write_template ( self . legacy_index_template_template , parser_config . settings_dir . joinpath ( f \" { dataset_config . name } -legacy-index-template.json\" ), variables , es , ) # write input configuration write_template ( self . input_template , parser_config . conf_dir . joinpath ( self . input_config_name ), variables , es , ) # write output configuration write_template ( self . output_template , parser_config . conf_dir . joinpath ( self . output_config_name ), variables , es , ) # write pre process configuration write_template ( self . pre_process_template , parser_config . conf_dir . joinpath ( self . pre_process_name ), variables , es , )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.LogstashSetupProcessor.validate_servers","text":"Validate the server logs configuration. Parameters: Name Type Description Default v Dict[str, Any] A single log configuration element required Returns: Type Description Dict[str, Any] The validated and parsed log configuration element Source code in dataset/processors.py @validator ( \"servers\" , each_item = True ) def validate_servers ( cls , v : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Validate the server logs configuration. Args: v: A single log configuration element Returns: The validated and parsed log configuration element \"\"\" assert \"logs\" in v , \"Each server must have a logs configuration\" v [ \"logs\" ] = parse_obj_as ( List [ LogstashLogConfig ], v [ \"logs\" ]) return v","title":"validate_servers()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor","text":"Processor for converting PCAP files to ndjson format. This processor uses tshark to convert PCAP files to a line based JSON format ( ek output). Examples: - name : Convert attacker pcap to elasticsearch json type : pcap.elasticsearch pcap : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.pcap dest : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/traffic.json tls_keylog : gather/attacker_0/logs/ait.aecid.attacker.wpdiscuz/premaster.txt read_filter : \"tcp or udp or icmp\"","title":"PcapElasticsearchProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.create_destination_dirs","text":"If the processor should create missing destination parent directories","title":"create_destination_dirs"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.dest","text":"The destination file","title":"dest"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.force","text":"If the pcap should be created even when the destination file already exists.","title":"force"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.packet_details","text":"If the packet details should be included, when packet_summary=False then details are always included (-V option).","title":"packet_details"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.packet_summary","text":"If the packet summaries should be included (-P option).","title":"packet_summary"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.pcap","text":"The pcap file to convert","title":"pcap"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.protocol_match_filter","text":"Display filter for protocols and their fields (-J option).Parent and child nodes are included for all matches lower level protocols must be added explicitly.","title":"protocol_match_filter"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.protocol_match_filter_parent","text":"Display filter for protocols and their fields. Only partent nodes are included (-j option).","title":"protocol_match_filter_parent"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.read_filter","text":"The read filter to use when reading the pcap file useful to reduce the number of packets (-Y option)","title":"read_filter"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.remove_filtered","text":"Remove filtered fields from the event dicts.","title":"remove_filtered"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.remove_index_messages","text":"If the elasticsearch bulk API index messages should be stripped from the output file. Useful when using logstash or similar instead of the bulk API.","title":"remove_index_messages"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.tls_keylog","text":"TLS keylog file to decrypt TLS on the fly.","title":"tls_keylog"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.tshark_bin","text":"Path to your tshark binary (searches in common paths if not supplied)","title":"tshark_bin"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PcapElasticsearchProcessor.execute","text":"Execute the processor and convert the pcap file. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and convert the pcap file. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . create_destination_dirs : # create destination parent directory if it does not exist self . dest . parent . mkdir ( parents = True , exist_ok = True ) if self . force or not self . dest . exists (): # convert the file convert_pcap_to_ecs ( self . pcap , self . dest , self . tls_keylog , self . tshark_bin , self . remove_index_messages , self . remove_filtered , self . packet_summary , self . packet_details , self . read_filter , self . protocol_match_filter , self . protocol_match_filter_parent , )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor","text":"Debug processor that simply prints a message. Examples: - name : Print Hello World type : print msg : Hello World","title":"PrintProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor.msg","text":"The message to print","title":"msg"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.PrintProcessor.execute","text":"Print the msg Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Print the `msg` Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" print ( self . msg )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.Processor","text":"Cyber Range Kyoushi Dataset processor interface For the Cyber Range Kyoushi Dataset tool processors are used during the pre and post processing phase. A processor class exposes configuration variables and is executed to achieve a certain task during the processing phases. They work similar to Ansible modules and it is possible to use Jinja2 templates and context variable to define partial configuration. Examples: - name : Render foo template type : template context : vars : foo : bar var_files : gather/server/foo-bar.yaml src : processing/templates/foo.yaml.j2 dest : processing/bar.yaml.j2","title":"Processor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.Processor.execute","text":"Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\"","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.Processor.render","text":"Renders all template strings of a processor. Parameters: Name Type Description Default context ProcessorContext The processors context variables required data Dict[str, Any] The raw templated processor configuration. required Returns: Type Description Dict[str, Any] The rendered processor configuration. Source code in dataset/processors.py @classmethod def render ( cls , context : ProcessorContext , data : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Renders all template strings of a processor. Args: context: The processors context variables data: The raw templated processor configuration. Returns: The rendered processor configuration. \"\"\" ...","title":"render()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase","text":"Pydantic base model of Cyber Range Kyoushi Dataset processor. Use this base model to implement processors it ensures that rendering and data loading is done correctly.","title":"ProcessorBase"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase.execute","text":"Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" raise NotImplementedError ( \"Incomplete processor implementation!\" )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorBase.render","text":"Renders all template strings of a processor. Parameters: Name Type Description Default context ProcessorContext The processors context variables required data Dict[str, Any] The raw templated processor configuration. required Returns: Type Description Dict[str, Any] The rendered processor configuration. Source code in dataset/processors.py @classmethod def render ( cls , context : ProcessorContext , data : Dict [ str , Any ], es : Elasticsearch , ) -> Dict [ str , Any ]: \"\"\"Renders all template strings of a processor. Args: context: The processors context variables data: The raw templated processor configuration. Returns: The rendered processor configuration. \"\"\" # handle main dict data_rendered = {} for key , val in data . items (): # do not render excluded fields if key not in cls . context_render_exclude : data_rendered [ key ] = cls . _render ( context , val , es ) else : data_rendered [ key ] = val return data_rendered","title":"render()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContainer","text":"Cyber Range Kyoushi Dataset processor container interface This interface definition defines the API used for classes implementing processor container types.","title":"ProcessorContainer"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContainer.execute","text":"Executes the processor. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Executes the processor. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\"","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContainer.processors","text":"Returns a list of processors contained in this container. Returns: Type Description List[Dict[str, Any]] List of processors Source code in dataset/processors.py def processors ( self ) -> List [ Dict [ str , Any ]]: \"\"\"Returns a list of processors contained in this container. Returns: List of processors \"\"\" ...","title":"processors()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContext","text":"Processor context model This model is used to configure the variable context used for rendering a processor. It is possible to either define variables directly ( variables ) or load them from variable files ( variable_files ).","title":"ProcessorContext"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContext.variable_files","text":"Config files to load into the render context","title":"variable_files"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContext.variables","text":"Context variables to use during rendering","title":"variables"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorContext.load","text":"Load the configured context variables into a combined dict. The loaded context variables are cached so that variable files are only read on the first call of load() . Returns: Type Description Dict[str, Any] A single dict containing all context variables. Source code in dataset/processors.py def load ( self ) -> Dict [ str , Any ]: \"\"\"Load the configured context variables into a combined dict. The loaded context variables are cached so that variable files are only read on the first call of `load()`. Returns: A single dict containing all context variables. \"\"\" if self . _loaded_variables is None : self . _loaded_variables = load_variables ( self . variable_files ) self . _loaded_variables . update ( self . variables ) return self . _loaded_variables","title":"load()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorPipeline","text":"The Cyber Range Kyoushi Dataset processing pipeline implementation. This class is used to configure, parse and execute the pre and post processing steps.","title":"ProcessorPipeline"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorPipeline.__init__","text":"Parameters: Name Type Description Default processor_map Optional[Dict[str, Any]] Dict of processors to execute None Source code in dataset/processors.py def __init__ ( self , processor_map : Optional [ Dict [ str , Any ]] = None ): \"\"\" Args: processor_map: Dict of processors to execute \"\"\" if processor_map is None : processor_map = {} self . processor_map : Dict [ str , Any ] = processor_map self . processor_map . update ( { PrintProcessor . type_ : PrintProcessor , TemplateProcessor . type_ : TemplateProcessor , ForEachProcessor . type_ : ForEachProcessor , CreateDirectoryProcessor . type_ : CreateDirectoryProcessor , GzipProcessor . type_ : GzipProcessor , LogstashSetupProcessor . type_ : LogstashSetupProcessor , PcapElasticsearchProcessor . type_ : PcapElasticsearchProcessor , ComponentTemplateCreateProcessor . type_ : ComponentTemplateCreateProcessor , TemplateCreateProcessor . type_ : TemplateCreateProcessor , LegacyTemplateCreateProcessor . type_ : LegacyTemplateCreateProcessor , IngestCreateProcessor . type_ : IngestCreateProcessor , TrimProcessor . type_ : TrimProcessor , } )","title":"__init__()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.ProcessorPipeline.execute","text":"Executes the processor pipeline by running all the configured processors. Parameters: Name Type Description Default data List[Dict[str, Any]] The raw processor information required dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , data : List [ Dict [ str , Any ]], dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ): \"\"\"Executes the processor pipeline by running all the configured processors. Args: data: The raw processor information dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" # pre-validate the processor list # check if all processors have a name and type parse_obj_as ( ProcessorList , data ) for p in data : # get the processor context and class context = p . setdefault ( \"context\" , {}) processor_class = self . processor_map [ p [ \"type\" ]] # render the processor template and parse it p_rendered = processor_class . render ( context = ProcessorContext . parse_obj ( context ), data = p , es = es , ) processor = processor_class . parse_obj ( p_rendered ) if isinstance ( processor , ProcessorContainer ): print ( f \"Expanding processor container - { processor . name } ...\" ) self . execute ( processor . processors (), dataset_dir , dataset_config , parser_config , es , ) else : print ( f \"Executing - { processor . name } ...\" ) processor . execute ( dataset_dir , dataset_config , parser_config , es )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor","text":"Processor for configuring Elasticsearch index templates. This processor can be used to configure Elasticsearch index templates. To prepare the Elasticsearch instance for the parsing phase. See the index templates doc for more details. Examples: - name : Add pcap index mapping type : elasticsearch.template template : processing/logstash/pcap-index-template.json template_name : pcap index_patterns : [ \"pcap-*\" ]","title":"TemplateCreateProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.composed_of","text":"Optional list of component templates the index template should be composed of.","title":"composed_of"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.create_only","text":"If true then an existing template with the given name will not be replaced.","title":"create_only"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.index_patterns","text":"The index patterns the template should be applied to. If this is not set then the index template file must contain this information already!","title":"index_patterns"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.priority","text":"The priority to assign to this index template (higher values take precedent).","title":"priority"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.template","text":"The index template to add to elasticsearch","title":"template"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.template_name","text":"The name to use for the index template","title":"template_name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateCreateProcessor.execute","text":"Execute the processor and configure Elasticsearch index template. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and configure Elasticsearch index template. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" template_data = load_file ( self . template ) # configure the index patterns if self . index_patterns is not None : template_data [ \"index_patterns\" ] = ( # if prefix is on add the prefix to all patterns [ f \" { dataset_config . name } - { p } \" for p in self . index_patterns ] if self . indices_prefix_dataset # else add list as is else self . index_patterns ) # set template priority to given value template_data [ \"priority\" ] = self . priority if self . composed_of is not None : template_data [ \"composed_of\" ] = self . composed_of ies = IndicesClient ( es ) ies . put_index_template ( name = self . template_name , body = template_data , create = self . create_only , )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor","text":"Processor for rendering template files. In addition to the normal processor context it is also possible to define a template_context . If template_context is defined it will be used for rendering the template otherwise the normal processor context will be used. Examples: - type : template name : Rendering labeling rule {{ item.src }} template_context : var_files : attacker : processing/config/attacker/attacker.yaml escalate : processing/config/attacker/escalate.yaml foothold : processing/config/attacker/foothold.yaml servers : processing/config/servers.yaml src : \"processing/templates/rules/{{ item.src }}\" dest : \"rules/{{ item.dest }}\"","title":"TemplateProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.dest","text":"The destination to save the rendered file to","title":"dest"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.src","text":"The template file to render","title":"src"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.template_context","text":"Optional template context if this is not set the processor context is used instead","title":"template_context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TemplateProcessor.execute","text":"Load and render the template file. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Load and render the template file. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . template_context is not None : variables = self . template_context . load () else : variables = self . context . load () variables [ \"DATASET_DIR\" ] = dataset_dir variables [ \"DATASET\" ] = dataset_config variables [ \"PARSER\" ] = parser_config write_template ( self . src , self . dest . absolute (), variables , es , dataset_config )","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor","text":"Processor for trimming log files to a defined time frame. This processor can be used to remove all log lines outside of defined dataset observation times. Note Currently only support simple time frames with a single start and end time. Examples: - name : Trim server logs to observation time type : dataset.trim context : var_files : groups : processing/config/groups.yaml # we only want to trim the logs of servers that will be part # of the IDS dataset indices : - attacker_0-*","title":"TrimProcessor"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.context","text":"The variable context for the processor","title":"context"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.end","text":"The end time to trim the logs to (defaults to dataset end)","title":"end"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.exclude","text":"Indices to exclude from triming. This will overwrite/exclude indices from any patterns supplied in indices","title":"exclude"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.indices","text":"The log indices to trim (defaults to <dataset>-* )","title":"indices"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.indices_prefix_dataset","text":"If set to true the <DATASET.name>- is automatically prefixed to each pattern. This is a convenience setting as per default all dataset indices start with this prefix.","title":"indices_prefix_dataset"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.name","text":"The processors name","title":"name"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.start","text":"The start time to trim the logs to (defaults to dataset start)","title":"start"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.type_field","text":"The processor type as passed in from the config","title":"type_field"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.execute","text":"Execute the processor and trim the log files. Parameters: Name Type Description Default dataset_dir Path The dataset path required dataset_config DatasetConfig The dataset configuration required parser_config LogstashParserConfig The dataset parser configuration required es Elasticsearch The elasticsearch client object required Source code in dataset/processors.py def execute ( self , dataset_dir : Path , dataset_config : DatasetConfig , parser_config : LogstashParserConfig , es : Elasticsearch , ) -> None : \"\"\"Execute the processor and trim the log files. Args: dataset_dir: The dataset path dataset_config: The dataset configuration parser_config: The dataset parser configuration es: The elasticsearch client object \"\"\" if self . indices is None : # if not explicitly set use dataset root indices pattern indices = [ f \" { dataset_config . name } -*\" ] else : indices = ( # if given and prefix flag is True add dataset name prefix [ f \" { dataset_config . name } - { ind } \" for ind in self . indices ] if self . indices_prefix_dataset else # otherwise use as is indices ) exclude = ( # add negative match indicator '-' and dataset name prefix [ f \"- { dataset_config . name } - { exc } \" for exc in self . exclude ] if self . indices_prefix_dataset # add negative match indicator only else [ f \"- { exc } \" for exc in self . exclude ] ) start = self . start or dataset_config . start end = self . end or dataset_config . end # exclude must be after indices for negative patterns to work as expected index = indices + exclude # get documents before trim docs_before = { bucket . key . path : bucket . doc_count for bucket in self . get_line_stats ( es , index ) } remove = Search ( using = es , index = index ) # setup trim range filter # start >= @timestamp < end valid_range = Range ( ** { \"@timestamp\" : { \"gte\" : start , \"lt\" : end }}) # remove all elements outside of range i.e., `not` valid_range remove = remove . filter ( ~ valid_range ) # refresh=\"true\" is important to ensure consecutive queries # use up to date information print ( remove . params ( refresh = \"true\" , request_cache = False ) . delete () . to_dict ()) # lines after trim lines_after = self . get_line_stats ( es , index ) # trim each file for bucket in lines_after : first_line = int ( bucket . min_line . value ) last_line : Optional [ int ] = int ( bucket . max_line . value ) if docs_before [ bucket . key . path ] - ( first_line - 1 ) == last_line : # if our last line is already correct we set it to None and skip truncate # since truncating requires us to read up to the truncate point last_line = None trim_file ( bucket . key . path , first_line , last_line ) # delete entry for this file so we later can detect if a file must be deleted completely del docs_before [ bucket . key . path ] # any file that still has a docs before entry # does not have any logs within the trim range and thus should be deleted for path in docs_before . keys (): print ( f \"Removing { path } as it does not have any log lines within the observation time.\" ) # delete the file Path ( path ) . unlink () # update entries in elastic search update_lines = UpdateByQuery ( using = es , index = index ) # adjust map for shifting line numbers in the db to start at our new min line adjust_map = { bucket . key . path : int ( bucket . min_line . value - 1 ) for bucket in lines_after # only include paths that need actual changing if int ( bucket . min_line . value - 1 ) > 0 } # we only have entries to update if the adjust map is non empty if len ( adjust_map ) > 0 : # pre filter our update query to only include file paths we # want to update update_lines = update_lines . filter ( \"terms\" , log__file__path = list ( adjust_map . keys ()), ) # ToDO might be better as async query due to threat of timeouts # (i.e., update_lines.to_dict() and then use low level async API) update_lines . script ( lang = \"painless\" , # subtract matching the entries log file path source = \"ctx._source.log.file.line -= params[ctx._source.log.file.path]\" , params = adjust_map , ) . execute ()","title":"execute()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.get_doc_stats","text":"Get a list of unique log file paths. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index List[str] The indices to get the data for required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of log file paths. Source code in dataset/processors.py def get_doc_stats ( self , es : Elasticsearch , index : List [ str ]) -> List [ Bucket ]: \"\"\"Get a list of unique log file paths. Args: es: The elasticsearch client object index: The indices to get the data for Returns: List of log file paths. \"\"\" # disable request cache to ensure we always get latest info search_lines = Search ( using = es , index = index ) . params ( request_cache = False ) # setup aggregations search_lines . aggs . bucket ( \"files\" , \"composite\" , sources = [{ \"path\" : { \"terms\" : { \"field\" : \"log.file.path\" }}}], ) # use custom scan function to ensure we get all the buckets return scan_composite ( search_lines , \"files\" )","title":"get_doc_stats()"},{"location":"reference/processors/#cr_kyoushi.dataset.processors.TrimProcessor.get_line_stats","text":"Retrieve minimum and maximum line numbers for the log files. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required index List[str] The indices to get the stats for required Returns: Type Description List[elasticsearch_dsl.response.aggs.Bucket] List of min and max line numbers per file Source code in dataset/processors.py def get_line_stats ( self , es : Elasticsearch , index : List [ str ]) -> List [ Bucket ]: \"\"\"Retrieve minimum and maximum line numbers for the log files. Args: es: The elasticsearch client object index: The indices to get the stats for Returns: List of min and max line numbers per file \"\"\" # disable request cache to ensure we always get latest info search_lines = Search ( using = es , index = index ) . params ( request_cache = False ) # setup aggregations search_lines . aggs . bucket ( \"files\" , \"composite\" , sources = [{ \"path\" : { \"terms\" : { \"field\" : \"log.file.path\" }}}], ) search_lines . aggs [ \"files\" ] . metric ( \"min_line\" , \"min\" , field = \"log.file.line\" ) search_lines . aggs [ \"files\" ] . metric ( \"max_line\" , \"max\" , field = \"log.file.line\" ) # use custom scan function to ensure we get all the buckets return scan_composite ( search_lines , \"files\" )","title":"get_line_stats()"},{"location":"reference/sample/","text":"Sample module \u00b6 This module contains utility functions used for sampling processed datasets. get_sample ( es , label_filter_script_id , labels , files = None , index = None , label_object = 'kyoushi_labels' , size = 10 , seed = None , seed_field = '_seq_no' , start = None , stop = None ) \u00b6 Retrieve a list of sample log lines. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required label_filter_script_id str The kyoushi filter scripts ID required labels Optional[List[str]] The labels to sample from required files Optional[List[str]] The log files to sample from None index Union[List[str], str] The elasticsearch indices to sample from None label_object str The field that contains the labeling data 'kyoushi_labels' size int The number of lines to sample 10 seed Optional[int] The seed to use for the sample randomization None seed_field str The elasticsearch field to use for the random sample order '_seq_no' start Union[str, datetime.datetime, float] The minimum time stamp to sample from None stop Union[str, datetime.datetime, float] The maximum time stamp to sample from None Returns: Type Description List[elasticsearch_dsl.response.hit.Hit] List of randomly sample log lines. Each line being represented as a dict of the following format: - @timestamp: The log event timestamp log: The elasticsearch log field (containing line number, original log line, etc.) <label_object>.list: List of labels <label_object>.rules: Map of labeling rules applied to the line type: Log type Source code in dataset/sample.py def get_sample ( es : Elasticsearch , label_filter_script_id : str , labels : Optional [ List [ str ]], files : Optional [ List [ str ]] = None , index : Union [ List [ str ], str , None ] = None , label_object : str = \"kyoushi_labels\" , size : int = 10 , seed : Optional [ int ] = None , seed_field : str = \"_seq_no\" , start : Union [ str , datetime , float , None ] = None , stop : Union [ str , datetime , float , None ] = None , ) -> List [ Hit ]: \"\"\"Retrieve a list of sample log lines. Args: es: The elasticsearch client object label_filter_script_id: The kyoushi filter scripts ID labels: The labels to sample from files: The log files to sample from index: The elasticsearch indices to sample from label_object: The field that contains the labeling data size: The number of lines to sample seed: The seed to use for the sample randomization seed_field: The elasticsearch field to use for the random sample order start: The minimum time stamp to sample from stop: The maximum time stamp to sample from Returns: List of randomly sample log lines. Each line being represented as a dict of the following format: ``` - @timestamp: The log event timestamp log: The elasticsearch log field (containing line number, original log line, etc.) <label_object>.list: List of labels <label_object>.rules: Map of labeling rules applied to the line type: Log type ``` \"\"\" search = Search ( using = es , index = index ) # use random score to get a random sampling random_score = { \"seed\" : seed , \"field\" : seed_field } if seed is not None else {} search = search . query ( \"function_score\" , random_score = random_score ) search = search . sort ( \"_score\" ) . extra ( size = size ) if labels is None or len ( labels ) == 0 : # if we are given no labels to search for we explicitly return # only log rows without any labels search = search . exclude ( \"exists\" , field = f \" { label_object } .rules\" ) else : # if we got a label then we filter for it using our script search filter search = search . filter ( \"script\" , script = { \"id\" : label_filter_script_id , \"params\" : { \"labels\" : labels }}, ) time_range = {} if start is not None : time_range [ \"gte\" ] = start if stop is not None : time_range [ \"lte\" ] = stop if len ( time_range ) > 0 : search = search . filter ( Range ( ** { \"@timestamp\" : time_range })) if files is not None and len ( files ) > 0 : search = search . filter ( \"bool\" , should = [{ \"match\" : { \"log.file.path\" : f }} for f in files ] ) search = search . source ( [ \"@timestamp\" , \"log\" , f \" { label_object } .list\" , f \" { label_object } .rules\" , \"type\" , \"_score\" , \"_seq_no\" , ] ) return search . execute () . hits get_sample_log ( es , sample , label , gather_dir , before = 5 , after = 5 , related = None , index = None ) \u00b6 Retrieves additional information for a sampled log entry. This function can be used to retrieve additional information such as, lines before or after. The information can be helpful when analyzing sampled log lines. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required sample Hit The sample log line required label str The label that the sample is fore required gather_dir Path The dataset gather directory required before int The number of lines before the sample to fetch 5 after int The number of line after the sample to fetch 5 related Optional[List[str]] List of related elasticsearch indices to retrieve neighbor logs from None index Union[List[str], str] The index the sample was retrieved from None Returns: Type Description Dict[str, Any] Dictionary containing verbose information about the sample log. Format: label: <The label the sample is for> rules: <List of labeling rules applied to the sample log line> path: <The samples log files relative path> line_no: <The samples line number> before: <List of log lines before the sample> line: <The sample log line> after: <List of log lines after the sample> related: <List of log lines in related files with timestamps close to the sample.> Source code in dataset/sample.py def get_sample_log ( es : Elasticsearch , sample : Hit , label : str , gather_dir : Path , before : int = 5 , after : int = 5 , related : Optional [ List [ str ]] = None , index : Union [ List [ str ], str , None ] = None , ) -> Dict [ str , Any ]: \"\"\"Retrieves additional information for a sampled log entry. This function can be used to retrieve additional information such as, lines before or after. The information can be helpful when analyzing sampled log lines. Args: es: The elasticsearch client object sample: The sample log line label: The label that the sample is fore gather_dir: The dataset gather directory before: The number of lines before the sample to fetch after: The number of line after the sample to fetch related: List of related elasticsearch indices to retrieve neighbor logs from index: The index the sample was retrieved from Returns: Dictionary containing verbose information about the sample log. Format: ``` label: <The label the sample is for> rules: <List of labeling rules applied to the sample log line> path: <The samples log files relative path> line_no: <The samples line number> before: <List of log lines before the sample> line: <The sample log line> after: <List of log lines after the sample> related: <List of log lines in related files with timestamps close to the sample.> ``` \"\"\" path = Path ( sample . log . file . path ) line_no = sample . log . file . line start = max ( 0 , line_no - before ) end = line_no + after before_lines : List [ str ] = [] sample_line : str after_lines : List [ str ] = [] # read the sample line and the requested surrounding lines with open ( path , \"r\" ) as f : for i , line in enumerate ( f , 1 ): if i >= start and i < line_no : before_lines . append ( line ) elif i == line_no : sample_line = line elif i > line_no and i <= end : after_lines . append ( line ) elif i > end : break _related : List [ Dict [ str , Any ]] = [] if related is not None : for rel in related : closest : Optional [ Hit ] = _get_closest ( es = es , related = rel , timestamp = sample [ \"@timestamp\" ] ) if closest is not None and closest . log . file . path != str ( path ): if closest is not None : _related . append ( { \"path\" : str ( Path ( closest . log . file . path ) . relative_to ( gather_dir ) ), \"line_no\" : closest . log . file . line , \"timestamp\" : closest [ \"@timestamp\" ], } ) return { \"label\" : label , \"rules\" : list ( sample . kyoushi_labels . rules ) if \"kyoushi_labels\" in sample else [], \"path\" : str ( path . relative_to ( gather_dir )), \"line_no\" : line_no , \"before\" : before_lines , \"line\" : sample_line , \"after\" : after_lines , \"related\" : _related , }","title":"Sampling"},{"location":"reference/sample/#sample-module","text":"This module contains utility functions used for sampling processed datasets.","title":"Sample module"},{"location":"reference/sample/#cr_kyoushi.dataset.sample.get_sample","text":"Retrieve a list of sample log lines. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required label_filter_script_id str The kyoushi filter scripts ID required labels Optional[List[str]] The labels to sample from required files Optional[List[str]] The log files to sample from None index Union[List[str], str] The elasticsearch indices to sample from None label_object str The field that contains the labeling data 'kyoushi_labels' size int The number of lines to sample 10 seed Optional[int] The seed to use for the sample randomization None seed_field str The elasticsearch field to use for the random sample order '_seq_no' start Union[str, datetime.datetime, float] The minimum time stamp to sample from None stop Union[str, datetime.datetime, float] The maximum time stamp to sample from None Returns: Type Description List[elasticsearch_dsl.response.hit.Hit] List of randomly sample log lines. Each line being represented as a dict of the following format: - @timestamp: The log event timestamp log: The elasticsearch log field (containing line number, original log line, etc.) <label_object>.list: List of labels <label_object>.rules: Map of labeling rules applied to the line type: Log type Source code in dataset/sample.py def get_sample ( es : Elasticsearch , label_filter_script_id : str , labels : Optional [ List [ str ]], files : Optional [ List [ str ]] = None , index : Union [ List [ str ], str , None ] = None , label_object : str = \"kyoushi_labels\" , size : int = 10 , seed : Optional [ int ] = None , seed_field : str = \"_seq_no\" , start : Union [ str , datetime , float , None ] = None , stop : Union [ str , datetime , float , None ] = None , ) -> List [ Hit ]: \"\"\"Retrieve a list of sample log lines. Args: es: The elasticsearch client object label_filter_script_id: The kyoushi filter scripts ID labels: The labels to sample from files: The log files to sample from index: The elasticsearch indices to sample from label_object: The field that contains the labeling data size: The number of lines to sample seed: The seed to use for the sample randomization seed_field: The elasticsearch field to use for the random sample order start: The minimum time stamp to sample from stop: The maximum time stamp to sample from Returns: List of randomly sample log lines. Each line being represented as a dict of the following format: ``` - @timestamp: The log event timestamp log: The elasticsearch log field (containing line number, original log line, etc.) <label_object>.list: List of labels <label_object>.rules: Map of labeling rules applied to the line type: Log type ``` \"\"\" search = Search ( using = es , index = index ) # use random score to get a random sampling random_score = { \"seed\" : seed , \"field\" : seed_field } if seed is not None else {} search = search . query ( \"function_score\" , random_score = random_score ) search = search . sort ( \"_score\" ) . extra ( size = size ) if labels is None or len ( labels ) == 0 : # if we are given no labels to search for we explicitly return # only log rows without any labels search = search . exclude ( \"exists\" , field = f \" { label_object } .rules\" ) else : # if we got a label then we filter for it using our script search filter search = search . filter ( \"script\" , script = { \"id\" : label_filter_script_id , \"params\" : { \"labels\" : labels }}, ) time_range = {} if start is not None : time_range [ \"gte\" ] = start if stop is not None : time_range [ \"lte\" ] = stop if len ( time_range ) > 0 : search = search . filter ( Range ( ** { \"@timestamp\" : time_range })) if files is not None and len ( files ) > 0 : search = search . filter ( \"bool\" , should = [{ \"match\" : { \"log.file.path\" : f }} for f in files ] ) search = search . source ( [ \"@timestamp\" , \"log\" , f \" { label_object } .list\" , f \" { label_object } .rules\" , \"type\" , \"_score\" , \"_seq_no\" , ] ) return search . execute () . hits","title":"get_sample()"},{"location":"reference/sample/#cr_kyoushi.dataset.sample.get_sample_log","text":"Retrieves additional information for a sampled log entry. This function can be used to retrieve additional information such as, lines before or after. The information can be helpful when analyzing sampled log lines. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required sample Hit The sample log line required label str The label that the sample is fore required gather_dir Path The dataset gather directory required before int The number of lines before the sample to fetch 5 after int The number of line after the sample to fetch 5 related Optional[List[str]] List of related elasticsearch indices to retrieve neighbor logs from None index Union[List[str], str] The index the sample was retrieved from None Returns: Type Description Dict[str, Any] Dictionary containing verbose information about the sample log. Format: label: <The label the sample is for> rules: <List of labeling rules applied to the sample log line> path: <The samples log files relative path> line_no: <The samples line number> before: <List of log lines before the sample> line: <The sample log line> after: <List of log lines after the sample> related: <List of log lines in related files with timestamps close to the sample.> Source code in dataset/sample.py def get_sample_log ( es : Elasticsearch , sample : Hit , label : str , gather_dir : Path , before : int = 5 , after : int = 5 , related : Optional [ List [ str ]] = None , index : Union [ List [ str ], str , None ] = None , ) -> Dict [ str , Any ]: \"\"\"Retrieves additional information for a sampled log entry. This function can be used to retrieve additional information such as, lines before or after. The information can be helpful when analyzing sampled log lines. Args: es: The elasticsearch client object sample: The sample log line label: The label that the sample is fore gather_dir: The dataset gather directory before: The number of lines before the sample to fetch after: The number of line after the sample to fetch related: List of related elasticsearch indices to retrieve neighbor logs from index: The index the sample was retrieved from Returns: Dictionary containing verbose information about the sample log. Format: ``` label: <The label the sample is for> rules: <List of labeling rules applied to the sample log line> path: <The samples log files relative path> line_no: <The samples line number> before: <List of log lines before the sample> line: <The sample log line> after: <List of log lines after the sample> related: <List of log lines in related files with timestamps close to the sample.> ``` \"\"\" path = Path ( sample . log . file . path ) line_no = sample . log . file . line start = max ( 0 , line_no - before ) end = line_no + after before_lines : List [ str ] = [] sample_line : str after_lines : List [ str ] = [] # read the sample line and the requested surrounding lines with open ( path , \"r\" ) as f : for i , line in enumerate ( f , 1 ): if i >= start and i < line_no : before_lines . append ( line ) elif i == line_no : sample_line = line elif i > line_no and i <= end : after_lines . append ( line ) elif i > end : break _related : List [ Dict [ str , Any ]] = [] if related is not None : for rel in related : closest : Optional [ Hit ] = _get_closest ( es = es , related = rel , timestamp = sample [ \"@timestamp\" ] ) if closest is not None and closest . log . file . path != str ( path ): if closest is not None : _related . append ( { \"path\" : str ( Path ( closest . log . file . path ) . relative_to ( gather_dir ) ), \"line_no\" : closest . log . file . line , \"timestamp\" : closest [ \"@timestamp\" ], } ) return { \"label\" : label , \"rules\" : list ( sample . kyoushi_labels . rules ) if \"kyoushi_labels\" in sample else [], \"path\" : str ( path . relative_to ( gather_dir )), \"line_no\" : line_no , \"before\" : before_lines , \"line\" : sample_line , \"after\" : after_lines , \"related\" : _related , }","title":"get_sample_log()"},{"location":"reference/templates/","text":"Templates module \u00b6 This module contains utility functions used with and supporting template rendering as part of the processing pipeline and labeling rules. as_datetime ( v ) \u00b6 Utility filter for converting a string to datetime. Parameters: Name Type Description Default v str The string to convert required Returns: Type Description datetime Converted datetime object. Source code in dataset/templates.py def as_datetime ( v : str ) -> datetime : \"\"\"Utility filter for converting a string to datetime. Args: v: The string to convert Returns: Converted datetime object. \"\"\" return parse_obj_as ( datetime , v ) create_environment ( templates_dirs = None , es = None , dataset_config = None ) \u00b6 Create Jinja2 native environment for rendering dataset templates. Parameters: Name Type Description Default templates_dirs Union[str, pathlib.Path, List[Union[str, pathlib.Path]]] The template directories None es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description NativeEnvironment Jinja2 template environment Source code in dataset/templates.py def create_environment ( templates_dirs : Optional [ Union [ Text , Path , List [ Union [ Text , Path ]]]] = None , es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> NativeEnvironment : \"\"\"Create Jinja2 native environment for rendering dataset templates. Args: templates_dirs: The template directories es: The elasticsearch client object dataset_config: The dataset configuration Returns: Jinja2 template environment \"\"\" if templates_dirs is None : templates_dirs = [ Path ( \"./templates\" ), Path ( \"./\" ), ] env_loader = ChoiceLoader ( [ FileSystemLoader ( templates_dirs ), PackageLoader ( \"cr_kyoushi.dataset\" , \"templates\" ), ] ) env = NativeEnvironment ( loader = env_loader , undefined = StrictUndefined , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) custom_tests = { \"match_any\" : match_any , \"regex\" : regex , \"regex_search\" : regex_search , \"regex_match\" : regex_match , } custom_filters = { \"as_datetime\" : as_datetime , } custom_globals = { \"context\" : get_context , \"datetime\" : datetime , \"timedelta\" : timedelta , } if es is not None : if dataset_config is not None : search_function = functools . partial ( elastic_dsl_search , using = es , dataset_name = dataset_config . name ) eql_function = functools . partial ( elastic_eql_search , es = es , dataset_name = dataset_config . name ) else : search_function = functools . partial ( elastic_dsl_search , using = es ) eql_function = functools . partial ( elastic_eql_search , es = es ) custom_globals [ \"Search\" ] = search_function custom_globals [ \"Q\" ] = Q custom_globals [ \"Q_ALL\" ] = q_all custom_globals [ \"Q_MATCH_ALL\" ] = functools . partial ( q_all , \"match\" ) custom_globals [ \"Q_TERM_ALL\" ] = functools . partial ( q_all , \"term\" ) custom_globals [ \"EQL\" ] = eql_function env . tests . update ( custom_tests ) env . filters . update ( custom_filters ) env . globals . update ( custom_globals ) return env elastic_dsl_search ( using , dataset_name = None , prefix_dataset_name = True , index = None , ** kwargs ) \u00b6 Create an Elasticsearch DSL search object. Parameters: Name Type Description Default using Elasticsearch The elasticsearch client object required dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed to the indices or not True index Union[Sequence[str], str] The indices to create the search object for None Returns: Type Description Search Configured elasticsearch DSL search object Source code in dataset/templates.py def elastic_dsl_search ( using : Elasticsearch , dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ** kwargs , ) -> Search : \"\"\"Create an Elasticsearch DSL search object. Args: using: The elasticsearch client object dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed to the indices or not index: The indices to create the search object for Returns: Configured elasticsearch DSL search object \"\"\" _index = resolve_indices ( dataset_name , prefix_dataset_name , index ) return Search ( using = using , index = _index , ** kwargs ) elastic_eql_search ( es , body , dataset_name = None , prefix_dataset_name = True , index = None ) \u00b6 Perform an Elasticsearch EQL query. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required body Dict[str, Any] The EQL query body required dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed to the indices or not True index Union[Sequence[str], str] The indices to perform the query on. None Returns: Type Description Dict[str, Any] The EQL query result Source code in dataset/templates.py def elastic_eql_search ( es : Elasticsearch , body : Dict [ str , Any ], dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ) -> Dict [ str , Any ]: \"\"\"Perform an Elasticsearch EQL query. Args: es: The elasticsearch client object body: The EQL query body dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed to the indices or not index: The indices to perform the query on. Returns: The EQL query result \"\"\" _index = resolve_indices ( dataset_name , prefix_dataset_name , index ) eql = EqlClient ( es ) return eql . search ( index = _index , body = body ) get_context ( c ) \u00b6 Utility function for getting the Jinja2 context. Parameters: Name Type Description Default c Context The Jinja2 context required Returns: Type Description Context The Jinja2 context Source code in dataset/templates.py @contextfunction def get_context ( c : Context ) -> Context : \"\"\"Utility function for getting the Jinja2 context. Args: c: The Jinja2 context Returns: The Jinja2 context \"\"\" return c match_any ( value , regex_list ) \u00b6 Perform multiple re.match and return True if at least on match is found. Parameters: Name Type Description Default value str The string to search in required regex_list List[str] Lis tof patterns to try matching required Returns: Type Description bool True if at least one pattern matches False otherwise Source code in dataset/templates.py def match_any ( value : str , regex_list : List [ str ]) -> bool : \"\"\"Perform multiple `re.match` and return `True` if at least on match is found. Args: value: The string to search in regex_list: Lis tof patterns to try matching Returns: `True` if at least one pattern matches `False` otherwise \"\"\" return any ( re . match ( regex , value ) for regex in regex_list ) q_all ( qry_type , ** kwargs ) \u00b6 Create elasticsearch DSL bool term requiring all given terms to be true. Parameters: Name Type Description Default qry_type str The DSL query term type required Returns: Type Description <function Q at 0x7fe2bf285d40> The configured DSL query term Source code in dataset/templates.py def q_all ( qry_type : str , ** kwargs ) -> Q : \"\"\"Create elasticsearch DSL bool term requiring all given terms to be true. Args: qry_type: The DSL query term type Returns: The configured DSL query term \"\"\" must = [] for key , val in kwargs . items (): if isinstance ( val , Query ): must . append ( val ) else : must . append ( Q ( qry_type , ** { key : val })) return Q ( \"bool\" , must = must ) regex ( value = '' , pattern = '' , ignorecase = False , multiline = False , match_type = 'search' ) \u00b6 Expose re as a boolean filter using the search method by default. This is likely only useful for search and match which already have their own filters. Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in '' pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False match_type str The re pattern match type to use 'search' Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex ( value : str = \"\" , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False , match_type : str = \"search\" , ) -> bool : \"\"\"Expose `re` as a boolean filter using the `search` method by default. This is likely only useful for `search` and `match` which already have their own filters. !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not match_type: The re pattern match type to use Returns: `True` if a match was found `False` otherwise. \"\"\" flags = 0 if ignorecase : flags |= re . I if multiline : flags |= re . M _re = re . compile ( pattern , flags = flags ) return bool ( getattr ( _re , match_type , \"search\" )( value )) regex_match ( value , pattern = '' , ignorecase = False , multiline = False ) \u00b6 Perform a re.match returning a boolean Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in required pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex_match ( value : str , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False ) -> bool : \"\"\"Perform a `re.match` returning a boolean !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not Returns: `True` if a match was found `False` otherwise. \"\"\" return regex ( value , pattern , ignorecase , multiline , \"match\" ) regex_search ( value , pattern = '' , ignorecase = False , multiline = False ) \u00b6 Perform a re.search returning a boolean Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in required pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex_search ( value : str , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False ) -> bool : \"\"\"Perform a `re.search` returning a boolean !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not Returns: `True` if a match was found `False` otherwise. \"\"\" return regex ( value , pattern , ignorecase , multiline , \"search\" ) render_template ( template , variables , es = None , dataset_config = None ) \u00b6 Renders a dataset Jinja2 template string or file. Parameters: Name Type Description Default template Union[str, pathlib.Path] The template string or file required variables Dict[str, Any] The context variables to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description Any The rendered Jinja2 template Source code in dataset/templates.py def render_template ( template : Union [ Text , Path ], variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> Any : \"\"\"Renders a dataset Jinja2 template string or file. Args: template: The template string or file variables: The context variables to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration Returns: The rendered Jinja2 template \"\"\" # get jinja2 environment env = create_environment ( es = es , dataset_config = dataset_config ) # convert strings to template if isinstance ( template , Path ): _template = env . get_template ( str ( template )) else : _template = env . from_string ( template ) value = _template . render ( ** variables ) if isinstance ( value , Undefined ): value . _fail_with_undefined_error () return value render_template_recursive ( data , variables , es = None , dataset_config = None ) \u00b6 Renders a complex object containing Jinja2 templates The complex object can be either a string, list or dictionary. This function will recurse all sub elements (e.g., dictionary values) and render any Jinja2 template strings it finds. Parameters: Name Type Description Default data Any The object to render required variables Dict[str, Any] The context variables to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description Any The object with all its Jinja2 templates rendered. Source code in dataset/templates.py def render_template_recursive ( data : Any , variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> Any : \"\"\"Renders a complex object containing Jinja2 templates The complex object can be either a string, list or dictionary. This function will recurse all sub elements (e.g., dictionary values) and render any Jinja2 template strings it finds. Args: data: The object to render variables: The context variables to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration Returns: The object with all its Jinja2 templates rendered. \"\"\" # handle sub dicts if isinstance ( data , dict ): data_rendered = {} for key , val in data . items (): # for sub dicts keys we also allow temp key = render_template_recursive ( key , variables , es , dataset_config ) val = render_template_recursive ( val , variables , es , dataset_config ) data_rendered [ key ] = val return data_rendered # handle list elements if isinstance ( data , list ): return [ render_template_recursive ( val , variables , es , dataset_config ) for val in data ] # handle str and template strings if isinstance ( data , str ): return render_template ( data , variables , es , dataset_config ) # all other basic types are returned as is return data write_template ( src , dest , variables , es = None , dataset_config = None ) \u00b6 Render and write a dataset Jinja2 template file. Parameters: Name Type Description Default src Path The template source required dest Path The file to write the rendered string to required variables Dict[str, Any] The variable context to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Source code in dataset/templates.py def write_template ( src : Path , dest : Path , variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ): \"\"\"Render and write a dataset Jinja2 template file. Args: src: The template source dest: The file to write the rendered string to variables: The variable context to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration \"\"\" template_rendered = render_template ( src , variables , es , dataset_config ) if ( # mappings are converted to json or yaml isinstance ( template_rendered , Mapping ) # lists are also converted to json or ( # need to exclude str types as they are also sequences not isinstance ( template_rendered , Text ) and isinstance ( template_rendered , Sequence ) ) ): write_config_file ( template_rendered , dest ) # everything else is coerced to string and written as is else : with open ( dest , \"w\" ) as dest_file : dest_file . write ( str ( template_rendered ))","title":"Templates"},{"location":"reference/templates/#templates-module","text":"This module contains utility functions used with and supporting template rendering as part of the processing pipeline and labeling rules.","title":"Templates module"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.as_datetime","text":"Utility filter for converting a string to datetime. Parameters: Name Type Description Default v str The string to convert required Returns: Type Description datetime Converted datetime object. Source code in dataset/templates.py def as_datetime ( v : str ) -> datetime : \"\"\"Utility filter for converting a string to datetime. Args: v: The string to convert Returns: Converted datetime object. \"\"\" return parse_obj_as ( datetime , v )","title":"as_datetime()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.create_environment","text":"Create Jinja2 native environment for rendering dataset templates. Parameters: Name Type Description Default templates_dirs Union[str, pathlib.Path, List[Union[str, pathlib.Path]]] The template directories None es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description NativeEnvironment Jinja2 template environment Source code in dataset/templates.py def create_environment ( templates_dirs : Optional [ Union [ Text , Path , List [ Union [ Text , Path ]]]] = None , es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> NativeEnvironment : \"\"\"Create Jinja2 native environment for rendering dataset templates. Args: templates_dirs: The template directories es: The elasticsearch client object dataset_config: The dataset configuration Returns: Jinja2 template environment \"\"\" if templates_dirs is None : templates_dirs = [ Path ( \"./templates\" ), Path ( \"./\" ), ] env_loader = ChoiceLoader ( [ FileSystemLoader ( templates_dirs ), PackageLoader ( \"cr_kyoushi.dataset\" , \"templates\" ), ] ) env = NativeEnvironment ( loader = env_loader , undefined = StrictUndefined , extensions = [ \"jinja2.ext.do\" , \"jinja2.ext.loopcontrols\" ], ) custom_tests = { \"match_any\" : match_any , \"regex\" : regex , \"regex_search\" : regex_search , \"regex_match\" : regex_match , } custom_filters = { \"as_datetime\" : as_datetime , } custom_globals = { \"context\" : get_context , \"datetime\" : datetime , \"timedelta\" : timedelta , } if es is not None : if dataset_config is not None : search_function = functools . partial ( elastic_dsl_search , using = es , dataset_name = dataset_config . name ) eql_function = functools . partial ( elastic_eql_search , es = es , dataset_name = dataset_config . name ) else : search_function = functools . partial ( elastic_dsl_search , using = es ) eql_function = functools . partial ( elastic_eql_search , es = es ) custom_globals [ \"Search\" ] = search_function custom_globals [ \"Q\" ] = Q custom_globals [ \"Q_ALL\" ] = q_all custom_globals [ \"Q_MATCH_ALL\" ] = functools . partial ( q_all , \"match\" ) custom_globals [ \"Q_TERM_ALL\" ] = functools . partial ( q_all , \"term\" ) custom_globals [ \"EQL\" ] = eql_function env . tests . update ( custom_tests ) env . filters . update ( custom_filters ) env . globals . update ( custom_globals ) return env","title":"create_environment()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.elastic_dsl_search","text":"Create an Elasticsearch DSL search object. Parameters: Name Type Description Default using Elasticsearch The elasticsearch client object required dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed to the indices or not True index Union[Sequence[str], str] The indices to create the search object for None Returns: Type Description Search Configured elasticsearch DSL search object Source code in dataset/templates.py def elastic_dsl_search ( using : Elasticsearch , dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ** kwargs , ) -> Search : \"\"\"Create an Elasticsearch DSL search object. Args: using: The elasticsearch client object dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed to the indices or not index: The indices to create the search object for Returns: Configured elasticsearch DSL search object \"\"\" _index = resolve_indices ( dataset_name , prefix_dataset_name , index ) return Search ( using = using , index = _index , ** kwargs )","title":"elastic_dsl_search()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.elastic_eql_search","text":"Perform an Elasticsearch EQL query. Parameters: Name Type Description Default es Elasticsearch The elasticsearch client object required body Dict[str, Any] The EQL query body required dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed to the indices or not True index Union[Sequence[str], str] The indices to perform the query on. None Returns: Type Description Dict[str, Any] The EQL query result Source code in dataset/templates.py def elastic_eql_search ( es : Elasticsearch , body : Dict [ str , Any ], dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ) -> Dict [ str , Any ]: \"\"\"Perform an Elasticsearch EQL query. Args: es: The elasticsearch client object body: The EQL query body dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed to the indices or not index: The indices to perform the query on. Returns: The EQL query result \"\"\" _index = resolve_indices ( dataset_name , prefix_dataset_name , index ) eql = EqlClient ( es ) return eql . search ( index = _index , body = body )","title":"elastic_eql_search()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.get_context","text":"Utility function for getting the Jinja2 context. Parameters: Name Type Description Default c Context The Jinja2 context required Returns: Type Description Context The Jinja2 context Source code in dataset/templates.py @contextfunction def get_context ( c : Context ) -> Context : \"\"\"Utility function for getting the Jinja2 context. Args: c: The Jinja2 context Returns: The Jinja2 context \"\"\" return c","title":"get_context()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.match_any","text":"Perform multiple re.match and return True if at least on match is found. Parameters: Name Type Description Default value str The string to search in required regex_list List[str] Lis tof patterns to try matching required Returns: Type Description bool True if at least one pattern matches False otherwise Source code in dataset/templates.py def match_any ( value : str , regex_list : List [ str ]) -> bool : \"\"\"Perform multiple `re.match` and return `True` if at least on match is found. Args: value: The string to search in regex_list: Lis tof patterns to try matching Returns: `True` if at least one pattern matches `False` otherwise \"\"\" return any ( re . match ( regex , value ) for regex in regex_list )","title":"match_any()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.q_all","text":"Create elasticsearch DSL bool term requiring all given terms to be true. Parameters: Name Type Description Default qry_type str The DSL query term type required Returns: Type Description <function Q at 0x7fe2bf285d40> The configured DSL query term Source code in dataset/templates.py def q_all ( qry_type : str , ** kwargs ) -> Q : \"\"\"Create elasticsearch DSL bool term requiring all given terms to be true. Args: qry_type: The DSL query term type Returns: The configured DSL query term \"\"\" must = [] for key , val in kwargs . items (): if isinstance ( val , Query ): must . append ( val ) else : must . append ( Q ( qry_type , ** { key : val })) return Q ( \"bool\" , must = must )","title":"q_all()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.regex","text":"Expose re as a boolean filter using the search method by default. This is likely only useful for search and match which already have their own filters. Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in '' pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False match_type str The re pattern match type to use 'search' Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex ( value : str = \"\" , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False , match_type : str = \"search\" , ) -> bool : \"\"\"Expose `re` as a boolean filter using the `search` method by default. This is likely only useful for `search` and `match` which already have their own filters. !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not match_type: The re pattern match type to use Returns: `True` if a match was found `False` otherwise. \"\"\" flags = 0 if ignorecase : flags |= re . I if multiline : flags |= re . M _re = re . compile ( pattern , flags = flags ) return bool ( getattr ( _re , match_type , \"search\" )( value ))","title":"regex()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.regex_match","text":"Perform a re.match returning a boolean Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in required pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex_match ( value : str , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False ) -> bool : \"\"\"Perform a `re.match` returning a boolean !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not Returns: `True` if a match was found `False` otherwise. \"\"\" return regex ( value , pattern , ignorecase , multiline , \"match\" )","title":"regex_match()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.regex_search","text":"Perform a re.search returning a boolean Note Taken from Ansible Parameters: Name Type Description Default value str The string to search in required pattern str The pattern to search '' ignorecase bool If the case should be ignored or not False multiline bool If multiline matching should be used or not False Returns: Type Description bool True if a match was found False otherwise. Source code in dataset/templates.py def regex_search ( value : str , pattern : str = \"\" , ignorecase : bool = False , multiline : bool = False ) -> bool : \"\"\"Perform a `re.search` returning a boolean !!! Note Taken from Ansible Args: value: The string to search in pattern: The pattern to search ignorecase: If the case should be ignored or not multiline: If multiline matching should be used or not Returns: `True` if a match was found `False` otherwise. \"\"\" return regex ( value , pattern , ignorecase , multiline , \"search\" )","title":"regex_search()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.render_template","text":"Renders a dataset Jinja2 template string or file. Parameters: Name Type Description Default template Union[str, pathlib.Path] The template string or file required variables Dict[str, Any] The context variables to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description Any The rendered Jinja2 template Source code in dataset/templates.py def render_template ( template : Union [ Text , Path ], variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> Any : \"\"\"Renders a dataset Jinja2 template string or file. Args: template: The template string or file variables: The context variables to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration Returns: The rendered Jinja2 template \"\"\" # get jinja2 environment env = create_environment ( es = es , dataset_config = dataset_config ) # convert strings to template if isinstance ( template , Path ): _template = env . get_template ( str ( template )) else : _template = env . from_string ( template ) value = _template . render ( ** variables ) if isinstance ( value , Undefined ): value . _fail_with_undefined_error () return value","title":"render_template()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.render_template_recursive","text":"Renders a complex object containing Jinja2 templates The complex object can be either a string, list or dictionary. This function will recurse all sub elements (e.g., dictionary values) and render any Jinja2 template strings it finds. Parameters: Name Type Description Default data Any The object to render required variables Dict[str, Any] The context variables to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Returns: Type Description Any The object with all its Jinja2 templates rendered. Source code in dataset/templates.py def render_template_recursive ( data : Any , variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ) -> Any : \"\"\"Renders a complex object containing Jinja2 templates The complex object can be either a string, list or dictionary. This function will recurse all sub elements (e.g., dictionary values) and render any Jinja2 template strings it finds. Args: data: The object to render variables: The context variables to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration Returns: The object with all its Jinja2 templates rendered. \"\"\" # handle sub dicts if isinstance ( data , dict ): data_rendered = {} for key , val in data . items (): # for sub dicts keys we also allow temp key = render_template_recursive ( key , variables , es , dataset_config ) val = render_template_recursive ( val , variables , es , dataset_config ) data_rendered [ key ] = val return data_rendered # handle list elements if isinstance ( data , list ): return [ render_template_recursive ( val , variables , es , dataset_config ) for val in data ] # handle str and template strings if isinstance ( data , str ): return render_template ( data , variables , es , dataset_config ) # all other basic types are returned as is return data","title":"render_template_recursive()"},{"location":"reference/templates/#cr_kyoushi.dataset.templates.write_template","text":"Render and write a dataset Jinja2 template file. Parameters: Name Type Description Default src Path The template source required dest Path The file to write the rendered string to required variables Dict[str, Any] The variable context to use for rendering required es Optional[elasticsearch.client.Elasticsearch] The elasticsearch client object None dataset_config Optional[cr_kyoushi.dataset.config.DatasetConfig] The dataset configuration None Source code in dataset/templates.py def write_template ( src : Path , dest : Path , variables : Dict [ str , Any ], es : Optional [ Elasticsearch ] = None , dataset_config : Optional [ DatasetConfig ] = None , ): \"\"\"Render and write a dataset Jinja2 template file. Args: src: The template source dest: The file to write the rendered string to variables: The variable context to use for rendering es: The elasticsearch client object dataset_config: The dataset configuration \"\"\" template_rendered = render_template ( src , variables , es , dataset_config ) if ( # mappings are converted to json or yaml isinstance ( template_rendered , Mapping ) # lists are also converted to json or ( # need to exclude str types as they are also sequences not isinstance ( template_rendered , Text ) and isinstance ( template_rendered , Sequence ) ) ): write_config_file ( template_rendered , dest ) # everything else is coerced to string and written as is else : with open ( dest , \"w\" ) as dest_file : dest_file . write ( str ( template_rendered ))","title":"write_template()"},{"location":"reference/utils/","text":"Utils module \u00b6 This module contains general purpose utility functions. copy_package_file ( package , file , dest , overwrite = False ) \u00b6 Copies a package distributed file to some path. Parameters: Name Type Description Default package str The package of the file to copy required file str The file to copy required dest Path The path to copy the file to required overwrite bool If the destination path should be overwritten or not False Source code in dataset/utils.py def copy_package_file ( package : str , file : str , dest : Path , overwrite : bool = False ): \"\"\"Copies a package distributed file to some path. Args: package: The package of the file to copy file: The file to copy dest: The path to copy the file to overwrite: If the destination path should be overwritten or not \"\"\" if overwrite or not dest . exists (): with pkg_resources . path ( package , file ) as pkg_file : shutil . copy ( pkg_file , dest . absolute ()) create_dirs ( directories , always = False ) \u00b6 Creates the given list of directories. Parameters: Name Type Description Default directories List[pathlib.Path] The directories to create required always bool If the directories can already exist or not False Source code in dataset/utils.py def create_dirs ( directories : List [ Path ], always : bool = False ): \"\"\"Creates the given list of directories. Args: directories: The directories to create always: If the directories can already exist or not \"\"\" for d in directories : if always or not d . exists (): os . makedirs ( d , exist_ok = always ) load_file ( file ) \u00b6 Load data from a file (either JSON or YAML) The function will check the file extensions - .json - .yaml - .yml and will try to load using the respective parses. Any other file extension and file format will produce an error. Parameters: Name Type Description Default file Union[str, pathlib.Path] The file stream or path to load required Exceptions: Type Description NotImplementedError If the file format is not supported. Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_file ( file : Union [ Text , Path ]) -> Any : \"\"\"Load data from a file (either JSON or YAML) The function will check the file extensions - .json - .yaml - .yml and will try to load using the respective parses. Any other file extension and file format will produce an error. Args: file: The file stream or path to load Raises: NotImplementedError: If the file format is not supported. Returns: The loaded data \"\"\" if isinstance ( file , Text ): file = Path ( file ) ext = file . suffix if ext == \".json\" : return load_json_file ( file ) elif ext in ( \".yaml\" , \".yml\" ): return load_yaml_file ( file ) raise NotImplementedError ( f \"No file loader supported for { ext } files\" ) load_json_file ( file ) \u00b6 Parse and load a JSON file Parameters: Name Type Description Default file Union[BinaryIO, IO[str], _io.StringIO, str, pathlib.Path] The file stream or path to load required Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_json_file ( file : Union [ StreamTextType , Path ]) -> Any : \"\"\"Parse and load a JSON file Args: file: The file stream or path to load Returns: The loaded data \"\"\" if isinstance ( file , ( Text , Path )): with open ( file , \"r\" ) as f : return json . load ( f ) return json . load ( file ) load_variables ( sources ) \u00b6 Loads variables from variable files. Parameters: Name Type Description Default sources Union[pathlib.Path, Dict[str, pathlib.Path]] The variable file/s to load required Returns: Type Description Any The loaded variables Source code in dataset/utils.py def load_variables ( sources : Union [ Path , Dict [ str , Union [ Path ]]]) -> Any : \"\"\"Loads variables from variable files. Args: sources: The variable file/s to load Returns: The loaded variables \"\"\" if isinstance ( sources , dict ): variables = {} for key , path in sources . items (): variables [ key ] = load_file ( path ) return variables return load_file ( sources ) load_yaml_file ( file ) \u00b6 Parse and load a YAML file. Parameters: Name Type Description Default file Union[BinaryIO, IO[str], _io.StringIO, str, pathlib.Path] The file stream or path to load required Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_yaml_file ( file : Union [ StreamTextType , Path ]) -> Any : \"\"\"Parse and load a YAML file. Args: file: The file stream or path to load Returns: The loaded data \"\"\" yaml = YAML ( typ = \"safe\" ) return yaml . load ( file ) remove_first_lines ( path , n , inclusive = False ) \u00b6 Removes the first lines up until n In inclusive mode the nth line is also deleted otherwise only the lines before the nth line are delted. Parameters: Name Type Description Default path Union[str, pathlib.Path] The path to the file required n int The \"nth\" line required inclusive If the nth line should be deleted as well or not False Source code in dataset/utils.py def remove_first_lines ( path : Union [ Text , Path ], n : int , inclusive = False ): \"\"\"Removes the first lines up until `n` In inclusive mode the nth line is also deleted otherwise only the lines before the nth line are delted. Args: path: The path to the file n: The \"nth\" line inclusive: If the nth line should be deleted as well or not \"\"\" if not inclusive : n -= 1 if n <= 0 : # nothing to do here return with open ( path , \"rb\" ) as original , NamedTemporaryFile ( \"wb\" , delete = False ) as temp : # skip the first n iterations for i in range ( n ): next ( original ) # now start the iterator at our new first line for line in original : temp . write ( line ) # replace old file with new file shutil . move ( temp . name , path ) resolve_indices ( dataset_name = None , prefix_dataset_name = True , index = None ) \u00b6 Resolves a given list of indices prefixing the dataset name if necessary. Parameters: Name Type Description Default dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed or not True index Union[Sequence[str], str] The indices to process None Returns: Type Description Union[Sequence[str], str] The prepared indices Source code in dataset/utils.py def resolve_indices ( dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ) -> Optional [ Union [ Sequence [ str ], str ]]: \"\"\"Resolves a given list of indices prefixing the dataset name if necessary. Args: dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed or not index: The indices to process Returns: The prepared indices \"\"\" if not prefix_dataset_name or dataset_name is None : # if prefix is disabled or we do not have a dataset name # use index as is return index if index is None : # no index then simply query whole dataset return f \" { dataset_name } -*\" if isinstance ( index , Text ): # prefix single index return f \" { dataset_name } - { index } \" # prefix index list return [ f \" { dataset_name } - { i } \" for i in index ] trim_file ( path , start_line , last_line ) \u00b6 Trim a file to the given start and end lines. Removes all lines before the start_line and after the last_line . If either of the values is omitted the very first or last line is used by default. Parameters: Name Type Description Default path Union[str, pathlib.Path] The file to trim required start_line Optional[int] The start line number required last_line Optional[int] The last line number required Source code in dataset/utils.py def trim_file ( path : Union [ Text , Path ], start_line : Optional [ int ], last_line : Optional [ int ], ): \"\"\"Trim a file to the given start and end lines. Removes all lines before the `start_line` and after the `last_line`. If either of the values is omitted the very first or last line is used by default. Args: path: The file to trim start_line: The start line number last_line: The last line number \"\"\" if last_line is not None and start_line is not None and start_line > 1 : print ( f \"Trimming file: { path } to be { start_line } - { last_line } \" ) # first truncate the file # so we don't have to adjust for new max line count if last_line is not None : truncate_file ( path , last_line ) # remove all lines before the start line if start_line is not None : remove_first_lines ( path , start_line , inclusive = False ) truncate_file ( path , last_line ) \u00b6 Truncates file to be only last_line long. Parameters: Name Type Description Default path Union[str, pathlib.Path] The file to truncate required last_line int The new last line required Source code in dataset/utils.py def truncate_file ( path : Union [ Text , Path ], last_line : int ): \"\"\"Truncates file to be only `last_line` long. Args: path: The file to truncate last_line: The new last line \"\"\" with open ( path , \"rb+\" ) as f : # read up to the last line for i in range ( last_line ): f . readline () # and then truncate the file to the current pointer f . truncate () version_info ( cli_info ) \u00b6 Returns formatted version information about the cr_kyoushi.simulation package . Adapted from Pydantic version.py Args; cli_info: The CLI info object Returns: Type Description str A formated string showing CLI tool information Source code in dataset/utils.py def version_info ( cli_info : Info ) -> str : \"\"\"Returns formatted version information about the `cr_kyoushi.simulation package`. Adapted from [Pydantic version.py](https://github.com/samuelcolvin/pydantic/blob/master/pydantic/version.py) Args; cli_info: The CLI info object Returns: A formated string showing CLI tool information \"\"\" import platform import sys from . import __version__ info = { \"cr_kyoushi.dataset version\" : __version__ , \"install path\" : Path ( __file__ ) . resolve () . parent , \"python version\" : sys . version , \"platform\" : platform . platform (), } return \" \\n \" . join ( \" {:>30} {} \" . format ( k + \":\" , str ( v ) . replace ( \" \\n \" , \" \" )) for k , v in info . items () ) write_config_file ( data , path ) \u00b6 Serialize config data into a file. Depending on the given destinations path file extension either YAML or JSON serialization is used (defaults to JSON). Parameters: Name Type Description Default data Any The data to serialize required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_config_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize config data into a file. Depending on the given destinations path file extension either YAML or JSON serialization is used (defaults to JSON). Args: data: The data to serialize path: The file to write to \"\"\" if isinstance ( path , Text ): path = Path ( path ) ext = path . suffix if ext in ( \".yaml\" , \".yml\" ): write_yaml_file ( data , path ) # unless yaml ext are defined we always write json else : write_json_file ( data , path ) write_json_file ( data , path ) \u00b6 Serialize data into a JSON file Parameters: Name Type Description Default data Any The data to serialize required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_json_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize data into a JSON file Args: data: The data to serialize path: The file to write to \"\"\" with open ( path , \"w\" ) as f : if isinstance ( data , BaseModel ): f . write ( data . json ()) else : json . dump ( data , f ) write_model_to_yaml ( model , path ) \u00b6 Serialize a Pydantic model into a YAML file Parameters: Name Type Description Default model BaseModel The Pydantic model required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_model_to_yaml ( model : BaseModel , path : Union [ Text , Path ]): \"\"\"Serialize a Pydantic model into a YAML file Args: model: The Pydantic model path: The file to write to \"\"\" # first serialize to json and reload as simple data # and then load serialized data and dump it as yaml # we have to do this since model.dict() would not serialize sub-models model_json = json . loads ( model . json ()) write_yaml_file ( model_json , path ) write_yaml_file ( data , path ) \u00b6 Serialize data into a YAML file Parameters: Name Type Description Default data Any The data to write required path Union[str, pathlib.Path] The file path to write to required Source code in dataset/utils.py def write_yaml_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize data into a YAML file Args: data: The data to write path: The file path to write to \"\"\" yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . default_flow_style = False with open ( path , \"w\" ) as f : yaml . dump ( data , f )","title":"Utility Functions"},{"location":"reference/utils/#utils-module","text":"This module contains general purpose utility functions.","title":"Utils module"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.copy_package_file","text":"Copies a package distributed file to some path. Parameters: Name Type Description Default package str The package of the file to copy required file str The file to copy required dest Path The path to copy the file to required overwrite bool If the destination path should be overwritten or not False Source code in dataset/utils.py def copy_package_file ( package : str , file : str , dest : Path , overwrite : bool = False ): \"\"\"Copies a package distributed file to some path. Args: package: The package of the file to copy file: The file to copy dest: The path to copy the file to overwrite: If the destination path should be overwritten or not \"\"\" if overwrite or not dest . exists (): with pkg_resources . path ( package , file ) as pkg_file : shutil . copy ( pkg_file , dest . absolute ())","title":"copy_package_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.create_dirs","text":"Creates the given list of directories. Parameters: Name Type Description Default directories List[pathlib.Path] The directories to create required always bool If the directories can already exist or not False Source code in dataset/utils.py def create_dirs ( directories : List [ Path ], always : bool = False ): \"\"\"Creates the given list of directories. Args: directories: The directories to create always: If the directories can already exist or not \"\"\" for d in directories : if always or not d . exists (): os . makedirs ( d , exist_ok = always )","title":"create_dirs()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.load_file","text":"Load data from a file (either JSON or YAML) The function will check the file extensions - .json - .yaml - .yml and will try to load using the respective parses. Any other file extension and file format will produce an error. Parameters: Name Type Description Default file Union[str, pathlib.Path] The file stream or path to load required Exceptions: Type Description NotImplementedError If the file format is not supported. Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_file ( file : Union [ Text , Path ]) -> Any : \"\"\"Load data from a file (either JSON or YAML) The function will check the file extensions - .json - .yaml - .yml and will try to load using the respective parses. Any other file extension and file format will produce an error. Args: file: The file stream or path to load Raises: NotImplementedError: If the file format is not supported. Returns: The loaded data \"\"\" if isinstance ( file , Text ): file = Path ( file ) ext = file . suffix if ext == \".json\" : return load_json_file ( file ) elif ext in ( \".yaml\" , \".yml\" ): return load_yaml_file ( file ) raise NotImplementedError ( f \"No file loader supported for { ext } files\" )","title":"load_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.load_json_file","text":"Parse and load a JSON file Parameters: Name Type Description Default file Union[BinaryIO, IO[str], _io.StringIO, str, pathlib.Path] The file stream or path to load required Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_json_file ( file : Union [ StreamTextType , Path ]) -> Any : \"\"\"Parse and load a JSON file Args: file: The file stream or path to load Returns: The loaded data \"\"\" if isinstance ( file , ( Text , Path )): with open ( file , \"r\" ) as f : return json . load ( f ) return json . load ( file )","title":"load_json_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.load_variables","text":"Loads variables from variable files. Parameters: Name Type Description Default sources Union[pathlib.Path, Dict[str, pathlib.Path]] The variable file/s to load required Returns: Type Description Any The loaded variables Source code in dataset/utils.py def load_variables ( sources : Union [ Path , Dict [ str , Union [ Path ]]]) -> Any : \"\"\"Loads variables from variable files. Args: sources: The variable file/s to load Returns: The loaded variables \"\"\" if isinstance ( sources , dict ): variables = {} for key , path in sources . items (): variables [ key ] = load_file ( path ) return variables return load_file ( sources )","title":"load_variables()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.load_yaml_file","text":"Parse and load a YAML file. Parameters: Name Type Description Default file Union[BinaryIO, IO[str], _io.StringIO, str, pathlib.Path] The file stream or path to load required Returns: Type Description Any The loaded data Source code in dataset/utils.py def load_yaml_file ( file : Union [ StreamTextType , Path ]) -> Any : \"\"\"Parse and load a YAML file. Args: file: The file stream or path to load Returns: The loaded data \"\"\" yaml = YAML ( typ = \"safe\" ) return yaml . load ( file )","title":"load_yaml_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.remove_first_lines","text":"Removes the first lines up until n In inclusive mode the nth line is also deleted otherwise only the lines before the nth line are delted. Parameters: Name Type Description Default path Union[str, pathlib.Path] The path to the file required n int The \"nth\" line required inclusive If the nth line should be deleted as well or not False Source code in dataset/utils.py def remove_first_lines ( path : Union [ Text , Path ], n : int , inclusive = False ): \"\"\"Removes the first lines up until `n` In inclusive mode the nth line is also deleted otherwise only the lines before the nth line are delted. Args: path: The path to the file n: The \"nth\" line inclusive: If the nth line should be deleted as well or not \"\"\" if not inclusive : n -= 1 if n <= 0 : # nothing to do here return with open ( path , \"rb\" ) as original , NamedTemporaryFile ( \"wb\" , delete = False ) as temp : # skip the first n iterations for i in range ( n ): next ( original ) # now start the iterator at our new first line for line in original : temp . write ( line ) # replace old file with new file shutil . move ( temp . name , path )","title":"remove_first_lines()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.resolve_indices","text":"Resolves a given list of indices prefixing the dataset name if necessary. Parameters: Name Type Description Default dataset_name Optional[str] The dataset name None prefix_dataset_name bool If the dataset name should be prefixed or not True index Union[Sequence[str], str] The indices to process None Returns: Type Description Union[Sequence[str], str] The prepared indices Source code in dataset/utils.py def resolve_indices ( dataset_name : Optional [ str ] = None , prefix_dataset_name : bool = True , index : Optional [ Union [ Sequence [ str ], str ]] = None , ) -> Optional [ Union [ Sequence [ str ], str ]]: \"\"\"Resolves a given list of indices prefixing the dataset name if necessary. Args: dataset_name: The dataset name prefix_dataset_name: If the dataset name should be prefixed or not index: The indices to process Returns: The prepared indices \"\"\" if not prefix_dataset_name or dataset_name is None : # if prefix is disabled or we do not have a dataset name # use index as is return index if index is None : # no index then simply query whole dataset return f \" { dataset_name } -*\" if isinstance ( index , Text ): # prefix single index return f \" { dataset_name } - { index } \" # prefix index list return [ f \" { dataset_name } - { i } \" for i in index ]","title":"resolve_indices()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.trim_file","text":"Trim a file to the given start and end lines. Removes all lines before the start_line and after the last_line . If either of the values is omitted the very first or last line is used by default. Parameters: Name Type Description Default path Union[str, pathlib.Path] The file to trim required start_line Optional[int] The start line number required last_line Optional[int] The last line number required Source code in dataset/utils.py def trim_file ( path : Union [ Text , Path ], start_line : Optional [ int ], last_line : Optional [ int ], ): \"\"\"Trim a file to the given start and end lines. Removes all lines before the `start_line` and after the `last_line`. If either of the values is omitted the very first or last line is used by default. Args: path: The file to trim start_line: The start line number last_line: The last line number \"\"\" if last_line is not None and start_line is not None and start_line > 1 : print ( f \"Trimming file: { path } to be { start_line } - { last_line } \" ) # first truncate the file # so we don't have to adjust for new max line count if last_line is not None : truncate_file ( path , last_line ) # remove all lines before the start line if start_line is not None : remove_first_lines ( path , start_line , inclusive = False )","title":"trim_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.truncate_file","text":"Truncates file to be only last_line long. Parameters: Name Type Description Default path Union[str, pathlib.Path] The file to truncate required last_line int The new last line required Source code in dataset/utils.py def truncate_file ( path : Union [ Text , Path ], last_line : int ): \"\"\"Truncates file to be only `last_line` long. Args: path: The file to truncate last_line: The new last line \"\"\" with open ( path , \"rb+\" ) as f : # read up to the last line for i in range ( last_line ): f . readline () # and then truncate the file to the current pointer f . truncate ()","title":"truncate_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.version_info","text":"Returns formatted version information about the cr_kyoushi.simulation package . Adapted from Pydantic version.py Args; cli_info: The CLI info object Returns: Type Description str A formated string showing CLI tool information Source code in dataset/utils.py def version_info ( cli_info : Info ) -> str : \"\"\"Returns formatted version information about the `cr_kyoushi.simulation package`. Adapted from [Pydantic version.py](https://github.com/samuelcolvin/pydantic/blob/master/pydantic/version.py) Args; cli_info: The CLI info object Returns: A formated string showing CLI tool information \"\"\" import platform import sys from . import __version__ info = { \"cr_kyoushi.dataset version\" : __version__ , \"install path\" : Path ( __file__ ) . resolve () . parent , \"python version\" : sys . version , \"platform\" : platform . platform (), } return \" \\n \" . join ( \" {:>30} {} \" . format ( k + \":\" , str ( v ) . replace ( \" \\n \" , \" \" )) for k , v in info . items () )","title":"version_info()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.write_config_file","text":"Serialize config data into a file. Depending on the given destinations path file extension either YAML or JSON serialization is used (defaults to JSON). Parameters: Name Type Description Default data Any The data to serialize required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_config_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize config data into a file. Depending on the given destinations path file extension either YAML or JSON serialization is used (defaults to JSON). Args: data: The data to serialize path: The file to write to \"\"\" if isinstance ( path , Text ): path = Path ( path ) ext = path . suffix if ext in ( \".yaml\" , \".yml\" ): write_yaml_file ( data , path ) # unless yaml ext are defined we always write json else : write_json_file ( data , path )","title":"write_config_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.write_json_file","text":"Serialize data into a JSON file Parameters: Name Type Description Default data Any The data to serialize required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_json_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize data into a JSON file Args: data: The data to serialize path: The file to write to \"\"\" with open ( path , \"w\" ) as f : if isinstance ( data , BaseModel ): f . write ( data . json ()) else : json . dump ( data , f )","title":"write_json_file()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.write_model_to_yaml","text":"Serialize a Pydantic model into a YAML file Parameters: Name Type Description Default model BaseModel The Pydantic model required path Union[str, pathlib.Path] The file to write to required Source code in dataset/utils.py def write_model_to_yaml ( model : BaseModel , path : Union [ Text , Path ]): \"\"\"Serialize a Pydantic model into a YAML file Args: model: The Pydantic model path: The file to write to \"\"\" # first serialize to json and reload as simple data # and then load serialized data and dump it as yaml # we have to do this since model.dict() would not serialize sub-models model_json = json . loads ( model . json ()) write_yaml_file ( model_json , path )","title":"write_model_to_yaml()"},{"location":"reference/utils/#cr_kyoushi.dataset.utils.write_yaml_file","text":"Serialize data into a YAML file Parameters: Name Type Description Default data Any The data to write required path Union[str, pathlib.Path] The file path to write to required Source code in dataset/utils.py def write_yaml_file ( data : Any , path : Union [ Text , Path ]): \"\"\"Serialize data into a YAML file Args: data: The data to write path: The file path to write to \"\"\" yaml = YAML () yaml . indent ( mapping = 2 , sequence = 4 , offset = 2 ) yaml . default_flow_style = False with open ( path , \"w\" ) as f : yaml . dump ( data , f )","title":"write_yaml_file()"}]}